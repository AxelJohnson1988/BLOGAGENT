{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AxelJohnson1988/BLOGAGENT/blob/main/Copy_of_Making_the_Most_of_your_Colab_Subscription.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKQ4bH7qMGrA"
      },
      "source": [
        "# Making the Most of your Colab Subscription\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjTLZD16HebW"
      },
      "source": [
        "# Access Popular LLMs via Google-Colab-AI Without an API Key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwVZpSEoHkQH"
      },
      "source": [
        "Users with Colab's paid plans have free access to most popular LLMs via google-colab-ai Python library. For more details, refer to the [getting started with google colab ai](https://colab.research.google.com/github/googlecolab/colabtools/blob/main/notebooks/Getting_started_with_google_colab_ai.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebl8tzi917TU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "157eda2b-ddac-45e0-f774-a2d981e9a6bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The capital of France is **Paris**.\n"
          ]
        }
      ],
      "source": [
        "from google.colab import ai\n",
        "response = ai.generate_text(\"What is the capital of France?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMMqmdiYMkvi"
      },
      "source": [
        "## Faster GPUs\n",
        "\n",
        "Users who have purchased one of Colab's paid plans have access to faster GPUs and more memory. You can upgrade your notebook's GPU settings in `Runtime > Change runtime type` in the menu to select from several accelerator options, subject to availability.\n",
        "\n",
        "The free of charge version of Colab grants access to Nvidia's T4 GPUs subject to quota restrictions and availability.\n",
        "\n",
        "You can see what GPU you've been assigned at any time by executing the following cell. If the execution result of running the code cell below is \"Not connected to a GPU\", you can change the runtime by going to `Runtime > Change runtime type` in the menu to enable a GPU accelerator, and then re-execute the code cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23TOba33L4qf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "031b7764-a498-4f7a-865b-f7d8ca71fed9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sa-IrJS1aRVJ"
      },
      "source": [
        "In order to use a GPU with your notebook, select the `Runtime > Change runtime type` menu, and then set the hardware accelerator to the desired option."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65MSuHKqNeBZ"
      },
      "source": [
        "## More memory\n",
        "\n",
        "Users who have purchased one of Colab's paid plans have access to high-memory VMs when they are available. More powerful GPUs are always offered with high-memory VMs.\n",
        "\n",
        "\n",
        "\n",
        "You can see how much memory you have available at any time by running the following code cell. If the execution result of running the code cell below is \"Not using a high-RAM runtime\", then you can enable a high-RAM runtime via `Runtime > Change runtime type` in the menu. Then select High-RAM in the Runtime shape toggle button. After, re-execute the code cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1G82GuO-tez",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5095f307-1f9d-4316-a1eb-df655cef5c02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 13.6 gigabytes of available RAM\n",
            "\n",
            "Not using a high-RAM runtime\n"
          ]
        }
      ],
      "source": [
        "import psutil\n",
        "\n",
        "ram_gb = psutil.virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJW8Qi-pPpep"
      },
      "source": [
        "## Longer runtimes\n",
        "\n",
        "All Colab runtimes are reset after some period of time (which is faster if the runtime isn't executing code). Colab Pro and Pro+ users have access to longer runtimes than those who use Colab free of charge.\n",
        "\n",
        "## Background execution\n",
        "\n",
        "Colab Pro+ users have access to background execution, where notebooks will continue executing even after you've closed a browser tab. This is always enabled in Pro+ runtimes as long as you have compute units available.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLlTRcMM_h0k"
      },
      "source": [
        "## Relaxing resource limits in Colab Pro\n",
        "\n",
        "Your resources are not unlimited in Colab. To make the most of Colab, avoid using resources when you don't need them. For example, only use a GPU when required and close Colab tabs when finished.\n",
        "\n",
        "\n",
        "\n",
        "If you encounter limitations, you can relax those limitations by purchasing more compute units via Pay As You Go. Anyone can purchase compute units via [Pay As You Go](https://colab.research.google.com/signup); no subscription is required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mm8FzEidvPs6"
      },
      "source": [
        "## Send us feedback!\n",
        "\n",
        "If you have any feedback for us, please let us know. The best way to send feedback is by using the Help > 'Send feedback...' menu. If you encounter usage limits in Colab Pro consider subscribing to Pro+.\n",
        "\n",
        "If you encounter errors or other issues with billing (payments) for Colab Pro, Pro+, or Pay As You Go, please email [colab-billing@google.com](mailto:colab-billing@google.com)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB3bdLe8jkAa"
      },
      "source": [
        "## More Resources\n",
        "\n",
        "### Working with Notebooks in Colab\n",
        "- [Overview of Colab](/notebooks/basic_features_overview.ipynb)\n",
        "- [Guide to Markdown](/notebooks/markdown_guide.ipynb)\n",
        "- [Importing libraries and installing dependencies](/notebooks/snippets/importing_libraries.ipynb)\n",
        "- [Saving and loading notebooks in GitHub](https://colab.research.google.com/github/googlecolab/colabtools/blob/main/notebooks/colab-github-demo.ipynb)\n",
        "- [Interactive forms](/notebooks/forms.ipynb)\n",
        "- [Interactive widgets](/notebooks/widgets.ipynb)\n",
        "\n",
        "<a name=\"working-with-data\"></a>\n",
        "### Working with Data\n",
        "- [Loading data: Drive, Sheets, and Google Cloud Storage](/notebooks/io.ipynb)\n",
        "- [Charts: visualizing data](/notebooks/charts.ipynb)\n",
        "- [Getting started with BigQuery](/notebooks/bigquery.ipynb)\n",
        "\n",
        "### Machine Learning Crash Course\n",
        "These are a few of the notebooks from Google's online Machine Learning course. See the [full course website](https://developers.google.com/machine-learning/crash-course/) for more.\n",
        "- [Intro to Pandas DataFrame](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/pandas_dataframe_ultraquick_tutorial.ipynb)\n",
        "- [Linear regression with tf.keras using synthetic data](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/linear_regression_with_synthetic_data.ipynb)\n",
        "\n",
        "\n",
        "<a name=\"using-accelerated-hardware\"></a>\n",
        "### Using Accelerated Hardware\n",
        "- [TensorFlow with GPUs](/notebooks/gpu.ipynb)\n",
        "- [TPUs in Colab](/notebooks/tpu.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58f5a163",
        "outputId": "e53d233c-acd2-4aa5-8b14-3880b2d779ba"
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "file_path = Path('/content/gemini_export.py')\n",
        "\n",
        "if file_path.exists():\n",
        "    try:\n",
        "        content = file_path.read_text(encoding='utf-8')\n",
        "        print(f\"Contents of {file_path}:\")\n",
        "        print(content)\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while reading {file_path}: {e}\")\n",
        "else:\n",
        "    print(f\"Error: The file {file_path} was not found.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contents of /content/gemini_export.py:\n",
            "#!/usr/bin/env python3\n",
            "import argparse, time\n",
            "from pathlib import Path\n",
            "import orjson\n",
            "from markdownify import markdownify\n",
            "from playwright.sync_api import sync_playwright\n",
            "\n",
            "def save_json(path, obj):\n",
            "    Path(path).write_bytes(orjson.dumps(obj, option=orjson.OPT_INDENT_2))\n",
            "\n",
            "def save_md(path, messages):\n",
            "    lines = []\n",
            "    for m in messages:\n",
            "        # Basic markdown formatting for role and text\n",
            "        role_text = f\"**{m['role'].title()}:**\" if m['role'] else \"Unknown Role:\"\n",
            "        lines.append(f\"{role_text} {m['text']}\\n\") # Use \\n for newline in string literal\n",
            "\n",
            "    Path(path).write_text(\"\\n\".join(lines), encoding=\"utf-8\") # Join with actual newline\n",
            "\n",
            "def export_gemini(url, outdir=\"export\"):\n",
            "    out = Path(outdir); out.mkdir(parents=True, exist_ok=True)\n",
            "    print(f\"Attempting to export Gemini conversation from: {url}\")\n",
            "    try:\n",
            "        with sync_playwright() as p:\n",
            "            # Use a timeout for the browser launch in case of issues\n",
            "            browser = p.chromium.launch(headless=True, timeout=60000) # 60 seconds\n",
            "            page = browser.new_page()\n",
            "            page.goto(url, wait_until=\"domcontentloaded\", timeout=60000) # Wait for content, 60 seconds\n",
            "\n",
            "            # Wait for messages to potentially load dynamically\n",
            "            # Look for a common element containing messages, adjust selector as needed\n",
            "            page.wait_for_selector(\"div[data-message-id]\", timeout=30000) # Wait up to 30 seconds for a message element\n",
            "\n",
            "            # Extract messages - adjust selectors based on Gemini page structure\n",
            "            # This is a heuristic and might need refinement based on the current Gemini UI\n",
            "            messages = []\n",
            "            # Target potential message bubble containers\n",
            "            message_elements = page.query_selector_all(\"div[data-message-id] > div > div\")\n",
            "\n",
            "            if not message_elements:\n",
            "                 print(\"Warning: No message elements found with the selector 'div[data-message-id] > div > div'. Trying alternative selectors.\")\n",
            "                 # Fallback selectors - examine the Gemini shared page HTML\n",
            "                 message_elements = page.query_selector_all(\"main div[role='text'], article div[role='text'], div.message-content\") # Example fallback selectors\n",
            "\n",
            "            if not message_elements:\n",
            "                 print(\"Error: Could not find any message elements on the page.\")\n",
            "                 return 0 # Return 0 messages exported\n",
            "\n",
            "            print(f\"Found {len(message_elements)} potential message elements.\")\n",
            "\n",
            "            for i, bubble in enumerate(message_elements):\n",
            "                try:\n",
            "                    # Attempt to find role (e.g., strong tag for role name) and text\n",
            "                    role_element = bubble.query_selector(\"strong\")\n",
            "                    role = role_element.inner_text().strip().replace(\":\", \"\") if role_element else f\"Part {i+1}\" # Default role if not found\n",
            "                    text_element = bubble # Sometimes the bubble element itself contains the text\n",
            "                    if role_element:\n",
            "                         # If role is found, text might be in the rest of the bubble or a sibling\n",
            "                         # This part is highly dependent on Gemini's ever-changing HTML structure\n",
            "                         # A more robust approach might involve examining the DOM structure around the role_element\n",
            "                         # For now, let's assume the rest of the bubble's text is the message, excluding the role part\n",
            "                         text = text_element.inner_text().replace(role_element.inner_text(), \"\", 1).strip() if role_element.inner_text() in text_element.inner_text() else text_element.inner_text().strip()\n",
            "                    else:\n",
            "                         # If no role element, just take the text content\n",
            "                         text = text_element.inner_text().strip()\n",
            "\n",
            "\n",
            "                    if not text: continue # Skip empty messages\n",
            "\n",
            "                    # Basic heuristic for user/assistant if role not explicitly found\n",
            "                    # This is fragile and based on common patterns; a better way is needed\n",
            "                    lower_text = text.lower()\n",
            "                    inferred_role = \"assistant\" if any(kw in lower_text for kw in [\"as a large language model\", \"i am a chatbot\", \"i am gemini\"]) else \"user\" # Very rough heuristic\n",
            "\n",
            "                    messages.append({\"role\": role or inferred_role, \"text\": text})\n",
            "                except Exception as e:\n",
            "                    print(f\"Error processing message element {i}: {e}\")\n",
            "                    # Continue processing other messages even if one fails\n",
            "\n",
            "\n",
            "            if not messages:\n",
            "                 print(\"Warning: Extracted elements but failed to parse any messages.\")\n",
            "                 return 0\n",
            "\n",
            "            ts = int(time.time())\n",
            "            json_path = out/f\"gemini_{ts}.json\"\n",
            "            md_path = out/f\"gemini_{ts}.md\"\n",
            "\n",
            "            save_json(json_path, messages)\n",
            "            save_md(md_path, messages)\n",
            "\n",
            "            print(f\"✅ Exported {len(messages)} messages to {json_path} and {md_path}\")\n",
            "            return len(messages)\n",
            "\n",
            "    except Exception as e:\n",
            "        print(f\"❌ An error occurred during export: {e}\")\n",
            "        # Optional: log the error\n",
            "        return 0 # Indicate export failed\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    ap = argparse.ArgumentParser(description=\"Export Gemini shared conversation.\")\n",
            "    ap.add_argument(\"url\", help=\"Gemini share link (https://g.co/gemini/share/..)\")\n",
            "    ap.add_argument(\"-o\", \"--outdir\", default=\"gemini_exports\", help=\"Output directory\")\n",
            "    args = ap.parse_args()\n",
            "\n",
            "    export_gemini(args.url, args.outdir)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a36053ad",
        "outputId": "441dd123-e504-4618-9772-f71a37c48923"
      },
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "plan_file_path = Path('./preflight/plan.json') # Assuming PLAN_OUT was ./preflight\n",
        "\n",
        "if plan_file_path.exists():\n",
        "    try:\n",
        "        with open(plan_file_path, 'r', encoding='utf-8') as f:\n",
        "            plan_content = json.load(f)\n",
        "        print(f\"Content of {plan_file_path}:\")\n",
        "        display(plan_content)\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error: Could not decode JSON from {plan_file_path}. The file might be corrupted or not in valid JSON format.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while reading {plan_file_path}: {e}\")\n",
        "else:\n",
        "    print(f\"Error: The file {plan_file_path} was not found. Please ensure the preflight_plan.py script generated it in the expected location.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: The file preflight/plan.json was not found. Please ensure the preflight_plan.py script generated it in the expected location.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFm2S0Gijqo8"
      },
      "source": [
        "<a name=\"machine-learning-examples\"></a>\n",
        "\n",
        "## Machine Learning Examples\n",
        "\n",
        "To see end-to-end examples of the interactive machine learning analyses that Colab makes possible, check out these tutorials using models from [TensorFlow Hub](https://tfhub.dev).\n",
        "\n",
        "A few featured examples:\n",
        "\n",
        "- [Retraining an Image Classifier](https://tensorflow.org/hub/tutorials/tf2_image_retraining): Build a Keras model on top of a pre-trained image classifier to distinguish flowers.\n",
        "- [Text Classification](https://tensorflow.org/hub/tutorials/tf2_text_classification): Classify IMDB movie reviews as either *positive* or *negative*.\n",
        "- [Style Transfer](https://tensorflow.org/hub/tutorials/tf2_arbitrary_image_stylization): Use deep learning to transfer style between images.\n",
        "- [Multilingual Universal Sentence Encoder Q&A](https://tensorflow.org/hub/tutorials/retrieval_with_tf_hub_universal_encoder_qa): Use a machine learning model to answer questions from the SQuAD dataset.\n",
        "- [Video Interpolation](https://tensorflow.org/hub/tutorials/tweening_conv3d): Predict what happened in a video between the first and the last frame.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9104fd24"
      },
      "source": [
        "# Task\n",
        "Explain how to use ML for topic modeling within the proposed integration architecture, including data collection, preprocessing, topic modeling techniques, integration, visualization, automation, and refinement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07b116b0"
      },
      "source": [
        "## Data collection\n",
        "\n",
        "### Subtask:\n",
        "Collect text data from your integrated tools (e.g., GitHub issue bodies, Notion page content, communication platform messages) via your integration hub (n8n).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0781451a"
      },
      "source": [
        "## Data preprocessing\n",
        "\n",
        "### Subtask:\n",
        "Clean and preprocess the collected text data to prepare it for topic modeling. This typically involves steps like removing punctuation, converting text to lowercase, removing stop words, and tokenization.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10c5f674"
      },
      "source": [
        "**Reasoning**:\n",
        "Describe the steps for cleaning and preprocessing text data for topic modeling, explaining the purpose of each step and listing relevant Python libraries. Then consider the specific data sources and their implications for preprocessing. Since this subtask only involves describing concepts and not writing code, I will use a markdown cell.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2159b531"
      },
      "source": [
        "# This subtask involves explaining concepts, not writing code.\n",
        "# I will use a markdown cell to provide the explanation."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ada1de12"
      },
      "source": [
        "**Reasoning**:\n",
        "Explain the typical steps, purpose, relevant libraries, and data source considerations for text data cleaning and preprocessing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "1201d08e",
        "outputId": "eb4c4563-7b2a-4d3e-e031-2b80f27b7ae9"
      },
      "source": [
        "\n",
        "# Explanation of text data cleaning and preprocessing for topic modeling\n",
        "\n",
        "## Text Data Cleaning and Preprocessing Steps for Topic Modeling\n",
        "\n",
        "Preparing text data for topic modeling is crucial to ensure accurate and meaningful results. The typical steps involved are:\n",
        "\n",
        "1.  **Removing Punctuation:** Eliminating characters like periods, commas, question marks, etc.\n",
        "2.  **Converting Text to Lowercase:** Standardizing the text by converting all characters to lowercase.\n",
        "3.  **Removing Stop Words:** Removing common words that do not carry significant meaning (e.g., \"the\", \"a\", \"is\").\n",
        "4.  **Tokenization:** Breaking down the text into individual words or tokens.\n",
        "5.  **Stemming or Lemmatization (Optional):** Reducing words to their root form (stemming) or their base form (lemmatization). While not strictly required for all topic modeling techniques, it can sometimes improve results by grouping variations of the same word.\n",
        "\n",
        "## Purpose of Each Step\n",
        "\n",
        "*   **Removing Punctuation:** Punctuation marks generally do not contribute to the meaning of words and can introduce noise. Removing them helps in focusing on the words themselves.\n",
        "*   **Converting Text to Lowercase:** This step ensures that the same word with different capitalization is treated as the same word (e.g., \"Topic\" and \"topic\"). This is important for accurate frequency counts and topic identification.\n",
        "*   **Removing Stop Words:** Stop words are highly frequent words that appear in almost all documents and do not help in distinguishing between topics. Removing them reduces the dimensionality of the data and helps the topic model focus on more informative words.\n",
        "*   **Tokenization:** This is a fundamental step that breaks the continuous text into discrete units (tokens) which are the basic building blocks for further analysis and processing by topic modeling algorithms.\n",
        "*   **Stemming or Lemmatization:** These techniques aim to reduce word variations to a common base form. For example, \"running,\" \"runs,\" and \"ran\" might all be reduced to \"run.\" This can help in grouping related words and improving the coherence of identified topics. Lemmatization is generally preferred over stemming as it produces actual words, whereas stemming might produce non-words.\n",
        "\n",
        "## Python Libraries for Text Preprocessing\n",
        "\n",
        "Several powerful Python libraries are available for text preprocessing tasks:\n",
        "\n",
        "*   **NLTK (Natural Language Toolkit):** A comprehensive library providing a wide range of tools for text processing, including tokenization, stop word removal, stemming, and lemmatization.\n",
        "*   **spaCy:** An efficient library for advanced natural language processing tasks, including tokenization, lemmatization, and part-of-speech tagging. It is generally faster and more memory-efficient than NLTK for large datasets.\n",
        "*   **scikit-learn:** While primarily a machine learning library, scikit-learn provides utilities for text feature extraction, such as `CountVectorizer` and `TfidfVectorizer`, which can perform tokenization and stop word removal.\n",
        "\n",
        "## Preprocessing Considerations for Specific Data Sources\n",
        "\n",
        "The specific data sources (GitHub issues, Notion pages, communication platform messages) might require some tailored preprocessing:\n",
        "\n",
        "*   **GitHub Issues:** May contain code snippets, links, mentions (@users), and issue numbers (#numbers). It might be necessary to remove or handle these specific elements during preprocessing to avoid them interfering with topic modeling.\n",
        "*   **Notion Pages:** Can contain various formatting elements, links, and potentially embedded content. Extracting the plain text content is crucial, and then similar considerations as GitHub issues regarding links and mentions might apply.\n",
        "*   **Communication Platform Messages:** Often contain informal language, emojis, mentions (@users), hashtags (#topics), and links. Cleaning might involve handling emojis, removing mentions and hashtags if they are not relevant to the topic, and addressing informal language or slang.\n",
        "\n",
        "In all cases, a preliminary exploration of the data is recommended to identify any domain-specific noise or patterns that need to be addressed during the preprocessing stage."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-2186588880.py, line 5)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2186588880.py\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    Preparing text data for topic modeling is crucial to ensure accurate and meaningful results. The typical steps involved are:\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "703ad6d6"
      },
      "source": [
        "First, ensure you have the Google Cloud Storage client library installed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4525f039",
        "outputId": "863e8d48-fb34-46ab-910b-da3294353ed0"
      },
      "source": [
        "!pip install google-cloud-storage"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=2.26.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage) (2.38.0)\n",
            "Requirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage) (2.25.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage) (2.4.3)\n",
            "Requirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage) (2.7.2)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage) (2.32.4)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage) (1.7.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage) (1.70.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage) (5.29.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage) (1.26.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (4.9.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2025.8.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.26.1->google-cloud-storage) (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e3abea8"
      },
      "source": [
        "Next, authenticate your Colab session to Google Cloud Platform. You've already done this, but I'll include the cell here as a reminder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e622e4e6"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05712c1f"
      },
      "source": [
        "You'll need to set your Google Cloud Project ID. Replace `[your Cloud Platform project ID]` with your actual project ID."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acc4846a"
      },
      "source": [
        "# https://cloud.google.com/resource-manager/docs/creating-managing-projects\n",
        "project_id = '[your Cloud Platform project ID]'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49dd3d5b"
      },
      "source": [
        "Now you can use the `google-cloud-storage` library to interact with GCS.\n",
        "\n",
        "Here's how to list your buckets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "id": "bd487a5e",
        "outputId": "98b90041-2c78-430d-b276-0a1da49000ab"
      },
      "source": [
        "from google.cloud import storage\n",
        "\n",
        "client = storage.Client(project=project_id)\n",
        "\n",
        "print(\"Buckets:\")\n",
        "for bucket in client.list_buckets():\n",
        "  print(f\"- {bucket.name}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Buckets:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "BadRequest",
          "evalue": "400 GET https://storage.googleapis.com/storage/v1/b?project=%5Byour+Cloud+Platform+project+ID%5D&projection=noAcl&prettyPrint=false: Project id: 0 is invalid or not found",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadRequest\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2550077622.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Buckets:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbucket\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_buckets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"- {bucket.name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/page_iterator.py\u001b[0m in \u001b[0;36m_items_iter\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_items_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Iterator for each item returned.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_page_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mincrement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_results\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/page_iterator.py\u001b[0m in \u001b[0;36m_page_iter\u001b[0;34m(self, increment)\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0mPage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0meach\u001b[0m \u001b[0mpage\u001b[0m \u001b[0mof\u001b[0m \u001b[0mitems\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mAPI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \"\"\"\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0mpage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_number\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/page_iterator.py\u001b[0m in \u001b[0;36m_next_page\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    371\u001b[0m         \"\"\"\n\u001b[1;32m    372\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_next_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_next_page_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0mpage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_to_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_page\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/page_iterator.py\u001b[0m in \u001b[0;36m_get_next_page_response\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_query_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_HTTP_METHOD\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"GET\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m             return self.api_request(\n\u001b[0m\u001b[1;32m    433\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_HTTP_METHOD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/cloud/storage/_http.py\u001b[0m in \u001b[0;36mapi_request\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                     \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maximum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multiplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             )\n\u001b[0;32m--> 294\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    295\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predicate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;31m# defer to shared logic for handling errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             next_sleep = _retry_error_helper(\n\u001b[0m\u001b[1;32m    157\u001b[0m                 \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0mdeadline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_base.py\u001b[0m in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0moriginal_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         )\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfinal_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msource_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mon_error_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mon_error_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misawaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ASYNC_RETRY_WARNING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/cloud/_http/__init__.py\u001b[0m in \u001b[0;36mapi_request\u001b[0;34m(self, method, path, query_params, data, content_type, headers, api_base_url, api_version, expect_json, _target_object, timeout, extra_api_info)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_http_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexpect_json\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBadRequest\u001b[0m: 400 GET https://storage.googleapis.com/storage/v1/b?project=%5Byour+Cloud+Platform+project+ID%5D&projection=noAcl&prettyPrint=false: Project id: 0 is invalid or not found"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7deab13"
      },
      "source": [
        "You can also create a new bucket (bucket names must be globally unique):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe0a1644",
        "outputId": "c16430e3-e25a-44a8-a453-e936a076b525"
      },
      "source": [
        "from google.cloud import storage\n",
        "import uuid\n",
        "\n",
        "client = storage.Client(project=project_id)\n",
        "\n",
        "# Generate a unique bucket name\n",
        "bucket_name = 'colab-demo-bucket-' + str(uuid.uuid4())\n",
        "location = 'us' # Choose a location, e.g., 'us', 'europe-west2'\n",
        "\n",
        "try:\n",
        "    bucket = client.create_bucket(bucket_name, location=location)\n",
        "    print(f\"Bucket '{bucket.name}' created.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error creating bucket: {e}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error creating bucket: 400 POST https://storage.googleapis.com/storage/v1/b?project=%5Byour+Cloud+Platform+project+ID%5D&prettyPrint=false: Unknown project id: [your Cloud Platform project ID]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1292028a"
      },
      "source": [
        "Here's how to upload a file to a bucket:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "057b82ec",
        "outputId": "710938da-1599-4d4e-c5d7-29e67c2ac3eb"
      },
      "source": [
        "from google.cloud import storage\n",
        "from pathlib import Path\n",
        "\n",
        "client = storage.Client(project=project_id)\n",
        "\n",
        "# Create a dummy local file\n",
        "local_file_path = Path('/tmp/my_upload_file.txt')\n",
        "local_file_path.write_text(\"This is a test file from Colab.\")\n",
        "\n",
        "bucket_name_to_upload = '[your bucket name]' # Replace with an existing bucket name\n",
        "destination_blob_name = 'uploaded_from_colab.txt'\n",
        "\n",
        "try:\n",
        "    bucket = client.get_bucket(bucket_name_to_upload)\n",
        "    blob = bucket.blob(destination_blob_name)\n",
        "    blob.upload_from_filename(local_file_path)\n",
        "    print(f\"File {local_file_path} uploaded to {destination_blob_name} in bucket {bucket_name_to_upload}.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error uploading file: {e}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error uploading file: Bucket names must start and end with a number or letter.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5a13a08"
      },
      "source": [
        "And here's how to download a file from a bucket:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0d558493",
        "outputId": "a709618b-5cff-4379-caba-17e3eec66f64"
      },
      "source": [
        "from google.cloud import storage\n",
        "from pathlib import Path\n",
        "\n",
        "client = storage.Client(project=project_id)\n",
        "\n",
        "bucket_name_to_download = '[your bucket name]' # Replace with an existing bucket name\n",
        "source_blob_name = 'uploaded_from_colab.txt' # Replace with the name of the object to download\n",
        "destination_file_path = Path('/tmp/my_downloaded_file.txt')\n",
        "\n",
        "try:\n",
        "    bucket = client.get_bucket(bucket_name_to_download)\n",
        "    blob = bucket.blob(source_blob_name)\n",
        "    blob.download_to_filename(destination_file_path)\n",
        "    print(f\"File {source_blob_name} downloaded from bucket {bucket_name_to_download} to {destination_file_path}.\")\n",
        "    # You can then read the downloaded file\n",
        "    # print(\"\\nDownloaded content:\")\n",
        "    # print(destination_file_path.read_text())\n",
        "except Exception as e:\n",
        "    print(f\"Error downloading file: {e}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error downloading file: Bucket names must start and end with a number or letter.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfad1fbc"
      },
      "source": [
        "To authenticate your Colab notebook to Google Cloud Platform (GCP), you can use the `google.colab.auth.authenticate_user()` function. This will open an authentication flow where you grant Colab permission to access GCP resources on your behalf."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "081dcdd3"
      },
      "source": [
        "**Remember to replace `[your bucket name]` in the code below with your actual bucket name before running the cell.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e423c075"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  return filename"
      ],
      "metadata": {
        "id": "OeTRgwxdyN2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "with open('example.txt', 'w') as f:\n",
        "  f.write('some content')\n",
        "\n",
        "files.download('example.txt')"
      ],
      "metadata": {
        "id": "haIrc14T0Gz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d794daa0"
      },
      "source": [
        "!SCAN_ROOT=/content PLAN_OUT=./preflight python preflight_plan.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython\n",
        "js_code = '''\n",
        "document.querySelector(\"#output-area\").appendChild(document.createTextNode(\"hello world!\"));\n",
        "'''\n",
        "display(IPython.display.Javascript(js_code))"
      ],
      "metadata": {
        "id": "ztv4VPkcyQR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Configure Gemini API key\n",
        "\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "\n",
        "gemini_api_secret_name = 'GOOGLE_API_KEY'  # @param {type: \"string\"}\n",
        "\n",
        "try:\n",
        "  GOOGLE_API_KEY=userdata.get(gemini_api_secret_name)\n",
        "  genai.configure(api_key=GOOGLE_API_KEY)\n",
        "except userdata.SecretNotFoundError as e:\n",
        "   print(f'Secret not found\\n\\nThis expects you to create a secret named {gemini_api_secret_name} in Colab\\n\\nVisit https://aistudio.google.com/app/apikey to create an API key\\n\\nStore that in the secrets section on the left side of the notebook (key icon)\\n\\nName the secret {gemini_api_secret_name}')\n",
        "   raise e\n",
        "except userdata.NotebookAccessError as e:\n",
        "  print(f'You need to grant this notebook access to the {gemini_api_secret_name} secret in order for the notebook to access Gemini on your behalf.')\n",
        "  raise e\n",
        "except Exception as e:\n",
        "  print(f\"There was an unknown error. Ensure you have a secret {gemini_api_secret_name} stored in Colab and it's a valid key from https://aistudio.google.com/app/apikey\")\n",
        "  raise e"
      ],
      "metadata": {
        "id": "9wQyHWQ4yRYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Create a prompt\n",
        "\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "\n",
        "api_key_name = 'GOOGLE_API_KEY' # @param {type: \"string\"}\n",
        "prompt = 'What is the velocity of an unladen swallow?' # @param {type: \"string\"}\n",
        "system_instructions = 'You have a tendency to speak in riddles.' # @param {type: \"string\"}\n",
        "model = 'gemini-2.0-flash' # @param {type: \"string\"} [\"gemini-1.0-pro\", \"gemini-1.5-pro\", \"gemini-1.5-flash\", \"gemini-2.0-flash\"]\n",
        "temperature = 0.5 # @param {type: \"slider\", min: 0, max: 2, step: 0.05}\n",
        "stop_sequence = '' # @param {type: \"string\"}\n",
        "\n",
        "if model == 'gemini-1.0-pro' and system_instructions is not None:\n",
        "  system_instructions = None\n",
        "  print('\\x1b[31m(WARNING: System instructions ignored, gemini-1.0-pro does not support system instructions)\\x1b[0m')\n",
        "\n",
        "if model == 'gemini-1.0-pro' and temperature > 1:\n",
        "  temperature = 1\n",
        "  print('\\x1b[34m(INFO: Temperature set to 1, gemini-1.0-pro does not support temperature > 1)\\x1b[0m')\n",
        "\n",
        "if system_instructions == '':\n",
        "  system_instructions = None\n",
        "\n",
        "api_key = userdata.get(api_key_name)\n",
        "genai.configure(api_key=api_key)\n",
        "model = genai.GenerativeModel(model, system_instruction=system_instructions)\n",
        "config = genai.GenerationConfig(temperature=temperature, stop_sequences=[stop_sequence])\n",
        "response = model.generate_content(contents=[prompt], generation_config=config)\n",
        "response.text"
      ],
      "metadata": {
        "id": "e44IJkCrySIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca04b834"
      },
      "source": [
        "### Practical Ways to Integrate Google Colab with Other Applications and Services\n",
        "\n",
        "Based on the Stack Overflow answer, here are some practical examples of integrating Google Colab with other apps and services."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a318716e"
      },
      "source": [
        "#### Mount Google Drive\n",
        "\n",
        "This is the easiest way to read and write files that can be used by other applications."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "835741ef"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# You can now read/write files under /content/drive/MyDrive/\n",
        "# Example:\n",
        "# with open('/content/drive/MyDrive/my_shared_file.txt', 'w') as f:\n",
        "#   f.write('Hello from Colab!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fca86954"
      },
      "source": [
        "#### Use Google Cloud Storage (GCS)\n",
        "\n",
        "For scalable storage shared with Google Cloud Platform (GCP) applications."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c82c325b"
      },
      "source": [
        "!pip install google-cloud-storage"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96e4e847"
      },
      "source": [
        "from google.cloud import storage\n",
        "\n",
        "# Authenticate if running outside of a GCP environment with service account\n",
        "# from google.colab import auth\n",
        "# auth.authenticate_user()\n",
        "\n",
        "# Use a service account or authentication flow if needed\n",
        "client = storage.Client()\n",
        "bucket_name = 'my-unique-bucket-name' # Replace with your bucket name\n",
        "bucket = client.get_bucket(bucket_name)\n",
        "\n",
        "# Example: Upload a file\n",
        "# blob = bucket.blob('path/out.csv')\n",
        "# local_file_path = 'local_file.csv' # Replace with your local file path\n",
        "# with open(local_file_path, 'w') as f:\n",
        "#     f.write('colab_data,123\\n') # Create a dummy local file\n",
        "# blob.upload_from_filename(local_file_path)\n",
        "# print(f\"Uploaded {local_file_path} to gs://{bucket_name}/path/out.csv\")\n",
        "\n",
        "# Example: Download a file\n",
        "# blob = bucket.blob('path/out.csv') # Replace with the path to your file in GCS\n",
        "# download_path = '/tmp/downloaded_file.csv'\n",
        "# blob.download_to_filename(download_path)\n",
        "# print(f\"Downloaded gs://{bucket_name}/path/out.csv to {download_path}\")\n",
        "# with open(download_path, 'r') as f:\n",
        "#     print(\"Downloaded content:\")\n",
        "#     print(f.read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a844fa4"
      },
      "source": [
        "#### Call REST APIs / OAuth-protected APIs\n",
        "\n",
        "Integrate with various web services like Slack, GitHub, payment gateways, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6400c696"
      },
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "# Example: Making a simple GET request to a public API\n",
        "try:\n",
        "    response = requests.get('https://api.github.com/users/googlecolab')\n",
        "    response.raise_for_status() # Raise an exception for bad status codes\n",
        "    user_data = response.json()\n",
        "    print(\"GitHub user data for googlecolab:\")\n",
        "    print(json.dumps(user_data, indent=2))\n",
        "\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error making API request: {e}\")\n",
        "\n",
        "# Example: Making a POST request (replace with a real API endpoint and payload)\n",
        "# url = 'https://api.example.com/endpoint'\n",
        "# payload = {'data': 'example_data'}\n",
        "# headers = {'Authorization': 'Bearer YOUR_ACCESS_TOKEN'} # Replace with your token\n",
        "# try:\n",
        "#     response = requests.post(url, json=payload, headers=headers)\n",
        "#     response.raise_for_status()\n",
        "#     print(\"POST request successful:\", response.json())\n",
        "# except requests.exceptions.RequestException as e:\n",
        "#     print(f\"Error making POST request: {e}\")\n",
        "\n",
        "# For OAuth-protected APIs, you would typically use libraries like `requests-oauthlib`\n",
        "# or service-specific SDKs, and handle authentication flows (e.g., using `google-auth` for GCP APIs).\n",
        "# This often involves redirecting the user for authorization or using pre-authorized credentials.\n",
        "# Due to the interactive nature, this is more complex in a standard Colab notebook cell."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d7c18f3"
      },
      "source": [
        "#### Sync with Google Sheets\n",
        "\n",
        "Read and update spreadsheets shared with other apps or dashboards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "970d1a28"
      },
      "source": [
        "!pip install gspread oauth2client"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4313ce6c"
      },
      "source": [
        "import gspread\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate with Google Colab's default credentials\n",
        "# This requires you to have appropriate permissions for the spreadsheet\n",
        "try:\n",
        "    auth.authenticate_user()\n",
        "    gc = gspread.authorize(GoogleCredentials.get_application_default())\n",
        "\n",
        "    # Open the spreadsheet by title or key\n",
        "    # Replace 'Your spreadsheet name' or 'YOUR_SHEET_KEY' with your actual sheet details\n",
        "    spreadsheet_name = 'Your spreadsheet name' # or use gc.open_by_key('YOUR_SHEET_KEY')\n",
        "    try:\n",
        "        sh = gc.open(spreadsheet_name)\n",
        "        worksheet = sh.sheet1 # Or sh.worksheet('Sheet2') for a specific sheet\n",
        "\n",
        "        # Example: Read data from the sheet\n",
        "        # data = worksheet.get_all_values()\n",
        "        # print(\"Data from the sheet:\")\n",
        "        # for row in data:\n",
        "        #     print(row)\n",
        "\n",
        "        # Example: Update a cell\n",
        "        # worksheet.update('A1', 'Hello from Colab!')\n",
        "        # print(\"Updated cell A1 with 'Hello from Colab!'\")\n",
        "\n",
        "        # Example: Append a row\n",
        "        # new_row = [\"Colab\", \"Data\", 123]\n",
        "        # worksheet.append_row(new_row)\n",
        "        # print(f\"Appended row: {new_row}\")\n",
        "\n",
        "    except gspread.SpreadsheetNotFound:\n",
        "        print(f\"Error: Spreadsheet '{spreadsheet_name}' not found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while accessing the spreadsheet: {e}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during Google Colab authentication: {e}\")\n",
        "    print(\"Please ensure you are authenticated and have the necessary permissions for Google Sheets.\")\n",
        "\n",
        "# Note: For production or automated workflows, using a service account is recommended\n",
        "# instead of user authentication.\n",
        "# gc = gspread.service_account(filename='path/to/your/service_account.json')\n",
        "# sh = gc.open_by_key('SHEET_ID')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27a43bc8"
      },
      "source": [
        "#### Expose a Colab Process to External Apps\n",
        "\n",
        "Useful for simple webhooks or testing local web servers within Colab. Note that for production use, dedicated hosting solutions are recommended."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfa97c9b"
      },
      "source": [
        "# Example using Flask and pyngrok\n",
        "# First, install necessary libraries\n",
        "!pip install Flask pyngrok"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f97b175f"
      },
      "source": [
        "from flask import Flask, request\n",
        "from pyngrok import ngrok\n",
        "import threading\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Set a Flask secret key (important for security, even in examples)\n",
        "os.environ['SECRET_KEY'] = 'your_secret_key_here' # Replace with a strong random key\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/')\n",
        "def home():\n",
        "    return \"Hello from Colab Flask App!\"\n",
        "\n",
        "@app.route('/webhook', methods=['POST'])\n",
        "def webhook():\n",
        "    data = request.json\n",
        "    print(\"Received webhook data:\", data)\n",
        "    # Process the data as needed\n",
        "    return {\"status\": \"success\", \"received_data\": data}, 200\n",
        "\n",
        "# Start ngrok tunnel in a separate thread\n",
        "# This requires a free ngrok account and potentially setting an auth token\n",
        "# ngrok website: https://ngrok.com/\n",
        "# Get your auth token: https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "# Add it to Colab secrets as NGROK_AUTH_TOKEN and uncomment the line below\n",
        "# from google.colab import userdata\n",
        "# ngrok.set_auth_token(userdata.get(\"NGROK_AUTH_TOKEN\"))\n",
        "\n",
        "def start_flask_app():\n",
        "    # Use a port that is available and allowed by Colab (e.g., 5000)\n",
        "    app.run(port=5000, host='0.0.0.0')\n",
        "\n",
        "# Start the Flask app in a separate thread\n",
        "flask_thread = threading.Thread(target=start_flask_app)\n",
        "flask_thread.daemon = True # Allow the main program to exit even if the thread is running\n",
        "# flask_thread.start()\n",
        "# print(\"Flask app started in a background thread on port 5000.\")\n",
        "\n",
        "# Note: Running web servers directly in Colab is generally not recommended for production.\n",
        "# Use this for testing or simple demos. The tunnel will close when the Colab session ends.\n",
        "\n",
        "# To start the ngrok tunnel and get the public URL:\n",
        "# Ensure Flask app is running first.\n",
        "# try:\n",
        "#     # The tunnel will expose the specified local port (5000)\n",
        "#     public_url = ngrok.connect(5000)\n",
        "#     print(f\"Flask app exposed at public URL: {public_url}\")\n",
        "#     print(\"You can send POST requests to {public_url}/webhook\")\n",
        "# except Exception as e:\n",
        "#     print(f\"Error starting ngrok tunnel: {e}\")\n",
        "#     print(\"Please ensure you have pyngrok installed, ngrok is installed, and your auth token is set if required.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53434e0d"
      },
      "source": [
        "#### Local Runtime / SSH Access\n",
        "\n",
        "Run notebook code on your local machine to access local services and files directly.\n",
        "\n",
        "*   **Connect to local runtime:** In Colab, go to `Connect` -> `Connect to local runtime`. Follow the instructions to set up and connect to a local environment. This allows your Colab notebook code to execute on your computer, giving it access to your local file system and network.\n",
        "*   **SSH Access:** For more advanced scenarios, you can potentially set up SSH access to a machine where you want to run your Colab code, though this requires more configuration and is not a standard Colab feature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc55c191"
      },
      "source": [
        "These examples provide a starting point for integrating Colab with various external services and data sources, based on the methods outlined in the Stack Overflow answer. Remember to replace placeholder values (like bucket names, sheet IDs, API tokens) with your actual credentials and details."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a980aa7"
      },
      "source": [
        "# @title Create a prompt\n",
        "\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "\n",
        "api_key_name = 'GOOGLE_API_KEY' # @param {type: \"string\"}\n",
        "prompt = 'What is the velocity of an unladen swallow?' # @param {type: \"string\"}\n",
        "system_instructions = 'You have a tendency to speak in riddles.' # @param {type: \"string\"}\n",
        "model = 'gemini-2.0-flash' # @param {type: \"string\"} [\"gemini-1.0-pro\", \"gemini-1.5-pro\", \"gemini-1.5-flash\", \"gemini-2.0-flash\"]\n",
        "temperature = 0.5 # @param {type: \"slider\", min: 0, max: 2, step: 0.05}\n",
        "stop_sequence = '' # @param {type: \"string\"}\n",
        "\n",
        "if model == 'gemini-1.0-pro' and system_instructions is not None:\n",
        "  system_instructions = None\n",
        "  print('\\x1b[31m(WARNING: System instructions ignored, gemini-1.0-pro does not support system instructions)\\x1b[0m')\n",
        "\n",
        "if model == 'gemini-1.0-pro' and temperature > 1:\n",
        "  temperature = 1\n",
        "  print('\\x1b[34m(INFO: Temperature set to 1, gemini-1.0-pro does not support temperature > 1)\\x1b[0m')\n",
        "\n",
        "if system_instructions == '':\n",
        "  system_instructions = None\n",
        "\n",
        "# To use the Gemini API, you need an API key. If you don't already have one, create a key in Google AI Studio.\n",
        "# In Colab, add the key to the secrets manager under the \"🔑\" in the left panel.\n",
        "# Give it the name `GOOGLE_API_KEY`.\n",
        "try:\n",
        "    api_key = userdata.get(api_key_name)\n",
        "    genai.configure(api_key=api_key)\n",
        "    model = genai.GenerativeModel(model, system_instruction=system_instructions)\n",
        "    config = genai.GenerationConfig(temperature=temperature, stop_sequences=[stop_sequence])\n",
        "    response = model.generate_content(contents=[prompt], generation_config=config)\n",
        "    print(response.text)\n",
        "except userdata.SecretNotFoundError as e:\n",
        "    print(f'Secret not found\\n\\nThis expects you to create a secret named {api_key_name} in Colab\\n\\nVisit https://aistudio.google.com/app/apikey to create an API key\\n\\nStore that in the secrets section on the left side of the notebook (key icon)\\n\\nName the secret {api_key_name}')\n",
        "    raise e\n",
        "except userdata.NotebookAccessError as e:\n",
        "    print(f'You need to grant this notebook access to the {api_key_name} secret in order for the notebook to access Gemini on your behalf.')\n",
        "    raise e\n",
        "except Exception as e:\n",
        "    print(f\"There was an unknown error. Ensure you have a secret {api_key_name} stored in Colab and it's a valid key from https://aistudio.google.com/app/apikey\")\n",
        "    raise e"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib-venn"
      ],
      "metadata": {
        "id": "W6uPKXmFyTHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import gspread\n",
        "from google.auth import default\n",
        "creds, _ = default()\n",
        "\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "worksheet = gc.open('Your spreadsheet name').sheet1\n",
        "\n",
        "# get_all_values gives a list of rows.\n",
        "rows = worksheet.get_all_values()\n",
        "print(rows)\n",
        "\n",
        "# Convert to a DataFrame and render.\n",
        "import pandas as pd\n",
        "pd.DataFrame.from_records(rows)"
      ],
      "metadata": {
        "id": "_gb-LeTvyT8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://pypi.python.org/pypi/libarchive\n",
        "!apt-get -qq install -y libarchive-dev && pip install -U libarchive\n",
        "import libarchive"
      ],
      "metadata": {
        "id": "CZP2vLWfyUef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://pypi.python.org/pypi/pydot\n",
        "!apt-get -qq install -y graphviz && pip install pydot\n",
        "import pydot"
      ],
      "metadata": {
        "id": "uHK5aScFyVCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython\n",
        "from google.colab import output\n",
        "\n",
        "def Concat(a, b):\n",
        "  # Use display.JSON to transfer a structured result.\n",
        "  return IPython.display.JSON({'result': ' '.join((a, b))})\n",
        "\n",
        "output.register_callback('notebook.Concat', Concat)"
      ],
      "metadata": {
        "id": "NZkKjl4AyVdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "\n",
        "slider = widgets.IntSlider(20, min=0, max=100)\n",
        "slider"
      ],
      "metadata": {
        "id": "es7cCobeyV9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "\n",
        "slider = widgets.IntSlider(20, min=0, max=100)\n",
        "slider"
      ],
      "metadata": {
        "id": "5cdvHtPKyWVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import PyDrive and associated libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "from pydrive2.auth import GoogleAuth\n",
        "from pydrive2.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# List .txt files in the root.\n",
        "#\n",
        "# Search query reference:\n",
        "# https://developers.google.com/drive/v2/web/search-parameters\n",
        "listed = drive.ListFile({'q': \"title contains '.txt' and 'root' in parents\"}).GetList()\n",
        "for file in listed:\n",
        "  print('title {}, id {}'.format(file['title'], file['id']))"
      ],
      "metadata": {
        "id": "COEog7q-yXII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ee\n",
        "import geemap\n",
        "ee.Initialize()\n",
        "m = geemap.Map()\n",
        "\n",
        "x = -122.4194\n",
        "y = 37.7749\n",
        "\n",
        "start_date = '2022-06-01'\n",
        "end_date = '2022-06-15'\n",
        "\n",
        "image = (ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')\n",
        "              .filterDate(start_date, end_date)\n",
        "              .filterBounds(ee.Geometry.Point(x, y)).mosaic())\n",
        "\n",
        "srtm = ee.Image('USGS/SRTMGL1_003')\n",
        "\n",
        "elevation = srtm.select('elevation')\n",
        "\n",
        "elevation_threshold = 100\n",
        "\n",
        "masked_image = image.updateMask(elevation.lt(elevation_threshold))\n",
        "\n",
        "m.add_layer(\n",
        "    elevation,\n",
        "    {'min': 0, 'max': 1000, 'palette': ['0000ff', '00ffff', 'ffff00', 'ff0000']},\n",
        "    'Elevation')\n",
        "\n",
        "m.add_layer(\n",
        "    masked_image,\n",
        "    {'bands': ['B4', 'B3', 'B2'], 'min': 0, 'max': 1500}, 'Masked Image')\n",
        "m.set_center(x, y, 10)\n",
        "m"
      ],
      "metadata": {
        "id": "WRAlcf25yXs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Authenticate.\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Install Cloud Storage FUSE.\n",
        "!echo \"deb https://packages.cloud.google.com/apt gcsfuse-`lsb_release -c -s` main\" | sudo tee /etc/apt/sources.list.d/gcsfuse.list\n",
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
        "!apt -qq update && apt -qq install gcsfuse"
      ],
      "metadata": {
        "id": "zfQ2zIHEyYNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "metadata": {
        "id": "ZrN2Tn5wyYs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# https://cloud.google.com/resource-manager/docs/creating-managing-projects\n",
        "project_id = '[your Cloud Platform project ID]'\n",
        "!gcloud config set project {project_id}"
      ],
      "metadata": {
        "id": "7p52_uL5yZLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Authenticate to GCS.\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# https://cloud.google.com/resource-manager/docs/creating-managing-projects\n",
        "project_id = '[your Cloud Platform project ID]'\n",
        "\n",
        "# Create the service client.\n",
        "from googleapiclient.discovery import build\n",
        "gcs_service = build('storage', 'v1')\n",
        "\n",
        "from apiclient.http import MediaIoBaseDownload\n",
        "\n",
        "with open('/tmp/gsutil_download.txt', 'wb') as f:\n",
        "  # Download the file from a given Google Cloud Storage bucket.\n",
        "  request = gcs_service.objects().get_media(bucket=bucket_name,\n",
        "                                            object='to_upload.txt')\n",
        "  media = MediaIoBaseDownload(f, request)\n",
        "\n",
        "  done = False\n",
        "  while not done:\n",
        "    # _ is a placeholder for a progress object that we ignore.\n",
        "    # (Our file is small, so we skip reporting progress.)\n",
        "    _, done = media.next_chunk()\n",
        "\n",
        "print('Download complete')"
      ],
      "metadata": {
        "id": "FoDt6swZyZmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the entire repo.\n",
        "!git clone -l -s git://github.com/jakevdp/PythonDataScienceHandbook.git cloned-repo\n",
        "%cd cloned-repo\n",
        "!ls"
      ],
      "metadata": {
        "id": "nKXl-rOgyaAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "metadata": {
        "id": "wD2exLaNyamc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "metadata": {
        "id": "3CW0WHo9ybDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "last_names = ['Connor', 'Connor', 'Reese']\n",
        "first_names = ['Sarah', 'John', 'Kyle']\n",
        "df = pd.DataFrame({\n",
        "  'first_name': first_names,\n",
        "  'last_name': last_names,\n",
        "})\n",
        "df"
      ],
      "metadata": {
        "id": "Xk3Berreyb1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame({\n",
        "  'first_name': ['Sarah', 'John', 'Kyle', 'Joe'],\n",
        "  'last_name': ['Connor', 'Connor', 'Reese', 'Bonnot'],\n",
        "})\n",
        "df.set_index('last_name', inplace=True)\n",
        "\n",
        "df.loc[~df.index.duplicated(), :]"
      ],
      "metadata": {
        "id": "PJG1imRsycJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code transforms or splits the dictionary column into many columns.\n",
        "\n",
        "E.g. The output DataFrame of this cell will have columns named [`date, letter, fruit, weather`]."
      ],
      "metadata": {
        "id": "ej9dC-t7yctD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame({\n",
        "  'request': ['GET /index.html?baz=3', 'GET /foo.html?bar=1'],\n",
        "})\n",
        "\n",
        "df['request'].str.extract('GET /([^?]+)\\?', expand=True)"
      ],
      "metadata": {
        "id": "OIjZqNJeyc-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame({\n",
        "  'time': ['2022-09-14 00:52:00-07:00', '2022-09-14 00:52:30-07:00',\n",
        "           '2022-09-14 01:52:30-07:00'],\n",
        "  'letter': ['A', 'B', 'C'],\n",
        "})\n",
        "df['time'] = pd.to_datetime(df.time)\n",
        "df.set_index('time', inplace=True)\n",
        "\n",
        "df.loc['2022-09-14':'2022-09-14 00:53']"
      ],
      "metadata": {
        "id": "TSF9jyYAydUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame({\n",
        "  'time': ['2022-09-14 00:52:00-07:00', '2022-09-14 00:52:30-07:00',\n",
        "           '2022-09-14 01:52:30-07:00'],\n",
        "  'letter': ['A', 'B', 'C'],\n",
        "})\n",
        "df['time'] = pd.to_datetime(df.time)\n",
        "\n",
        "def rows_in_time_range(df, time_column, start_ts_str, timedelta_str):\n",
        "  # Return rows from df, where start_ts < time_column <= start_ts + delta.\n",
        "  # start_ts_str can be a date '2022-09-01' or a time '2022-09-14 00:52:00-07:00'\n",
        "  # timedelta_str examples: '2 minutes'  '2 days 2 hours 15 minutes 30 seconds'\n",
        "  start_ts = pd.Timestamp(start_ts_str).tz_localize('US/Pacific')\n",
        "  end_ts = start_ts + pd.to_timedelta(timedelta_str)\n",
        "  return df.query(\"@start_ts <= {0} < @end_ts\".format(time_column))\n",
        "\n",
        "rows_in_time_range(df, 'time', '2022-09-14 00:00', '52 minutes 31 seconds')"
      ],
      "metadata": {
        "id": "CkoQo9tIydo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame({\n",
        "  'first_name': ['Sarah', 'John', 'Kyle', 'Joe'],\n",
        "  'last_name': ['Connor', 'Connor', 'Reese', 'Bonnot'],\n",
        "})\n",
        "\n",
        "df.loc[:, df.columns!='last_name']"
      ],
      "metadata": {
        "id": "Vp-WSOh-yd8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "terminator_df = pd.DataFrame({\n",
        "  'first_name': ['Sarah', 'John', 'Kyle'],\n",
        "  'last_name': ['Connor', 'Connor', 'Reese'],\n",
        "})\n",
        "terminator_df.set_index('first_name', inplace=True)\n",
        "\n",
        "buckaroo_df = pd.DataFrame({\n",
        "  'first_name': ['John', 'John', 'Buckaroo'],\n",
        "  'last_name': ['Parker', 'Whorfin', 'Banzai'],\n",
        "})\n",
        "buckaroo_df.set_index('first_name', inplace=True)\n",
        "\n",
        "terminator_df.index.intersection(buckaroo_df.index).shape"
      ],
      "metadata": {
        "id": "XMklS49Xyeor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame({\n",
        "  'first_name': ['Sarah', 'John', 'Kyle', 'Joe'],\n",
        "  'last_name': ['Connor', 'Connor', 'Reese', 'Bonnot'],\n",
        "})\n",
        "\n",
        "df[df.last_name.str.match('.*onno.*')]"
      ],
      "metadata": {
        "id": "sMO4nu-Uye_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame({\n",
        "  'time': ['2022-09-14 00:52:00-07:00', '2022-09-14 00:52:30-07:00',\n",
        "           '2022-09-14 01:52:30-07:00'],\n",
        "  'letter': ['A', 'B', 'C'],\n",
        "})\n",
        "df['time'] = pd.to_datetime(df.time)\n",
        "\n",
        "df.query('time >= \"2022-09-14 00:52:30-07:00\"')"
      ],
      "metadata": {
        "id": "4_xU0gLWyfS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate a variable as the value to find."
      ],
      "metadata": {
        "id": "Xcwq1hFLyfn5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate a variable as the value to find."
      ],
      "metadata": {
        "id": "Mwgo7OV_yf8X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate a variable, to use its value as the name of a column in a query.\n",
        "\n",
        "E.g. Query for rows where `John` is the value in the column named `first_name`."
      ],
      "metadata": {
        "id": "XOMi7veKygbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame({\n",
        "    'Year': [2016, 2015, 2014, 2013, 2012],\n",
        "    'Top Animal': ['Giant panda', 'Chicken', 'Pig', 'Turkey', 'Dog']\n",
        "})\n",
        "\n",
        "df.rename(columns={\n",
        "    'Year': 'Calendar Year',\n",
        "    'Top Animal': 'Favorite Animal',\n",
        "}, inplace=True)\n",
        "df"
      ],
      "metadata": {
        "id": "DLil9AKZyg3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creates a new DataFrame that is a transformed version of the input. E.g.\n",
        "*   Input: df with a column named `msg_ids` that is a list of values (i.e. many per row, at least in some rows).\n",
        "*   Output: new_df which has 1 row per unique value found in any of the original `msg_ids` lists, with that value in a new column named `msg_id`.\n"
      ],
      "metadata": {
        "id": "xN8KkLjAyhNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "terminator_df = pd.DataFrame({\n",
        "  'first_name': ['Sarah', 'John', 'Kyle'],\n",
        "  'last_name': ['Connor', 'Connor', 'Reese'],\n",
        "})\n",
        "terminator_df.set_index('first_name', inplace=True)\n",
        "\n",
        "buckaroo_df = pd.DataFrame({\n",
        "  'first_name': ['John', 'John', 'Buckaroo'],\n",
        "  'last_name': ['Parker', 'Whorfin', 'Banzai'],\n",
        "})\n",
        "buckaroo_df.set_index('first_name', inplace=True)\n",
        "\n",
        "terminator_df[~terminator_df.index.isin(buckaroo_df.index)]"
      ],
      "metadata": {
        "id": "lu5NRTjmyhhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the Series `map()` method.\n",
        "E.g. To filter by the length of a column values:"
      ],
      "metadata": {
        "id": "XyyOyl4myh2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame({\n",
        "  'first_name': ['Sarah', 'John', 'Kyle'],\n",
        "  'last_name': ['Connor', 'Connor', 'Reese'],\n",
        "})\n",
        "\n",
        "df.groupby(['last_name']).size().sort_values(ascending=False)"
      ],
      "metadata": {
        "id": "6IQ0SdaTyiMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.Timestamp('9/27/22 06:59').tz_localize('US/Pacific')"
      ],
      "metadata": {
        "id": "i_NlKmNNyiwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I.e. Midnight on the given date."
      ],
      "metadata": {
        "id": "0FE-l4EbyjE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.Timedelta('2 days 2 hours 15 minutes 30 seconds')"
      ],
      "metadata": {
        "id": "4a2n1v-myjkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From an integer.\n",
        "`unit` is a string, defaulting to `ns`. Possible values:\n"
      ],
      "metadata": {
        "id": "ub98VN3kyj_3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example keyworded args: {days, seconds, microseconds, milliseconds, minutes, hours, weeks}"
      ],
      "metadata": {
        "id": "u1SFvAwuykUV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can group timestamped data into intervals of arbitrary duration using a Grouper object to specify groupby instructions.  The `freq` parameter is a string that may contain an integer followed by an [offset alias](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases).  E.g. To see output for 2 minute long intervals:"
      ],
      "metadata": {
        "id": "UfyeWPrOykvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame({\n",
        "  'time': ['2022-09-14 00:52:00-07:00', '2022-09-14 00:52:30-07:00',\n",
        "           '2022-09-14 01:52:30-07:00'],\n",
        "  'letter': ['A', 'B', 'C'],\n",
        "})\n",
        "df['time'] = pd.to_datetime(df.time)\n",
        "\n",
        "df['time'].describe(datetime_is_numeric=True)"
      ],
      "metadata": {
        "id": "3nLKDQcmylE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext google.colab.data_table"
      ],
      "metadata": {
        "id": "Xs6jTOUIylal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "df = pd.DataFrame({\n",
        "  'dogs': [5, 10, np.nan, 7],\n",
        "})\n",
        "\n",
        "df['dogs'].replace(np.nan, 0, regex=True)"
      ],
      "metadata": {
        "id": "wfNeHH2vymDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython\n",
        "\n",
        "display(IPython.display.Javascript('''\n",
        "  const promise = new Promise((resolve, reject) => {\n",
        "    const script = document.createElement('script');\n",
        "    script.src = 'data:,window.value = \"hello world!\"';\n",
        "    script.onload = resolve;\n",
        "    script.onerror = reject;\n",
        "    document.head.appendChild(script);\n",
        "  });\n",
        "  // Pause subsequent outputs until the script has been loaded.\n",
        "  google.colab.output.pauseOutputUntil(promise);\n",
        "'''))\n",
        "\n",
        "display(IPython.display.Javascript('''\n",
        "    // Can immediately reference scripts loaded earlier since\n",
        "    // output processing was blocked on them.\n",
        "    document.body.appendChild(document.createTextNode(window.value));\n",
        "'''))\n"
      ],
      "metadata": {
        "id": "z_ilYqOnymfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import PyDrive and associated libraries.\n",
        "# This only needs to be done once in a notebook.\n",
        "from pydrive2.auth import GoogleAuth\n",
        "from pydrive2.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once in a notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Create & upload a text file.\n",
        "uploaded = drive.CreateFile({'title': 'Sample file.txt'})\n",
        "uploaded.SetContentString('Sample upload file content')\n",
        "uploaded.Upload()\n",
        "print('Uploaded file with ID {}'.format(uploaded.get('id')))"
      ],
      "metadata": {
        "id": "3SfuyXi1ym3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import gspread\n",
        "from google.auth import default\n",
        "creds, _ = default()\n",
        "\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "sh = gc.create('A new spreadsheet')\n",
        "\n",
        "# Open our new sheet and add some data.\n",
        "worksheet = gc.open('A new spreadsheet').sheet1\n",
        "\n",
        "cell_list = worksheet.range('A1:C2')\n",
        "\n",
        "import random\n",
        "for cell in cell_list:\n",
        "  cell.value = random.randint(1, 10)\n",
        "\n",
        "worksheet.update_cells(cell_list)\n",
        "# Go to https://sheets.google.com to see your new spreadsheet."
      ],
      "metadata": {
        "id": "wto3nvmXynVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# https://cloud.google.com/resource-manager/docs/creating-managing-projects\n",
        "project_id = '[your Cloud Platform project ID]'\n",
        "!gcloud config set project {project_id}"
      ],
      "metadata": {
        "id": "1qyu6kygynwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# https://cloud.google.com/resource-manager/docs/creating-managing-projects\n",
        "project_id = '[your Cloud Platform project ID]'"
      ],
      "metadata": {
        "id": "hZ8E7dmSyoFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An example of referencing these resources from outputs:"
      ],
      "metadata": {
        "id": "I1BwZA3nyocE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ee\n",
        "import geemap\n",
        "ee.Initialize()\n",
        "m = geemap.Map()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data import and filtering\n",
        "modis_collection = ee.ImageCollection(\n",
        "    'MODIS/061/MCD12Q1').filterDate('2019-01-01', '2020-01-01')\n",
        "\n",
        "country = ee.FeatureCollection('WM/geoLab/geoBoundaries/600/ADM0').filter(\n",
        "    ee.Filter.equals('shapeName', 'Suriname'))\n",
        "\n",
        "land_cover_image = modis_collection.select('LC_Type1').first().clip(country)\n",
        "\n",
        "land_cover_classes = ee.List.sequence(1, 17) # Assuming 17 land cover classes\n",
        "\n",
        "def calculate_area(class_value):\n",
        "  class_mask = land_cover_image.eq(ee.Number(class_value).toInt())\n",
        "  class_area = class_mask.multiply(ee.Image.pixelArea()).reduceRegion(\n",
        "    reducer=ee.Reducer.sum(),\n",
        "    geometry=country,\n",
        "    scale=500,\n",
        "    maxPixels=1e13\n",
        "    ).get('LC_Type1')\n",
        "  return ee.Feature(\n",
        "    None, {'class_value': class_value, 'class_area': class_area})\n",
        "\n",
        "area_results = ee.FeatureCollection(land_cover_classes.map(calculate_area))\n",
        "df = ee.data.computeFeatures(\n",
        "{'expression': area_results, 'fileFormat': 'PANDAS_DATAFRAME'})\n",
        "\n",
        "# Create bar chart using matplotlib\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(df['class_value'], df['class_area'])\n",
        "plt.xlabel('Land Cover Class')\n",
        "plt.ylabel('Area (square meters)')\n",
        "plt.title('Area of MODIS Land Cover Classes in Suriname (2019)')\n",
        "\n",
        "# Set x-ticks to be integers and show all classes\n",
        "plt.xticks(df['class_value'], rotation=45, ha='right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "I_CbkRKVypcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ],
      "metadata": {
        "id": "FIjGk6BiyqaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "24JONL4Dyq0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "jwlyUSbZyrna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "9C92T9NFyr_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load an example dataset\n",
        "from vega_datasets import data\n",
        "cars = data.cars()\n",
        "\n",
        "# plot the dataset, referencing dataframe column names\n",
        "import altair as alt\n",
        "alt.Chart(cars).mark_bar().encode(\n",
        "  x='mean(Miles_per_Gallon)',\n",
        "  y='Origin',\n",
        "  color='Origin'\n",
        ")"
      ],
      "metadata": {
        "id": "-pyrVyM9ystK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load an example dataset\n",
        "from vega_datasets import data\n",
        "cars = data.cars()\n",
        "\n",
        "# plot the dataset, referencing dataframe column names\n",
        "import altair as alt\n",
        "alt.Chart(cars).mark_bar().encode(\n",
        "  x=alt.X('Miles_per_Gallon', bin=True),\n",
        "  y='count()',\n",
        ")"
      ],
      "metadata": {
        "id": "c80SD2BXytEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load an example dataset\n",
        "from vega_datasets import data\n",
        "cars = data.cars()\n",
        "\n",
        "import altair as alt\n",
        "\n",
        "interval = alt.selection_interval()\n",
        "\n",
        "alt.Chart(cars).mark_point().encode(\n",
        "  x='Horsepower',\n",
        "  y='Miles_per_Gallon',\n",
        "  color=alt.condition(interval, 'Origin', alt.value('lightgray'))\n",
        ").properties(\n",
        "  selection=interval\n",
        ")"
      ],
      "metadata": {
        "id": "2VkxZ2p6ythy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load an example dataset\n",
        "from vega_datasets import data\n",
        "cars = data.cars()\n",
        "\n",
        "# plot the dataset, referencing dataframe column names\n",
        "import altair as alt\n",
        "alt.Chart(cars).mark_point().encode(\n",
        "  x='Horsepower',\n",
        "  y='Miles_per_Gallon',\n",
        "  color='Origin'\n",
        ").interactive()"
      ],
      "metadata": {
        "id": "795XIPq4yt6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load an example dataset\n",
        "from vega_datasets import data\n",
        "cars = data.cars()\n",
        "\n",
        "import altair as alt\n",
        "\n",
        "interval = alt.selection_interval()\n",
        "\n",
        "base = alt.Chart(cars).mark_point().encode(\n",
        "  y='Miles_per_Gallon',\n",
        "  color=alt.condition(interval, 'Origin', alt.value('lightgray'))\n",
        ").properties(\n",
        "  selection=interval\n",
        ")\n",
        "\n",
        "base.encode(x='Acceleration') | base.encode(x='Horsepower')"
      ],
      "metadata": {
        "id": "8SaontkQyuRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load an example dataset\n",
        "from vega_datasets import data\n",
        "cars = data.cars()\n",
        "\n",
        "import altair as alt\n",
        "\n",
        "interval = alt.selection_interval()\n",
        "\n",
        "points = alt.Chart(cars).mark_point().encode(\n",
        "  x='Horsepower',\n",
        "  y='Miles_per_Gallon',\n",
        "  color=alt.condition(interval, 'Origin', alt.value('lightgray'))\n",
        ").properties(\n",
        "  selection=interval\n",
        ")\n",
        "\n",
        "histogram = alt.Chart(cars).mark_bar().encode(\n",
        "  x='count()',\n",
        "  y='Origin',\n",
        "  color='Origin'\n",
        ").transform_filter(interval)\n",
        "\n",
        "points & histogram"
      ],
      "metadata": {
        "id": "tvRm7pR9yujx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load an example dataset\n",
        "from vega_datasets import data\n",
        "cars = data.cars()\n",
        "\n",
        "import altair as alt\n",
        "\n",
        "interval = alt.selection_interval()\n",
        "\n",
        "points = alt.Chart(cars).mark_point().encode(\n",
        "  x='Horsepower',\n",
        "  y='Miles_per_Gallon',\n",
        "  color=alt.condition(interval, 'Origin', alt.value('lightgray'))\n",
        ").properties(\n",
        "  selection=interval\n",
        ")\n",
        "\n",
        "histogram = alt.Chart(cars).mark_bar().encode(\n",
        "  x='count()',\n",
        "  y='Origin',\n",
        "  color='Origin'\n",
        ").transform_filter(interval)\n",
        "\n",
        "points & histogram"
      ],
      "metadata": {
        "id": "w0F3In0myu2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load an example dataset\n",
        "from vega_datasets import data\n",
        "cars = data.cars()\n",
        "\n",
        "import altair as alt\n",
        "\n",
        "points = alt.Chart(cars).mark_point().encode(\n",
        "  x='Year:T',\n",
        "  y='Miles_per_Gallon',\n",
        "  color='Origin'\n",
        ").properties(\n",
        "  width=800\n",
        ")\n",
        "\n",
        "lines = alt.Chart(cars).mark_line().encode(\n",
        "  x='Year:T',\n",
        "  y='mean(Miles_per_Gallon)',\n",
        "  color='Origin'\n",
        ").properties(\n",
        "  width=800\n",
        ").interactive(bind_y=False)\n",
        "\n",
        "points + lines"
      ],
      "metadata": {
        "id": "9g3auTjIyvPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load an example dataset\n",
        "from vega_datasets import data\n",
        "cars = data.cars()\n",
        "\n",
        "import altair as alt\n",
        "\n",
        "points = alt.Chart(cars).mark_point().encode(\n",
        "  x='Year:T',\n",
        "  y='Miles_per_Gallon',\n",
        "  color='Origin'\n",
        ").properties(\n",
        "  width=800\n",
        ")\n",
        "\n",
        "lines = alt.Chart(cars).mark_line().encode(\n",
        "  x='Year:T',\n",
        "  y='mean(Miles_per_Gallon)',\n",
        "  color='Origin'\n",
        ").properties(\n",
        "  width=800\n",
        ").interactive(bind_y=False)\n",
        "\n",
        "points + lines"
      ],
      "metadata": {
        "id": "1Ek0yULhyvha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load an example dataset\n",
        "from vega_datasets import data\n",
        "cars = data.cars()\n",
        "\n",
        "import altair as alt\n",
        "\n",
        "points = alt.Chart(cars).mark_point().encode(\n",
        "  x='Year:T',\n",
        "  y='Miles_per_Gallon',\n",
        "  color='Origin'\n",
        ").properties(\n",
        "  width=800\n",
        ")\n",
        "\n",
        "lines = alt.Chart(cars).mark_line().encode(\n",
        "  x='Year:T',\n",
        "  y='mean(Miles_per_Gallon)',\n",
        "  color='Origin'\n",
        ").properties(\n",
        "  width=800\n",
        ").interactive(bind_y=False)\n",
        "\n",
        "points + lines"
      ],
      "metadata": {
        "id": "q7g_1-koyvyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add `%load_ext cudf.pandas` before importing pandas to speed up operations using GPU"
      ],
      "metadata": {
        "id": "O_wUW1_Pyxdc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext cudf.pandas\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Randomly generated dataset of parking violations-\n",
        "# Define the number of rows\n",
        "num_rows = 1000000\n",
        "\n",
        "states = [\"NY\", \"NJ\", \"CA\", \"TX\"]\n",
        "violations = [\"Double Parking\", \"Expired Meter\", \"No Parking\",\n",
        "              \"Fire Hydrant\", \"Bus Stop\"]\n",
        "vehicle_types = [\"SUBN\", \"SDN\"]\n",
        "\n",
        "# Create a date range\n",
        "start_date = \"2022-01-01\"\n",
        "end_date = \"2022-12-31\"\n",
        "dates = pd.date_range(start=start_date, end=end_date, freq='D')\n",
        "\n",
        "# Generate random data\n",
        "data = {\n",
        "    \"Registration State\": np.random.choice(states, size=num_rows),\n",
        "    \"Violation Description\": np.random.choice(violations, size=num_rows),\n",
        "    \"Vehicle Body Type\": np.random.choice(vehicle_types, size=num_rows),\n",
        "    \"Issue Date\": np.random.choice(dates, size=num_rows),\n",
        "    \"Ticket Number\": np.random.randint(1000000000, 9999999999, size=num_rows)\n",
        "}\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Which parking violation is most commonly committed by vehicles from various U.S states?\n",
        "\n",
        "(df[[\"Registration State\", \"Violation Description\"]]  # get only these two columns\n",
        " .value_counts()  # get the count of offences per state and per type of offence\n",
        " .groupby(\"Registration State\")  # group by state\n",
        " .head(1)  # get the first row in each group (the type of offence with the largest count)\n",
        " .sort_index()  # sort by state name\n",
        " .reset_index()\n",
        ")"
      ],
      "metadata": {
        "id": "Kd3QsnjTyxdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery\n",
        "\n",
        "# https://cloud.google.com/resource-manager/docs/creating-managing-projects\n",
        "project_id = '[your Cloud Platform project ID]'\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "for dataset in client.list_datasets():\n",
        "  print(dataset.dataset_id)"
      ],
      "metadata": {
        "id": "vvCU62gUyr_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery\n",
        "\n",
        "# https://cloud.google.com/resource-manager/docs/creating-managing-projects\n",
        "project_id = '[your Cloud Platform project ID]'\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "for dataset in client.list_datasets():\n",
        "  print(dataset.dataset_id)"
      ],
      "metadata": {
        "id": "gNoOKnBiyrna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "project_id = '[your project ID]'"
      ],
      "metadata": {
        "id": "MSf89Bkvyq0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import bigframes.pandas as bpd\n",
        "from google.cloud import bigquery\n",
        "\n",
        "# Set BigQuery DataFrames options\n",
        "bpd.options.bigquery.project = project_id\n",
        "bpd.options.bigquery.location = \"US\""
      ],
      "metadata": {
        "id": "sWmtD3Mnyq0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import syntax\n",
        "query = syntax.sql('''\n",
        "    SELECT *\n",
        "    FROM `bigquery-public-data.ml_datasets.penguins`\n",
        "    LIMIT 20\n",
        "''')\n",
        "\n",
        "# Load data from a BigQuery table using BigFrames DataFrames:\n",
        "bq_df = bpd.read_gbq(query)"
      ],
      "metadata": {
        "id": "weMrmw3Kyq0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bq_df.describe()"
      ],
      "metadata": {
        "id": "gs69sYikyq0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bq_df.head(10)"
      ],
      "metadata": {
        "id": "Jb1r5VKEyq0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support for third party widgets will remain active for the duration of the session. To disable support:"
      ],
      "metadata": {
        "id": "9v2ML8uJyqaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.disable_custom_widget_manager()"
      ],
      "metadata": {
        "id": "c6UmB-AsyqaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%html\n",
        "<link rel=\"stylesheet\" href=\"/nbextensions/google.colab/tabbar.css\">\n",
        "<div class='goog-tab'>\n",
        "  Some content\n",
        "</div>"
      ],
      "metadata": {
        "id": "AjqKZqbcyocF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import portpicker\n",
        "import threading\n",
        "import socket\n",
        "import IPython\n",
        "\n",
        "from six.moves import socketserver\n",
        "from six.moves import SimpleHTTPServer\n",
        "\n",
        "class V6Server(socketserver.TCPServer):\n",
        "  address_family = socket.AF_INET6\n",
        "\n",
        "class Handler(SimpleHTTPServer.SimpleHTTPRequestHandler):\n",
        "  def do_GET(self):\n",
        "    self.send_response(200)\n",
        "    # If the response should not be cached in the notebook for\n",
        "    # offline access:\n",
        "    # self.send_header('x-colab-notebook-cache-control', 'no-cache')\n",
        "    self.end_headers()\n",
        "    self.wfile.write(b'''\n",
        "      document.querySelector('#output-area').appendChild(document.createTextNode('Script result!'));\n",
        "    ''')\n",
        "\n",
        "port = portpicker.pick_unused_port()\n",
        "\n",
        "def server_entry():\n",
        "    httpd = V6Server(('::', port), Handler)\n",
        "    # Handle a single request then exit the thread.\n",
        "    httpd.serve_forever()\n",
        "\n",
        "thread = threading.Thread(target=server_entry)\n",
        "thread.start()\n",
        "\n",
        "# Display some HTML referencing the resource.\n",
        "display(IPython.display.HTML('<script src=\"https://localhost:{port}/\"></script>'.format(port=port)))"
      ],
      "metadata": {
        "id": "UsOsnRjiyocF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.serve_kernel_port_as_iframe(port)"
      ],
      "metadata": {
        "id": "fvNnbZVJyocF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will create an iframe browsing the HTTP server hosted on the machine your kernel is running on.\n",
        "\n",
        "Alternatively to view the server in a separate browser tab:"
      ],
      "metadata": {
        "id": "G9tmsQXlyocF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.serve_kernel_port_as_window(port)"
      ],
      "metadata": {
        "id": "WpUDoDtvyocF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The server will only be accessible to the executor of the notebook while the notebook is being viewed in Colab."
      ],
      "metadata": {
        "id": "7AXdyFH_yocF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from googleapiclient.discovery import build\n",
        "gcs_service = build('storage', 'v1')\n",
        "\n",
        "# Generate a random bucket name to which we'll upload the file.\n",
        "import uuid\n",
        "bucket_name = 'colab-sample-bucket' + str(uuid.uuid1())\n",
        "\n",
        "body = {\n",
        "  'name': bucket_name,\n",
        "  # For a full list of locations, see:\n",
        "  # https://cloud.google.com/storage/docs/bucket-locations\n",
        "  'location': 'us',\n",
        "}\n",
        "gcs_service.buckets().insert(project=project_id, body=body).execute()\n",
        "print('Done')"
      ],
      "metadata": {
        "id": "xbt5_fAoyoFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from googleapiclient.http import MediaFileUpload\n",
        "\n",
        "media = MediaFileUpload('/tmp/to_upload.txt',\n",
        "                        mimetype='text/plain',\n",
        "                        resumable=True)\n",
        "\n",
        "request = gcs_service.objects().insert(bucket=bucket_name,\n",
        "                                       name='to_upload.txt',\n",
        "                                       media_body=media)\n",
        "\n",
        "response = None\n",
        "while response is None:\n",
        "  # _ is a placeholder for a progress object that we ignore.\n",
        "  # (Our file is small, so we skip reporting progress.)\n",
        "  _, response = request.next_chunk()\n",
        "\n",
        "print('Upload complete')\n",
        "print('https://console.cloud.google.com/storage/browser?project={}'.format(project_id))"
      ],
      "metadata": {
        "id": "k3bznvrkyoFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a local file with data to upload.\n",
        "with open('/tmp/to_upload.txt', 'w') as f:\n",
        "  f.write('my sample file')\n",
        "\n",
        "import uuid\n",
        "\n",
        "# Make a unique bucket to which we'll upload the file.\n",
        "# (GCS buckets are part of a single global namespace.)\n",
        "bucket_name = 'colab-sample-bucket-' + str(uuid.uuid1())\n",
        "\n",
        "# Full reference: https://cloud.google.com/storage/docs/gsutil/commands/mb\n",
        "!gsutil mb gs://{bucket_name}\n",
        "\n",
        "# Copy the file to our new bucket.\n",
        "# Full reference: https://cloud.google.com/storage/docs/gsutil/commands/cp\n",
        "!gsutil cp /tmp/to_upload.txt gs://{bucket_name}/\n",
        "\n",
        "# Finally, dump the contents of our newly copied file to make sure everything worked.\n",
        "!gsutil cat gs://{bucket_name}/to_upload.txt"
      ],
      "metadata": {
        "id": "oUQCbgwVynwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After executing the cell above, a new file named 'Sample file.txt' will appear in your [drive.google.com](https://drive.google.com/) file list."
      ],
      "metadata": {
        "id": "S0w9T8lVym3x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With this enabled, dataframes are shown as rich, interactive tables:"
      ],
      "metadata": {
        "id": "94H1NrjGylal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from vega_datasets import data\n",
        "data.cars()"
      ],
      "metadata": {
        "id": "awqfiUJ7ylal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To restore the standard static display, unload the extension:"
      ],
      "metadata": {
        "id": "Y4wsrbVMylal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%unload_ext google.colab.data_table"
      ],
      "metadata": {
        "id": "1So541Deylal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.cars()"
      ],
      "metadata": {
        "id": "x2apMV5Vylam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame({\n",
        "  'time': ['2022-09-01 00:00:01-07:00', '2022-09-01 00:00:02-07:00',\n",
        "           '2022-09-01 00:01:00-07:00', '2022-09-01 00:02:00-07:00',\n",
        "           '2022-09-01 00:03:00-07:00', '2022-09-01 00:04:00-07:00',\n",
        "           '2022-09-01 00:05:00-07:00', '2022-09-01 00:07:00-07:00'],\n",
        "  'requests': [1, 1, 1, 1, 1, 1, 1, 1],\n",
        "})\n",
        "df['time'] = pd.to_datetime(df.time)\n",
        "\n",
        "df.groupby(pd.Grouper(key='time', freq='2min')).sum()"
      ],
      "metadata": {
        "id": "RuvSWiMQykvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.Timedelta(days=2)"
      ],
      "metadata": {
        "id": "TAqnd8ppykUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.to_timedelta(1, unit='h')"
      ],
      "metadata": {
        "id": "OjuVTpQUyj_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.Timestamp('9/27/22').tz_localize('US/Pacific')"
      ],
      "metadata": {
        "id": "M1HaZsMCyjE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame({\n",
        "  'first_name': ['Sarah', 'John', 'Kyle'],\n",
        "  'last_name': ['Connor', 'Connor', 'Reese'],\n",
        "})\n",
        "\n",
        "df[df['last_name'].map(len) == 5]"
      ],
      "metadata": {
        "id": "LIvfhBEnyh2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame({\n",
        "  'date': ['9/1/22', '9/2/22', '9/3/22'],\n",
        "  'action': ['Add', 'Update', 'Delete'],\n",
        "  'msg_ids': [[1, 2, 3], [], [2, 3]],\n",
        "})\n",
        "df.set_index('date', inplace=True)\n",
        "\n",
        "\n",
        "temp_series = df['msg_ids'].apply(pd.Series, 1).stack()\n",
        "temp_series.index = temp_series.index.droplevel(-1)\n",
        "temp_series.name = 'msg_id'\n",
        "new_df = temp_series.to_frame()\n",
        "new_df.set_index('msg_id', inplace=True)\n",
        "new_df.loc[~new_df.index.duplicated(), :] # Drop duplicates."
      ],
      "metadata": {
        "id": "gf7QQzUXyhNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(data={\n",
        "  'first_name': ['Sarah', 'John', 'Kyle'],\n",
        "  'last_name': ['Connor', 'Connor', 'Reese'],\n",
        "})\n",
        "\n",
        "column_name = 'first_name'\n",
        "df.query(f\"`{column_name}` == 'John'\")"
      ],
      "metadata": {
        "id": "yFbiTXMTygbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame({\n",
        "  'first_name': ['Sarah', 'John', 'Kyle'],\n",
        "  'last_name': ['Connor', 'Connor', 'Reese'],\n",
        "})\n",
        "\n",
        "foo = 'Connor'\n",
        "df.query('last_name == @foo')"
      ],
      "metadata": {
        "id": "2LEMeVDCyf8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame({\n",
        "  'first_name': ['Sarah', 'John', 'Kyle'],\n",
        "  'last_name': ['Connor', 'Connor', 'Reese'],\n",
        "})\n",
        "\n",
        "foo = 'Connor'\n",
        "df.query('last_name == @foo')"
      ],
      "metadata": {
        "id": "uzwfJ48vyfn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame({\n",
        "  'date': ['2022-09-14', '2022-09-15', '2022-09-16'],\n",
        "  'letter': ['A', 'B', 'C'],\n",
        "  'dict' : [{ 'fruit': 'apple', 'weather': 'aces'},\n",
        "            { 'fruit': 'banana', 'weather': 'bad'},\n",
        "            { 'fruit': 'cantaloupe', 'weather': 'cloudy'}],\n",
        "})\n",
        "\n",
        "pd.concat([df.drop(['dict'], axis=1), df['dict'].apply(pd.Series)], axis=1)"
      ],
      "metadata": {
        "id": "xFpXAIByyctD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch a single <1MB file using the raw GitHub URL.\n",
        "!curl --remote-name \\\n",
        "     -H 'Accept: application/vnd.github.v3.raw' \\\n",
        "     --location https://api.github.com/repos/jakevdp/PythonDataScienceHandbook/contents/notebooks/data/california_cities.csv"
      ],
      "metadata": {
        "id": "L6s9o8_fyaAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect the file we downloaded to /tmp\n",
        "!cat /tmp/downloaded_from_gcs.txt"
      ],
      "metadata": {
        "id": "clj0PR85yZmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the file from a given Google Cloud Storage bucket.\n",
        "!gsutil cp gs://{bucket_name}/file_to_download.txt /tmp/gsutil_download.txt\n",
        "\n",
        "# Print the result to make sure the transfer worked.\n",
        "!cat /tmp/gsutil_download.txt"
      ],
      "metadata": {
        "id": "GeVYhe1YyZLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/gdrive/My Drive/foo.txt', 'w') as f:\n",
        "  f.write('Hello Google Drive!')\n",
        "!cat '/gdrive/My Drive/foo.txt'"
      ],
      "metadata": {
        "id": "e0ClwdLEyYs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can mount an entire bucket, or a path location within that bucket.\n",
        "The local path to mount it must exist."
      ],
      "metadata": {
        "id": "Q5Q3JI9LyYNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount a Cloud Storage bucket or location, without the gs:// prefix.\n",
        "mount_path = \"my-bucket\"  # or a location like \"my-bucket/path/to/mount\"\n",
        "local_path = f\"/mnt/gs/{mount_path}\"\n",
        "\n",
        "!mkdir -p {local_path}\n",
        "!gcsfuse --implicit-dirs {mount_path} {local_path}"
      ],
      "metadata": {
        "id": "schrAOPcyYNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Then you can access it like a local path.\n",
        "!ls -lh {local_path}"
      ],
      "metadata": {
        "id": "BKTI3P_uyYNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import altair as alt\n",
        "import ipywidgets as widgets\n",
        "from vega_datasets import data\n",
        "\n",
        "source = data.stocks()\n",
        "\n",
        "stock_picker = widgets.SelectMultiple(\n",
        "    options=source.symbol.unique(),\n",
        "    value=list(source.symbol.unique()),\n",
        "    description='Symbols')\n",
        "\n",
        "# The value of symbols will come from the stock_picker.\n",
        "@widgets.interact(symbols=stock_picker)\n",
        "def render(symbols):\n",
        "  selected = source[source.symbol.isin(list(symbols))]\n",
        "\n",
        "  return alt.Chart(selected).mark_line().encode(\n",
        "      x='date',\n",
        "      y='price',\n",
        "      color='symbol',\n",
        "      strokeDash='symbol',\n",
        "  )"
      ],
      "metadata": {
        "id": "EhoeLEB5yWVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import altair as alt\n",
        "import ipywidgets as widgets\n",
        "from vega_datasets import data\n",
        "\n",
        "source = data.stocks()\n",
        "\n",
        "stock_picker = widgets.SelectMultiple(\n",
        "    options=source.symbol.unique(),\n",
        "    value=list(source.symbol.unique()),\n",
        "    description='Symbols')\n",
        "\n",
        "# The value of symbols will come from the stock_picker.\n",
        "@widgets.interact(symbols=stock_picker)\n",
        "def render(symbols):\n",
        "  selected = source[source.symbol.isin(list(symbols))]\n",
        "\n",
        "  return alt.Chart(selected).mark_line().encode(\n",
        "      x='date',\n",
        "      y='price',\n",
        "      color='symbol',\n",
        "      strokeDash='symbol',\n",
        "  )"
      ],
      "metadata": {
        "id": "s5OQbbc9yV9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%javascript\n",
        "(async function() {\n",
        "  const result = await google.colab.kernel.invokeFunction(\n",
        "    'notebook.Concat', // The callback name.\n",
        "    ['hello', 'world!'], // The arguments.\n",
        "    {}); // kwargs\n",
        "  const text = result.data['application/json'];\n",
        "  document.querySelector(\"#output-area\").appendChild(document.createTextNode(text.result));\n",
        "})();"
      ],
      "metadata": {
        "id": "7SIRX2AayVdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython\n",
        "from google.colab import output\n",
        "\n",
        "display(IPython.display.HTML('''\n",
        "    The items:\n",
        "    <br><ol id=\"items\"></ol>\n",
        "    <button id='button'>Click to add</button>\n",
        "    <script>\n",
        "      document.querySelector('#button').onclick = () => {\n",
        "        google.colab.kernel.invokeFunction('notebook.AddListItem', [], {});\n",
        "      };\n",
        "    </script>\n",
        "    '''))\n",
        "\n",
        "def add_list_item():\n",
        "  # Use redirect_to_element to direct the elements which are being written.\n",
        "  with output.redirect_to_element('#items'):\n",
        "    # Use display to add items which will be persisted on notebook reload.\n",
        "    display(IPython.display.HTML('<li> Another item</li>'))\n",
        "\n",
        "output.register_callback('notebook.AddListItem', add_list_item)"
      ],
      "metadata": {
        "id": "dQJ-FYOWyVdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython\n",
        "import uuid\n",
        "from google.colab import output\n",
        "\n",
        "class InvokeButton(object):\n",
        "  def __init__(self, title, callback):\n",
        "    self._title = title\n",
        "    self._callback = callback\n",
        "\n",
        "  def _repr_html_(self):\n",
        "    callback_id = 'button-' + str(uuid.uuid4())\n",
        "    output.register_callback(callback_id, self._callback)\n",
        "\n",
        "    template = \"\"\"<button id=\"{callback_id}\">{title}</button>\n",
        "        <script>\n",
        "          document.querySelector(\"#{callback_id}\").onclick = (e) => {{\n",
        "            google.colab.kernel.invokeFunction('{callback_id}', [], {{}})\n",
        "            e.preventDefault();\n",
        "          }};\n",
        "        </script>\"\"\"\n",
        "    html = template.format(title=self._title, callback_id=callback_id)\n",
        "    return html\n",
        "\n",
        "def do_something():\n",
        "  print('here')\n",
        "\n",
        "InvokeButton('click me', do_something)"
      ],
      "metadata": {
        "id": "ZilSMUOYyVdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get -qq install -y libfluidsynth1"
      ],
      "metadata": {
        "id": "Lwf1jwyJyTHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Connect to the API and send an example message\n",
        "\n",
        "text = 'What is the velocity of an unladen swallow?' # @param {type: \"string\"}\n",
        "\n",
        "model = genai.GenerativeModel('gemini-2.0-flash')\n",
        "chat = model.start_chat(history=[])\n",
        "\n",
        "response = chat.send_message(text)\n",
        "response.text"
      ],
      "metadata": {
        "id": "7oUcpfeRyRYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "try:\n",
        "  filename = take_photo()\n",
        "  print('Saved to {}'.format(filename))\n",
        "\n",
        "  # Show the image which was just taken.\n",
        "  display(Image(filename))\n",
        "except Exception as err:\n",
        "  # Errors will be thrown if the user does not have a webcam or if they do not\n",
        "  # grant the page permission to access it.\n",
        "  print(str(err))"
      ],
      "metadata": {
        "id": "0RQz2zhKyN2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-43-_jK3rL4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VGBBbXm3rL-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0_-b4LBurOov"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46f4c0c4"
      },
      "source": [
        "# Create /warden/nano_skills/phoenix/phoenix_verify.py\n",
        "verify_file_content = \"\"\"from __future__ import annotations\n",
        "from typing import Dict, Any, List\n",
        "import orjson, hashlib\n",
        "\n",
        "from .phoenix_hcdi import deterministic_bytes, sha256_8\n",
        "\n",
        "def _sha256_hex(b: bytes) -> str:\n",
        "    return hashlib.sha256(b).hexdigest()\n",
        "\n",
        "def verify_manifest(manifest: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    report = {\n",
        "        \"schema_ok\": manifest.get(\"schema\") == \"PHOENIX_MANIFEST_V1\",\n",
        "        \"count_match\": False,\n",
        "        \"manifest_hash_ok\": False,\n",
        "        \"aggregate_payloads_hash_ok\": False,\n",
        "        \"record_issues\": [],  # list of {hcdi_name, problem}\n",
        "        \"total_records\": 0,\n",
        "        \"problems\": 0,\n",
        "    }\n",
        "\n",
        "    recs: List[Dict[str, Any]] = manifest.get(\"records\", []) or []\n",
        "    report[\"total_records\"] = len(recs)\n",
        "\n",
        "    # 1) count\n",
        "    report[\"count_match\"] = (manifest.get(\"count\") == len(recs))\n",
        "\n",
        "    # 2) recompute payloads_hash (concat raw payload digests)\n",
        "    concat = b\"\"\n",
        "    for r in recs:\n",
        "        # payload hash field is sha256:<hex>\n",
        "        ph = (r.get(\"payload_hash\") or \"\").replace(\"sha256:\", \"\")\n",
        "        if len(ph) != 64:\n",
        "            report[\"record_issues\"].append({\"hcdi_name\": r.get(\"hcdi_name\"), \"problem\": \"payload_hash_format\"})\n",
        "            continue\n",
        "        try:\n",
        "            concat += bytes.fromhex(ph)\n",
        "        except ValueError:\n",
        "            report[\"record_issues\"].append({\"hcdi_name\": r.get(\"hcdi_name\"), \"problem\": \"payload_hash_nonhex\"})\n",
        "\n",
        "    recomputed_payloads_hash = _sha256_hex(concat)\n",
        "    report[\"aggregate_payloads_hash_ok\"] = (\n",
        "        manifest.get(\"aggregate\", {}).get(\"payloads_hash\") == f\"sha256:https://blog.tensorflow.org/2022/09/automated-deployment-of-tensorflow-models-with-tensorflow-serving-and-github-actions.html{recomputed_payloads_hash}\"\n",
        "    )\n",
        "\n",
        "    # 3) recompute manifest_hash from the manifest with manifest_hash=None\n",
        "    stripped = dict(manifest)\n",
        "    agg = dict(stripped.get(\"aggregate\", {}))\n",
        "    agg[\"manifest_hash\"] = None\n",
        "    stripped[\"aggregate\"] = agg\n",
        "    recomputed_manifest_bytes = orjson.dumps(stripped, option=orjson.OPT_SORT_KEYS)\n",
        "    recomputed_manifest_hash = _sha256_hex(recomputed_manifest_bytes)\n",
        "    report[\"manifest_hash_ok\"] = (\n",
        "         manifest.get(\"aggregate\", {}).get(\"manifest_hash\") == f\"sha256:{recomputed_manifest_hash}\"\n",
        "    )\n",
        "\n",
        "\n",
        "    # 4) verify each record's content_hash and payload_hash\n",
        "    for r in recs:\n",
        "        hcdi_name = r.get(\"hcdi_name\", \"unknown\")\n",
        "        original_content_hash = r.get(\"content_hash\")\n",
        "        original_payload_hash = (r.get(\"payload_hash\") or \"\").replace(\"sha256:\", \"\")\n",
        "\n",
        "        # Check content_hash (8-char prefix)\n",
        "        # Need the original payload bytes to recompute\n",
        "        # This is a limitation - the manifest doesn't contain raw payload\n",
        "        # We can only verify the payload_hash here.\n",
        "        # A full verification would require the original record data or payload bytes.\n",
        "        # For now, we focus on what the manifest allows us to check.\n",
        "\n",
        "        # Verify payload_hash format (already done above, but add specific check here)\n",
        "        if len(original_payload_hash) != 64:\n",
        "             # Already reported, skip further checks for this record's hashes\n",
        "             continue\n",
        "\n",
        "\n",
        "        # Note: We cannot re-calculate content_hash and payload_hash here\n",
        "        # based *only* on the manifest data because the raw payload bytes\n",
        "        # are not included. The manifest relies on the hashes provided\n",
        "        # during the anchoring process.\n",
        "        # A true verification function would need access to the original\n",
        "        # data payload associated with each record in the manifest.\n",
        "        # For the scope of this `verify_manifest` function,\n",
        "        # we assume the hashes within the manifest are what we are checking.\n",
        "        # We check the format and the aggregate hash.\n",
        "        # We could add a check that the content_hash is the first 8 chars of the payload_hash,\n",
        "        # but this might not always be the design constraint (though it is in phoenix_hcdi).\n",
        "        # Let's add that check based on the phoenix_hcdi design.\n",
        "        if original_content_hash != original_payload_hash[:8]:\n",
        "             report[\"record_issues\"].append({\"hcdi_name\": hcdi_name, \"problem\": \"content_hash_mismatch_payload_hash\"})\n",
        "\n",
        "\n",
        "    report[\"problems\"] = len([k for k, v in report.items() if isinstance(v, bool) and not v]) + len(report[\"record_issues\"])\n",
        "    return report\n",
        "\n",
        "\n",
        "# --- Service Endpoint Integration (to be added to phoenix_service.py) ---\n",
        "# from fastapi import APIRouter\n",
        "# from .phoenix_verify import verify_manifest\n",
        "# from .phoenix_snapshot import make_manifest # Assuming snapshot is also available\n",
        "\n",
        "# verify_router = APIRouter()\n",
        "\n",
        "# @verify_router.post(\"/verify\")\n",
        "# async def verify_current_state():\n",
        "#     \"\"\"\n",
        "#     Verifies the integrity of the current in-memory anchored records.\n",
        "#     Creates a manifest from the current state and verifies it.\n",
        "#     \"\"\"\n",
        "#     if STATE.rdt is None:\n",
        "#          raise HTTPException(status_code=400, detail=\"Index not built. Build or load first.\")\n",
        "\n",
        "#     # Create a manifest from the current state\n",
        "#     temp_manifest = make_manifest(STATE.anchored)\n",
        "#     # The manifest hash is calculated *after* the manifest is created and serialized\n",
        "#     temp_manifest_bytes = orjson.dumps(temp_manifest, option=orjson.OPT_SORT_KEYS)\n",
        "#     manifest_hash_calculated = _sha256_hex(temp_manifest_bytes)\n",
        "#     temp_manifest[\"aggregate\"][\"manifest_hash\"] = f\"sha256:{manifest_hash_calculated}\"\n",
        "\n",
        "#     # Perform verification\n",
        "#     verification_report = verify_manifest(temp_manifest)\n",
        "\n",
        "#     if verification_report[\"problems\"] > 0:\n",
        "#          raise HTTPException(status_code=400, detail={\"status\": \"verification_failed\", \"report\": verification_report})\n",
        "#     else:\n",
        "#          return {\"status\": \"verification_succeeded\", \"report\": verification_report}\n",
        "\n",
        "# --- Pytest Integration (to be added to test_phoenix.py) ---\n",
        "# from warden.nano_skills.phoenix.phoenix_verify import verify_manifest\n",
        "\n",
        "# def test_verify_manifest_valid(anchored_records: List[LedgerRecord]):\n",
        "#      # Create a valid manifest from anchored records\n",
        "#      manifest = make_manifest(anchored_records)\n",
        "#      manifest_bytes = orjson.dumps(manifest, option=orjson.OPT_SORT_KEYS)\n",
        "#      manifest_hash_calculated = sha256_8(manifest_bytes) # Note: Using sha256_8 here, adjust if full hash needed\n",
        "#      manifest[\"aggregate\"][\"manifest_hash\"] = f\"sha256:{manifest_hash_calculated}\" # Assuming full hash for manifest hash field\n",
        "\n",
        "#      report = verify_manifest(manifest)\n",
        "#      assert report[\"schema_ok\"]\n",
        "#      assert report[\"count_match\"]\n",
        "#      assert report[\"manifest_hash_ok\"]\n",
        "#      assert report[\"aggregate_payloads_hash_ok\"]\n",
        "#      assert not report[\"record_issues\"]\n",
        "#      assert report[\"total_records\"] == len(anchored_records)\n",
        "#      assert report[\"problems\"] == 0\n",
        "#      assert report[\"status\"] == \"verification_succeeded\" # Assuming service endpoint structure\n",
        "\n",
        "# def test_verify_manifest_invalid(anchored_records: List[LedgerRecord]):\n",
        "#      # Create an invalid manifest for testing\n",
        "#      invalid_manifest = make_manifest(anchored_records)\n",
        "#      # Tamper with the manifest\n",
        "#      invalid_manifest[\"count\"] += 1 # Mismatch count\n",
        "#      if invalid_manifest[\"records\"]:\n",
        "#           invalid_manifest[\"records\"][0][\"content_hash\"] = \"invalidhash\" # Invalid content hash\n",
        "#           invalid_manifest[\"records\"][0][\"payload_hash\"] = \"sha256:invalidpayloadhash\" # Invalid payload hash\n",
        "\n",
        "#      # Recompute aggregate payloads hash based on potentially tampered payload hashes\n",
        "#      concat = b\"\"\n",
        "#      for r in invalid_manifest.get(\"records\", []):\n",
        "#          ph = (r.get(\"payload_hash\") or \"\").replace(\"sha256:\", \"\")\n",
        "#          if len(ph) == 64:\n",
        "#               try:\n",
        "#                   concat += bytes.fromhex(ph)\n",
        "#               except ValueError:\n",
        "#                   pass # Ignore non-hex for aggregate hash calculation\n",
        "\n",
        "#      recomputed_payloads_hash_for_invalid = _sha256_hex(concat)\n",
        "#      invalid_manifest[\"aggregate\"][\"payloads_hash\"] = f\"sha256:{recomputed_payloads_hash_for_invalid}\"\n",
        "\n",
        "#      # Calculate the manifest hash for the TAMPERED manifest\n",
        "#      invalid_manifest_bytes = orjson.dumps(invalid_manifest, option=orjson.OPT_SORT_KEYS)\n",
        "#      manifest_hash_calculated_for_invalid = sha256_8(invalid_manifest_bytes) # Again, adjust hash type if needed\n",
        "#      invalid_manifest[\"aggregate\"][\"manifest_hash\"] = f\"sha256:{manifest_hash_calculated_for_invalid}\"\n",
        "\n",
        "\n",
        "#      report = verify_manifest(invalid_manifest)\n",
        "\n",
        "#      assert not report[\"schema_ok\"] # Assuming tampering might affect schema if not careful\n",
        "#      assert not report[\"count_match\"]\n",
        "#      # manifest_hash_ok might be True if the hash was recalculated based on the tampered data\n",
        "#      # assert not report[\"manifest_hash_ok\"] # This assertion might fail depending on tampering and hash recalculation\n",
        "#      # aggregate_payloads_hash_ok might be True if calculated from tampered data\n",
        "#      # assert not report[\"aggregate_payloads_hash_ok\"] # This might also fail\n",
        "\n",
        "#      assert report[\"record_issues\"] # Should have issues reported\n",
        "#      assert report[\"problems\"] > 0\n",
        "#      # assert report[\"status\"] == \"verification_failed\" # Assuming service endpoint structure\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "with open('/warden/nano_skills/phoenix/phoenix_verify.py', 'w') as f:\n",
        "    f.write(verify_file_content)\n",
        "\n",
        "print(\"/warden/nano_skills/phoenix/phoenix_verify.py created.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cce0ed51"
      },
      "source": [
        "## ⚙️ Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "618e118c"
      },
      "source": [
        "# Install necessary libraries\n",
        "!pip install playwright orjson markdownify"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0f6fd64"
      },
      "source": [
        "# Install Playwright chromium browser\n",
        "!python -m playwright install chromium"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a32478d"
      },
      "source": [
        "## 📜 `gemini_export.py`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1604fde0"
      },
      "source": [
        "# Create gemini_export.py\n",
        "gemini_export_content = \"\"\"#!/usr/bin/env python3\n",
        "import argparse, time\n",
        "from pathlib import Path\n",
        "import orjson\n",
        "from markdownify import markdownify\n",
        "from playwright.sync_api import sync_playwright\n",
        "import hashlib, os, requests # Added for PHOENIX integration\n",
        "\n",
        "# Assume PHOENIX service runs locally on port 8000 by default\n",
        "PHX = os.getenv(\"PHOENIX_SERVICE_URL\", \"http://127.0.0.1:8000\")\n",
        "\n",
        "def save_json(path, obj):\n",
        "    Path(path).write_bytes(orjson.dumps(obj, option=orjson.OPT_INDENT_2))\n",
        "\n",
        "def save_md(path, messages):\n",
        "    lines = []\n",
        "    for m in messages:\n",
        "        # Basic markdown formatting for role and text\n",
        "        role_text = f\"**{m['role'].title()}:**\" if m['role'] else \"Unknown Role:\"\n",
        "        lines.append(f\"{role_text} {m['text']}\\\\n\") # Use \\\\n for newline in string literal\n",
        "\n",
        "    Path(path).write_text(\"\\\\n\".join(lines), encoding=\"utf-8\") # Join with actual newline\n",
        "\n",
        "# Added for PHOENIX integration\n",
        "def sha256_file(path: Path) -> str:\n",
        "    h = hashlib.sha256()\n",
        "    with open(path, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(1<<20), b\"\"):\n",
        "            h.update(chunk)\n",
        "    return h.hexdigest()\n",
        "\n",
        "# Added for PHOENIX integration\n",
        "def phoenix_anchor_and_snapshot(meta: dict, focus_text=\"gemini export\"):\n",
        "    print(f\"Attempting to anchor and snapshot via PHOENIX service at {PHX}...\")\n",
        "    try:\n",
        "        # 1) build/add record\n",
        "        # Add timestamp to the record payload\n",
        "        rec_payload = dict(meta) # Copy meta to add timestamp\n",
        "        rec_payload[\"timestamp\"] = time.time() # Add current timestamp\n",
        "\n",
        "        rec = [{\n",
        "            \"timestamp\": time.time(), # Add timestamp at the top level for anchor_one\n",
        "            \"semantic_path\": \"LLM/Gemini/Export\",\n",
        "            \"context_tag\": \"Anchored\",\n",
        "            \"payload\": rec_payload # Use the payload with timestamp\n",
        "        }]\n",
        "        print(f\"Attempting to add record to PHOENIX at {PHX}/add...\")\n",
        "        # try /add, fallback to /build\n",
        "        r = requests.post(f\"{PHX}/add\", json=rec, timeout=30)\n",
        "        if r.status_code != 200:\n",
        "            print(f\"'/add' failed ({r.status_code}). Attempting '/build' at {PHX}/build...\")\n",
        "            r = requests.post(f\"{PHX}/build\", json=rec, timeout=30)\n",
        "            r.raise_for_status() # Raise exception for bad status codes on build\n",
        "            print(\"'/build' successful.\")\n",
        "        else:\n",
        "            print(\"'/add' successful.\")\n",
        "\n",
        "        # Ensure the add/build request was successful before proceeding\n",
        "        r.raise_for_status()\n",
        "\n",
        "        # 2) snapshot\n",
        "        print(f\"Attempting to trigger snapshot at {PHX}/snapshot...\")\n",
        "        snap_payload = {\n",
        "            \"out_dir\":\"./phoenix_archive\", # Ensure this directory is accessible by the service\n",
        "            \"focus_text\":focus_text,\n",
        "            \"focus_k\":5\n",
        "            }\n",
        "        snap_r = requests.post(f\"{PHX}/snapshot\", json=snap_payload, timeout=60)\n",
        "        snap_r.raise_for_status()\n",
        "        snap = snap_r.json()\n",
        "        print(\"Snapshot triggered successfully.\")\n",
        "\n",
        "        # 3) verify\n",
        "        # The snapshot response should contain the manifest_path\n",
        "        manifest_path_from_snap = snap.get(\"manifest_path\")\n",
        "        if not manifest_path_from_snap:\n",
        "             print(\"Warning: Snapshot response did not contain 'manifest_path'. Skipping verification.\")\n",
        "             ver = {\"status\": \"skipped\", \"reason\": \"no manifest_path in snapshot response\"}\n",
        "        else:\n",
        "            print(f\"Attempting to verify manifest at {PHX}/verify...\")\n",
        "            ver_payload = {\"manifest_path\": manifest_path_from_snap}\n",
        "            ver_r = requests.post(f\"{PHX}/verify\", json=ver_payload, timeout=60)\n",
        "            ver_r.raise_for_status()\n",
        "            ver = ver_r.json()\n",
        "            print(\"Verification completed.\")\n",
        "\n",
        "        print(\"PHOENIX operations completed successfully.\")\n",
        "        return {\"snapshot\": snap, \"verify\": ver}\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"❌ Error communicating with PHOENIX service: {e}\")\n",
        "        # Handle specific HTTP errors if needed\n",
        "        if hasattr(e, 'response') and e.response is not None:\n",
        "            print(f\"Status Code: {e.response.status_code}\")\n",
        "            print(f\"Response Body: {e.response.text}\")\n",
        "        return {\"status\": \"phoenix_integration_failed\", \"error\": str(e)}\n",
        "    except Exception as e:\n",
        "        print(f\"❌ An unexpected error occurred during PHOENIX integration: {e}\")\n",
        "        return {\"status\": \"phoenix_integration_failed\", \"error\": str(e)}\n",
        "\n",
        "\n",
        "def export_gemini(url, outdir=\"export\"):\n",
        "    out = Path(outdir); out.mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"Attempting to export Gemini conversation from: {url}\")\n",
        "    exported_messages_count = 0\n",
        "    try:\n",
        "        with sync_playwright() as p:\n",
        "            # Ensure headless is True for headless-only\n",
        "            browser = p.chromium.launch(headless=True, timeout=60000) # 60 seconds\n",
        "            page = browser.new_page()\n",
        "            page.goto(url, wait_until=\"domcontentloaded\", timeout=60000) # Wait for content, 60 seconds\n",
        "\n",
        "            # Wait for messages to potentially load dynamically\n",
        "            # Use a minimal selector that targets message containers\n",
        "            page.wait_for_selector(\"div[data-message-id]\", timeout=30000) # Wait up to 30 seconds for a message element\n",
        "\n",
        "            # Extract messages using a more minimal selector strategy\n",
        "            messages = []\n",
        "            # Target potential message bubble containers directly if data-message-id is reliable\n",
        "            message_elements = page.query_selector_all(\"div[data-message-id]\")\n",
        "\n",
        "            if not message_elements:\n",
        "                 print(\"Warning: No message elements found with the selector 'div[data-message-id]'. Trying alternative selectors.\")\n",
        "                 # Fallback selectors - examine the Gemini shared page HTML\n",
        "                 # Use simpler role-based or common class selectors if data-message-id is not sufficient\n",
        "                 message_elements = page.query_selector_all(\"main div[role='text'], article div[role='text'], div.message-content\") # Example fallback selectors\n",
        "\n",
        "\n",
        "            if not message_elements:\n",
        "                 print(\"Error: Could not find any message elements on the page.\")\n",
        "                 return 0 # Return 0 messages exported\n",
        "\n",
        "            print(f\"Found {len(message_elements)} potential message elements.\")\n",
        "\n",
        "\n",
        "            for i, message_element in enumerate(message_elements):\n",
        "                try:\n",
        "                    # Attempt to find role (e.g., strong tag for role name) and text\n",
        "                    # This part might still require some heuristic based on the HTML structure\n",
        "                    # Let's try to find common text containers within the message element\n",
        "                    text_container = message_element.query_selector(\"div[role='text']\") or message_element # Use the message element itself as fallback\n",
        "\n",
        "                    text = text_container.inner_text().strip() if text_container else message_element.inner_text().strip()\n",
        "\n",
        "                    if not text: continue # Skip empty messages\n",
        "\n",
        "                    # Basic heuristic for user/assistant if role not explicitly found\n",
        "                    # This is fragile and based on common patterns; a better way is needed\n",
        "                    lower_text = text.lower()\n",
        "                    inferred_role = \"assistant\" if any(kw in lower_text for kw in [\"as a large language model\", \"i am a chatbot\", \"i am gemini\"]) else \"user\" # Very rough heuristic\n",
        "\n",
        "                    # Attempt to refine role based on surrounding structure or specific selectors if possible\n",
        "                    # (This is highly dependent on Gemini UI and difficult with minimal selectors)\n",
        "                    # For a truly minimal selector approach, the heuristic might be the best bet.\n",
        "\n",
        "                    messages.append({\"role\": inferred_role, \"text\": text}) # Use inferred role\n",
        "\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing message element {i}: {e}\")\n",
        "                    # Continue processing other messages even if one fails\n",
        "\n",
        "\n",
        "            exported_messages_count = len(messages)\n",
        "            if not messages:\n",
        "                 print(\"Warning: Extracted elements but failed to parse any messages.\")\n",
        "                 return 0\n",
        "\n",
        "            ts = int(time.time())\n",
        "            json_path = out/f\"gemini_{ts}.json\"\n",
        "            md_path = out/f\"gemini_{ts}.md\"\n",
        "\n",
        "            save_json(json_path, messages)\n",
        "            save_md(md_path, messages)\n",
        "\n",
        "            print(f\"✅ Exported {exported_messages_count} messages to {json_path} and {md_path}\")\n",
        "\n",
        "            # --- PHOENIX Integration ---\n",
        "            print(\"Initiating PHOENIX integration...\")\n",
        "            # Create metadata including file hashes\n",
        "            export_metadata = {\n",
        "                \"url\": url,\n",
        "                \"timestamp\": ts,\n",
        "                \"output_dir\": str(out),\n",
        "                \"json_file\": json_path.name,\n",
        "                \"md_file\": md_path.name,\n",
        "                \"messages_count\": exported_messages_count,\n",
        "                \"json_hash\": sha256_file(json_path),\n",
        "                \"md_hash\": sha256_file(md_path),\n",
        "                \"first_message_snippet\": messages[0]['text'][:100] + '...' if messages else '' # Add snippet for context\n",
        "            }\n",
        "            # Use a snippet of the first message as focus text for PHOENIX snapshot\n",
        "            phoenix_results = phoenix_anchor_and_snapshot(export_metadata, focus_text=messages[0]['text'][:50] if messages else \"Gemini Export\")\n",
        "            print(\"PHOENIX integration process finished.\")\n",
        "            print(f\"PHOENIX Results: {phoenix_results}\")\n",
        "            # --- End PHOENIX Integration ---\n",
        "\n",
        "\n",
        "            return exported_messages_count\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ An error occurred during export: {e}\")\n",
        "        # Optional: log the error\n",
        "        return 0 # Indicate export failed\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ap = argparse.ArgumentParser(description=\"Export Gemini shared conversation.\")\n",
        "    ap.add_argument(\"url\", help=\"Gemini share link (https://g.co/gemini/share/..)\")\n",
        "    ap.add_argument(\"-o\", \"--outdir\", default=\"gemini_exports\", help=\"Output directory\")\n",
        "    args = ap.parse_args()\n",
        "\n",
        "    exported_count = export_gemini(args.url, args.outdir)\n",
        "    # Exit with a non-zero code if export failed\n",
        "    if exported_count == 0:\n",
        "        exit(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4714737f"
      },
      "source": [
        "## Create `todo.json` for Scraping Tasks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97ac33a7"
      },
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "# Define the path for the todo.json file\n",
        "# Assuming a similar structure to the previous job queue template\n",
        "todo_file_path = '/content/drive/MyDrive/colab_jobs/todo_scraping.json'\n",
        "\n",
        "# Ensure the base directory exists (adjust if your base directory is different)\n",
        "base_dir = os.path.dirname(todo_file_path)\n",
        "os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "# Sample content for the todo.json file\n",
        "# Each entry represents a task (scraping a specific URL)\n",
        "todo_list_content = [\n",
        "    {\n",
        "        \"task_id\": \"scrape_tensorflow_blog_1\",\n",
        "        \"type\": \"scrape_url\",\n",
        "        \"url\": \"https://blog.tensorflow.org/2021/06/mlep-courses.html\",\n",
        "        \"status\": \"pending\",\n",
        "        \"retries\": 0,\n",
        "        \"max_retries\": 3,\n",
        "        \"output_dir\": \"tensorflow_blog_mlep\"\n",
        "    },\n",
        "    {\n",
        "        \"task_id\": \"scrape_tensorflow_blog_2\",\n",
        "        \"type\": \"scrape_url\",\n",
        "        \"url\": \"https://blog.tensorflow.org/2022/09/automated-deployment-of-tensorflow-models-with-tensorflow-serving-and-github-actions.html\",\n",
        "        \"status\": \"pending\",\n",
        "        \"retries\": 0,\n",
        "        \"max_retries\": 3,\n",
        "        \"output_dir\": \"tensorflow_blog_deployment\"\n",
        "    },\n",
        "    {\n",
        "        \"task_id\": \"scrape_example_com\",\n",
        "        \"type\": \"scrape_url\",\n",
        "        \"url\": \"https://www.example.com\",\n",
        "        \"status\": \"pending\",\n",
        "        \"retries\": 0,\n",
        "        \"max_retries\": 3,\n",
        "        \"output_dir\": \"example_website\"\n",
        "    }\n",
        "    # Add more URLs you want to scrape here\n",
        "]\n",
        "\n",
        "# Write the content to the todo.json file\n",
        "try:\n",
        "    with open(todo_file_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(todo_list_content, f, ensure_ascii=False, indent=4)\n",
        "    print(f\"Successfully created sample todo queue for scraping at: {todo_file_path}\")\n",
        "except IOError as e:\n",
        "    print(f\"Error writing todo.json file: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76379e4b"
      },
      "source": [
        "**Explanation of the `todo.json` structure:**\n",
        "\n",
        "*   **`task_id`**: A unique identifier for each scraping task.\n",
        "*   **`type`**: Specifies the type of task (here, \"scrape\\_url\"). You could add other types later (e.g., \"process\\_html\", \"download\\_images\").\n",
        "*   **`url`**: The URL to be scraped.\n",
        "*   **`status`**: The current status of the task (\"pending\", \"in\\_progress\", \"completed\", \"failed\").\n",
        "*   **`retries`**: Counter for how many times the task has been retried.\n",
        "*   **`max_retries`**: Maximum number of retries allowed before marking as \"failed\".\n",
        "*   **`output_dir`**: A suggested directory name to save the scraped data for this specific URL.\n",
        "\n",
        "You would then adapt your web scraping script to read this `todo.json` file, process \"pending\" tasks, update their status, and handle retries based on the information in the file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "318c340e"
      },
      "source": [
        "# Install necessary libraries\n",
        "!pip install requests beautifulsoup4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26f09d45"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "\n",
        "def scrape_website(url, output_dir=\"scraped_data\"):\n",
        "    \"\"\"\n",
        "    Basic web scraper to fetch text and image URLs from a given URL.\n",
        "\n",
        "    Args:\n",
        "        url (str): The URL of the webpage to scrape.\n",
        "        output_dir (str): Directory to save scraped data (not implemented in this basic version).\n",
        "    \"\"\"\n",
        "    print(f\"Attempting to scrape: {url}\")\n",
        "\n",
        "    try:\n",
        "        # Send a GET request to the URL\n",
        "        response = requests.get(url)\n",
        "\n",
        "        # Check if the request was successful (status code 200)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # Parse the HTML content of the page\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # --- Extract Text ---\n",
        "        # Get all text from the page, removing script and style elements\n",
        "        for script_or_style in soup([\"script\", \"style\"]):\n",
        "            script_or_style.extract()\n",
        "        text = soup.get_text(separator=' ', strip=True)\n",
        "        print(\"\\\\n--- Extracted Text ---\")\n",
        "        print(text[:500] + '...' if len(text) > 500 else text) # Print first 500 chars\n",
        "\n",
        "        # --- Extract Image URLs ---\n",
        "        img_urls = []\n",
        "        for img in soup.find_all('img'):\n",
        "            img_url = img.get('src')\n",
        "            if img_url:\n",
        "                # Make the URL absolute if it's relative\n",
        "                if not img_url.startswith('http') and not img_url.startswith('//'):\n",
        "                    img_url = requests.compat.urljoin(url, img_url)\n",
        "                img_urls.append(img_url)\n",
        "\n",
        "        print(\"\\\\n--- Extracted Image URLs ---\")\n",
        "        if img_urls:\n",
        "            for img_url in img_urls:\n",
        "                print(img_url)\n",
        "        else:\n",
        "            print(\"No image URLs found.\")\n",
        "\n",
        "        # --- Example: Saving data (requires more logic) ---\n",
        "        # You would typically save the text and image data to files\n",
        "        # os.makedirs(output_dir, exist_ok=True)\n",
        "        # with open(os.path.join(output_dir, \"text.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "        #     f.write(text)\n",
        "        # with open(os.path.join(output_dir, \"image_urls.txt\"), \"w\") as f:\n",
        "        #     for img_url in img_urls:\n",
        "        #         f.write(img_url + \"\\\\n\")\n",
        "        # print(f\"Text and image URLs extracted from {url}\")\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching the URL {url}: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during scraping: {e}\")\n",
        "\n",
        "# --- Example Usage ---\n",
        "# Replace with the URL you want to scrape\n",
        "# scrape_website(\"https://blog.tensorflow.org/\")\n",
        "# scrape_website(\"https://www.example.com\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7da255ba"
      },
      "source": [
        "import os\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs('/warden/nano_skills/phoenix/', exist_ok=True)\n",
        "\n",
        "# Content for __init__.py\n",
        "init_content = \"\"\"# Intentionally empty; marks this directory as a package.\n",
        "\"\"\"\n",
        "\n",
        "# Content for phoenix_hcdi.py (using content from previous turns)\n",
        "hcdi_content = \"\"\"from __future__ import annotations\n",
        "import hashlib, time\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict\n",
        "import orjson\n",
        "\n",
        "__all__ = [\n",
        "    \"LedgerRecord\",\n",
        "    \"deterministic_bytes\",\n",
        "    \"sha256_8\",\n",
        "    \"build_hcdi_name\",\n",
        "    \"anchor_one\",\n",
        "]\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class LedgerRecord:\n",
        "    timestamp: float\n",
        "    semantic_path: str          # e.g., \"System/Git/Commit\"\n",
        "    context_tag: str            # e.g., \"Executed\"\n",
        "    payload: Dict[str, Any]     # deterministic JSON content\n",
        "    content_hash: str           # 8-char SHA-256 prefix\n",
        "    hcdi_name: str              # VSF_V01_PATH-Context_HASH_TYPE.json\n",
        "\n",
        "def deterministic_bytes(obj: Any) -> bytes:\n",
        "    # Deterministic, canonical JSON for hashing & audit\n",
        "    return orjson.dumps(obj, option=orjson.OPT_SORT_KEYS)\n",
        "\n",
        "def sha256_8(b: bytes) -> str:\n",
        "    return hashlib.sha256(b).hexdigest()[:8]\n",
        "\n",
        "def build_hcdi_name(semantic_path: str, context_tag: str, content_hash: str, typ: str=\"json\") -> str:\n",
        "    # Example: VSF_V01_System-Git-Commit-Executed_1a2b3c4d_json.json\n",
        "    path_token = semantic_path.replace(\"/\", \"-\")\n",
        "    return f\"VSF_V01_{path_token}-{context_tag}_{content_hash}_{typ}.json\"\n",
        "\n",
        "def anchor_one(raw: Dict[str, Any]) -> LedgerRecord:\n",
        "    payload_bytes = deterministic_bytes(raw[\"payload\"])\n",
        "    h8 = sha256_8(payload_bytes)\n",
        "    name = build_hcdi_name(raw[\"semantic_path\"],\n",
        "                           raw[\"context_tag\"],\n",
        "                           h8)\n",
        "    return LedgerRecord(\n",
        "        timestamp=raw[\"timestamp\"],\n",
        "        semantic_path=raw[\"semantic_path\"],\n",
        "        context_tag=raw[\"context_tag\"],\n",
        "        payload=raw[\"payload\"],\n",
        "        content_hash=h8,\n",
        "        hcdi_name=name\n",
        "    )\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Content for phoenix_rdt.py (using content from previous turns)\n",
        "rdt_content = \"\"\"from __future__ import annotations\n",
        "import os\n",
        "import torch\n",
        "import faiss\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from typing import List, Dict, Any, Tuple\n",
        "\n",
        "__all__ = [\n",
        "    \"USE_FEATURE_HASH_EMBEDDER\",\n",
        "    \"FeatureHashEmbedder\",\n",
        "    \"SemanticFilter\",\n",
        "    \"LedgerRecord\", # Re-export for convenience\n",
        "]\n",
        "\n",
        "# Use a deterministic hash embedder when offline or requested\n",
        "USE_FEATURE_HASH_EMBEDDER = os.environ.get(\"PHOENIX_FEATURE_HASH_EMBEDDER\", \"\").lower() == \"true\"\n",
        "\n",
        "# ---------- Mock / Deterministic Embedder for Offline Testing ----------\n",
        "class FeatureHashEmbedder:\n",
        "    \"\"\"\n",
        "    A deterministic embedder using feature hashing, useful for offline testing\n",
        "    or when no GPU/SentenceTransformer is available. Not semantically meaningful.\n",
        "    \"\"\"\n",
        "    def __init__(self, output_dim: int = 384, num_hashes: int = 1024):\n",
        "        self.output_dim = output_dim\n",
        "        self.num_hashes = num_hashes\n",
        "        self.seed = 42 # Deterministic hash seed\n",
        "\n",
        "    def encode(self, sentences: List[str], convert_to_numpy: bool = True) -> np.ndarray:\n",
        "        embeddings = []\n",
        "        for sentence in sentences:\n",
        "            # Use a simple, deterministic hash for each sentence\n",
        "            hash_val = hash(f\"{self.seed}:{sentence}\")\n",
        "            # Map hash to a fixed-size vector (very basic feature hashing concept)\n",
        "            vec = np.zeros(self.output_dim, dtype=np.float32)\n",
        "            for i in range(self.output_dim):\n",
        "                vec[i] = (hash_val % (i + 1) - (self.output_dim // 2)) / self.output_dim # Simple deterministic value based on hash\n",
        "            embeddings.append(vec)\n",
        "        return np.array(embeddings, dtype=np.float32) if convert_to_numpy else torch.tensor(embeddings, dtype=torch.float32)\n",
        "\n",
        "# ---------- Real Semantic Embedder (SentenceTransformer) ----------\n",
        "class SemanticEmbedder:\n",
        "    \"\"\"\n",
        "    Uses a SentenceTransformer model for semantic embeddings.\n",
        "    Requires internet access to download the model the first time.\n",
        "    \"\"\"\n",
        "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
        "        try:\n",
        "            self.model = SentenceTransformer(model_name)\n",
        "            self.is_available = True\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: SentenceTransformer model '{model_name}' could not be loaded ({e}). \"\n",
        "                  \"Semantic filtering will use the fallback FeatureHashEmbedder.\")\n",
        "            self.model = FeatureHashEmbedder(output_dim=384) # Fallback\n",
        "            self.is_available = False\n",
        "\n",
        "    def encode(self, sentences: List[str], convert_to_numpy: bool = True) -> np.ndarray:\n",
        "        return self.model.encode(sentences, convert_to_numpy=convert_to_numpy, show_progress_bar=False)\n",
        "\n",
        "\n",
        "# ---------- Semantic Filter (FAISS) ----------\n",
        "class SemanticFilter:\n",
        "    \"\"\"\n",
        "    Performs nearest-neighbor search on embeddings using FAISS (CPU or GPU).\n",
        "    \"\"\"\n",
        "    def __init__(self, documents: List[Dict[str, Any]], embedder):\n",
        "        self.documents = documents\n",
        "        self.embedder = embedder\n",
        "        self.index = None\n",
        "        self.document_map = {doc['hcdi_name']: i for i, doc in enumerate(documents)}\n",
        "        self._build_index()\n",
        "\n",
        "    def _build_index(self):\n",
        "        print(f\"Building semantic index for {len(self.documents)} documents...\")\n",
        "        if not self.documents:\n",
        "            self.index = None\n",
        "            print(\"No documents to index.\")\n",
        "            return\n",
        "\n",
        "        texts = [doc['payload'].get('text', '') for doc in self.documents] # Assuming 'text' is in payload\n",
        "        if not any(texts):\n",
        "             print(\"Warning: No 'text' field found in document payloads. Building index on empty data.\")\n",
        "             texts = [\"\"] * len(self.documents) # Index empty strings if no text found\n",
        "\n",
        "        embeddings = self.embedder.encode(texts)\n",
        "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
        "\n",
        "        dimension = embeddings.shape[1]\n",
        "        # Check for GPU availability and choose FAISS index type\n",
        "        if faiss.get_gpus() > 0 and not USE_FEATURE_HASH_EMBEDDER:\n",
        "            print(\"Using FAISS GPU index.\")\n",
        "            res = faiss.StandardGpuResources()\n",
        "            index_flat = faiss.IndexFlatL2(dimension) # L2 distance\n",
        "            self.index = faiss.index_cpu_to_gpu(res, 0, index_flat)\n",
        "        else:\n",
        "            print(\"Using FAISS CPU index.\")\n",
        "            self.index = faiss.IndexFlatL2(dimension) # L2 distance\n",
        "\n",
        "        self.index.add(embeddings)\n",
        "        print(\"FAISS index built.\")\n",
        "\n",
        "    def find_similar(self, query_text: str, k: int = 5) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Finds the k most semantically similar documents to the query.\n",
        "        Returns a list of documents with added 'distance' and 'score' (inverse distance).\n",
        "        \"\"\"\n",
        "        if self.index is None or self.index.ntotal == 0:\n",
        "            print(\"Index is empty or not built.\")\n",
        "            return []\n",
        "\n",
        "        query_embedding = self.embedder.encode([query_text], convert_to_numpy=True)\n",
        "        distances, indices = self.index.search(query_embedding, k)\n",
        "\n",
        "        results = []\n",
        "        for i, doc_idx in enumerate(indices[0]):\n",
        "            if doc_idx == -1: # FAISS returns -1 for empty slots if k > ntotal\n",
        "                continue\n",
        "            doc = self.documents[doc_idx].copy() # Copy to add results\n",
        "            distance = distances[0][i]\n",
        "            # Simple scoring: inverse of distance (handle distance=0 to avoid division by zero)\n",
        "            score = 1.0 / (distance + 1e-6) # Add small epsilon\n",
        "            doc['distance'] = float(distance)\n",
        "            doc['score'] = float(score)\n",
        "            results.append(doc)\n",
        "\n",
        "        # Sort by score (descending)\n",
        "        results.sort(key=lambda x: x.get('score', 0), reverse=True)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def filter_by_hcdi_names(self, hcdi_names: List[str]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Retrieves documents based on their HCDI names.\n",
        "        \"\"\"\n",
        "        return [self.documents[self.document_map[name]]\n",
        "                for name in hcdi_names if name in self.document_map]\n",
        "\n",
        "# Re-export LedgerRecord for use with SemanticFilter\n",
        "from .phoenix_hcdi import LedgerRecord\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Write the files\n",
        "with open('/warden/nano_skills/phoenix/__init__.py', 'w') as f:\n",
        "    f.write(init_content)\n",
        "print(\"/warden/nano_skills/phoenix/__init__.py created.\")\n",
        "\n",
        "with open('/warden/nano_skills/phoenix/phoenix_hcdi.py', 'w') as f:\n",
        "    f.write(hcdi_content)\n",
        "print(\"/warden/nano_skills/phoenix/phoenix_hcdi.py created.\")\n",
        "\n",
        "with open('/warden/nano_skills/phoenix/phoenix_rdt.py', 'w') as f:\n",
        "    f.write(rdt_content)\n",
        "print(\"/warden/nano_skills/phoenix/phoenix_rdt.py created.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2c35dbd9"
      },
      "source": [
        "# Create /warden/nano_skills/phoenix/phoenix_snapshot.py\n",
        "snapshot_file_content = \"\"\"from __future__ import annotations\n",
        "import os, time, tempfile, hashlib\n",
        "from typing import List, Dict, Any\n",
        "import orjson\n",
        "\n",
        "from .phoenix_hcdi import LedgerRecord, deterministic_bytes, sha256_8\n",
        "\n",
        "__all__ = [\"make_manifest\", \"atomic_write\", \"hash_bytes\"]\n",
        "\n",
        "def hash_bytes(b: bytes) -> str:\n",
        "    return hashlib.sha256(b).hexdigest()\n",
        "\n",
        "def atomic_write(path: str, data: bytes) -> None:\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    with tempfile.NamedTemporaryFile(\"wb\", dir=os.path.dirname(path), delete=False) as tmp:\n",
        "        tmp.write(data)\n",
        "        tmp.flush()\n",
        "        os.fsync(tmp.fileno())\n",
        "        tmp_path = tmp.name\n",
        "    os.replace(tmp_path, path)\n",
        "\n",
        "def make_manifest(records: List[LedgerRecord]) -> Dict[str, Any]:\n",
        "    \\\"\\\"\\\"\n",
        "    MANIFEST schema (minimal, deterministic, stable):\n",
        "    {\n",
        "      \"schema\": \"PHOENIX_MANIFEST_V1\",\n",
        "      \"created_at\": 1735941000.123,\n",
        "      \"count\": N,\n",
        "      \"records\": [\n",
        "        {\n",
        "          \"ts\": 1735940999.5,\n",
        "          \"hcdi_name\": \"...\",\n",
        "          \"semantic_path\": \"System/Git/Commit\",\n",
        "          \"context_tag\": \"Executed\",\n",
        "          \"content_hash\": \"1a2b3c4d\",\n",
        "          \"payload_hash\": \"sha256:abcd...ef\",\n",
        "        }, ...\n",
        "      ],\n",
        "      \"aggregate\": {\n",
        "        \"payloads_hash\": \"sha256:....\",\n",
        "        \"manifest_hash\": \"sha256:....\"  # set by caller after serialization\n",
        "      }\n",
        "    }\n",
        "    \\\"\\\"\\\"\n",
        "    payload_hashes_concatenated = b\"\"\n",
        "    items = []\n",
        "    for r in records:\n",
        "        pbytes = deterministic_bytes(r.payload)\n",
        "        ph = hash_bytes(pbytes)\n",
        "        items.append({\n",
        "            \"ts\": float(r.timestamp),\n",
        "            \"hcdi_name\": r.hcdi_name,\n",
        "            \"semantic_path\": r.semantic_path,\n",
        "            \"context_tag\": r.context_tag,\n",
        "            \"content_hash\": r.content_hash,\n",
        "            \"payload_hash\": f\"sha256:{ph}\",\n",
        "        })\n",
        "        payload_hashes_concatenated += ph.encode('utf-8')\n",
        "\n",
        "    aggregate_payloads_hash = hash_bytes(payload_hashes_concatenated)\n",
        "\n",
        "    manifest = {\n",
        "        \"schema\": \"PHOENIX_MANIFEST_V1\",\n",
        "        \"created_at\": time.time(), # Use current time for creation timestamp\n",
        "        \"count\": len(records),\n",
        "        \"records\": items,\n",
        "        \"aggregate\": {\n",
        "            \"payloads_hash\": f\"sha256:{aggregate_payloads_hash}\",\n",
        "            \"manifest_hash\": \"sha256:\" # Placeholder, set after serialization\n",
        "        }\n",
        "    }\n",
        "    return manifest\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "with open('/warden/nano_skills/phoenix/phoenix_snapshot.py', 'w') as f:\n",
        "    f.write(snapshot_file_content)\n",
        "\n",
        "print(\"/warden/nano_skills/phoenix/phoenix_snapshot.py created.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38eb2216"
      },
      "source": [
        "# Create /warden/nano_skills/phoenix/phoenix_service.py\n",
        "service_file_content = \"\"\"from __future__ import annotations\n",
        "import os\n",
        "import asyncio\n",
        "from typing import List, Dict, Any\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "from .phoenix_hcdi import LedgerRecord, anchor_one\n",
        "from .phoenix_rdt import RDTIndex\n",
        "\n",
        "app = FastAPI(title=\"PHOENIX RDT Service\", version=\"1.0.0\")\n",
        "\n",
        "# --- In-memory singleton state ---\n",
        "class _State:\n",
        "    def __init__(self):\n",
        "        self.lock = asyncio.Lock()\n",
        "        self.rdt: RDTIndex | None = None\n",
        "        self.anchored: List[LedgerRecord] = []\n",
        "\n",
        "STATE = _State()\n",
        "\n",
        "# --- Schemas ---\n",
        "class RawRecord(BaseModel):\n",
        "    timestamp: float | None = None\n",
        "    semantic_path: str\n",
        "    context_tag: str\n",
        "    payload: Dict[str, Any]\n",
        "\n",
        "class QueryResponse(BaseModel):\n",
        "    prompt_block: str\n",
        "    hits: int\n",
        "\n",
        "# --- Helpers ---\n",
        "def _anchor_many(raw_records: List[RawRecord]) -> List[LedgerRecord]:\n",
        "    out: List[LedgerRecord] = []\n",
        "    for r in raw_records:\n",
        "        out.append(anchor_one(r.model_dump()))\n",
        "    return out\n",
        "\n",
        "def _format_block(hits: List[tuple[float, LedgerRecord]]) -> str:\n",
        "    lines = []\n",
        "    for dist, rec in hits:\n",
        "        lines.append(\n",
        "            f\"[{rec.hcdi_name}] ({rec.semantic_path} | {rec.context_tag} | d={dist:.4f})\\n\"\n",
        "            f\"{rec.payload}\\n\"\n",
        "            \"---\"\n",
        "        )\n",
        "    return \"\\\\n\".join(lines)\n",
        "\n",
        "# --- Routes ---\n",
        "@app.get(\"/health\")\n",
        "async def health():\n",
        "    mode = \"offline\" if os.getenv(\"PHOENIX_RDT_OFFLINE\", \"0\") == \"1\" else \"auto-GPU\"\n",
        "    status = \"ready\" if STATE.rdt is not None else \"initializing\"\n",
        "    return {\"status\": status, \"mode\": mode, \"anchored_records\": len(STATE.anchored)}\n",
        "\n",
        "@app.post(\"/build\")\n",
        "async def build_index(records: List[RawRecord]):\n",
        "    \"\"\"\n",
        "    Builds a new RDT index from a list of raw records.\n",
        "    Overwrites any existing index.\n",
        "    \"\"\"\n",
        "    async with STATE.lock:\n",
        "        print(f\"Building index with {len(records)} records...\")\n",
        "        STATE.anchored = _anchor_many(records)\n",
        "        STATE.rdt = RDTIndex(STATE.anchored)\n",
        "        print(\"Index built.\")\n",
        "        return {\"status\": \"success\", \"indexed_records\": len(STATE.anchored)}\n",
        "\n",
        "@app.post(\"/add\")\n",
        "async def add_records(records: List[RawRecord]):\n",
        "    \"\"\"\n",
        "    Adds records incrementally to the existing RDT index.\n",
        "    Requires the index to be built first via /build.\n",
        "    \"\"\"\n",
        "    if STATE.rdt is None:\n",
        "        raise HTTPException(status_code=400, detail=\"Index not built. Call /build first.\")\n",
        "\n",
        "    async with STATE.lock:\n",
        "        print(f\"Adding {len(records)} records...\")\n",
        "        new_anchored = _anchor_many(records)\n",
        "        STATE.anchored.extend(new_anchored) # Add to our in-memory list\n",
        "        STATE.rdt.add_documents(new_anchored) # Add to the FAISS index\n",
        "        print(f\"Added records. Total indexed: {len(STATE.anchored)}\")\n",
        "        return {\"status\": \"success\", \"indexed_records\": len(STATE.anchored)}\n",
        "\n",
        "@app.get(\"/query\", response_model=QueryResponse)\n",
        "async def query_index(text: str, k: int = 5):\n",
        "    \"\"\"\n",
        "    Queries the RDT index for the k most similar documents to the query text.\n",
        "    \"\"\"\n",
        "    if STATE.rdt is None:\n",
        "        raise HTTPException(status_code=400, detail=\"Index not built. Call /build first.\")\n",
        "\n",
        "    async with STATE.lock:\n",
        "        # Find similar uses the embedder and FAISS index\n",
        "        results = STATE.rdt.find_similar(query_text=text, k=k)\n",
        "\n",
        "    # Format results into a prompt-friendly block\n",
        "    prompt_block = _format_block([(hit['distance'], LedgerRecord(**hit)) for hit in results])\n",
        "\n",
        "    return QueryResponse(\n",
        "        prompt_block=prompt_block,\n",
        "        hits=len(results)\n",
        "    )\n",
        "\n",
        "# Optional: Add startup event to build an initial index if needed\n",
        "# @app.on_event(\"startup\")\n",
        "# async def startup_event():\n",
        "#     print(\"Starting up...\")\n",
        "#     # Example: Load initial data from a file and build index on startup\n",
        "#     # try:\n",
        "#     #     with open(\"initial_data.json\", \"r\") as f:\n",
        "#     #         initial_records_raw = [RawRecord(**rec) for rec in orjson.loads(f.read())]\n",
        "#     #     await build_index(initial_records_raw)\n",
        "#     # except FileNotFoundError:\n",
        "#     #     print(\"No initial_data.json found, starting with empty index.\")\n",
        "#     print(\"Startup complete.\")\n",
        "\n",
        "# To run this service with uvicorn:\n",
        "# uvicorn warden.nano_skills.phoenix.phoenix_service:app --reload\n",
        "\"\"\"\n",
        "\n",
        "with open('/warden/nano_skills/phoenix/phoenix_service.py', 'w') as f:\n",
        "    f.write(service_file_content)\n",
        "\n",
        "print(\"/warden/nano_skills/phoenix/phoenix_service.py created.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81368cb1"
      },
      "source": [
        "# Create /warden/nano_skills/phoenix/phoenix_cli.py\n",
        "cli_file_content = \"\"\"import argparse\n",
        "import asyncio\n",
        "import requests\n",
        "import json\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Assuming the service runs locally on port 8000\n",
        "SERVICE_URL = os.environ.get(\"PHOENIX_RDT_SERVICE_URL\", \"http://127.0.0.1:8000\")\n",
        "\n",
        "def build_index(file_path: str):\n",
        "    \"\"\"Builds the RDT index from a JSON file containing a list of raw records.\"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r') as f:\n",
        "            records = json.load(f)\n",
        "        if not isinstance(records, list):\n",
        "            raise ValueError(\"JSON file must contain a list of records.\")\n",
        "        print(f\"Sending {len(records)} records to build index...\")\n",
        "        response = requests.post(f\"{SERVICE_URL}/build\", json=records)\n",
        "        response.raise_for_status() # Raise an exception for bad status codes\n",
        "        print(\"Index built successfully:\", response.json())\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {file_path}\", file=sys.stderr)\n",
        "        sys.exit(1)\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error: Invalid JSON in file {file_path}\", file=sys.stderr)\n",
        "        sys.exit(1)\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error communicating with service: {e}\", file=sys.stderr)\n",
        "        sys.exit(1)\n",
        "    except ValueError as e:\n",
        "        print(f\"Data error: {e}\", file=sys.stderr)\n",
        "        sys.exit(1)\n",
        "\n",
        "def add_records(file_path: str):\n",
        "    \"\"\"Adds records from a JSON file to the existing RDT index.\"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r') as f:\n",
        "            records = json.load(f)\n",
        "        if not isinstance(records, list):\n",
        "            raise ValueError(\"JSON file must contain a list of records.\")\n",
        "        print(f\"Sending {len(records)} records to add to index...\")\n",
        "        response = requests.post(f\"{SERVICE_URL}/add\", json=records)\n",
        "        response.raise_for_status() # Raise an exception for bad status codes\n",
        "        print(\"Records added successfully:\", response.json())\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {file_path}\", file=sys.stderr)\n",
        "        sys.exit(1)\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error: Invalid JSON in file {file_path}\", file=sys.stderr)\n",
        "        sys.exit(1)\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error communicating with service: {e}\", file=sys.stderr)\n",
        "        sys.exit(1)\n",
        "    except ValueError as e:\n",
        "        print(f\"Data error: {e}\", file=sys.stderr)\n",
        "        sys.exit(1)\n",
        "\n",
        "\n",
        "def query_index(text: str, k: int):\n",
        "    \"\"\"Queries the RDT service for similar documents.\"\"\"\n",
        "    try:\n",
        "        print(f\"Querying for '{text}' (k={k})...\")\n",
        "        params = {\"text\": text, \"k\": k}\n",
        "        response = requests.get(f\"{SERVICE_URL}/query\", params=params)\n",
        "        response.raise_for_status() # Raise an exception for bad status codes\n",
        "        result = response.json()\n",
        "        print(\"\\\\n--- Prompt Block ---\\\\n\")\n",
        "        print(result[\"prompt_block\"])\n",
        "        print(f\"\\\\nHits: {result['hits']}\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error communicating with service: {e}\", file=sys.stderr)\n",
        "        sys.exit(1)\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description=\"PHOENIX RDT CLI\")\n",
        "    subparsers = parser.add_subparsers(dest=\"command\", help=\"Available commands\")\n",
        "\n",
        "    # Build command\n",
        "    build_parser = subparsers.add_parser(\"build\", help=\"Build the RDT index from a JSON file\")\n",
        "    build_parser.add_argument(\"file\", help=\"Path to the JSON file containing raw records\")\n",
        "\n",
        "    # Add command\n",
        "    add_parser = subparsers.add_parser(\"add\", help=\"Add records to the existing RDT index from a JSON file\")\n",
        "    add_parser.add_argument(\"file\", help=\"Path to the JSON file containing raw records\")\n",
        "\n",
        "    # Query command\n",
        "    query_parser = subparsers.add_parser(\"query\", help=\"Query the RDT index\")\n",
        "    query_parser.add_argument(\"text\", help=\"Query text\")\n",
        "    query_parser.add_argument(\"-k\", type=int, default=5, help=\"Number of similar documents to retrieve\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if args.command == \"build\":\n",
        "        build_index(args.file)\n",
        "    elif args.command == \"add\":\n",
        "        add_records(args.file)\n",
        "    elif args.command == \"query\":\n",
        "        query_index(args.text, args.k)\n",
        "    else:\n",
        "        parser.print_help()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\"\"\"\n",
        "\n",
        "with open('/warden/nano_skills/phoenix/phoenix_cli.py', 'w') as f:\n",
        "    f.write(cli_file_content)\n",
        "\n",
        "print(\"/warden/nano_skills/phoenix/phoenix_cli.py created.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56d37334"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt failed because I tried to put markdown content directly into a `code_block` command, which is incorrect. I need to use a markdown cell to write the explanation. I will regenerate the markdown content for the explanation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKHwmkYKnmxy"
      },
      "source": [
        "# This subtask involves explaining concepts, not writing code.\n",
        "# I will use a markdown cell to provide the explanation."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d212e5c"
      },
      "source": [
        "**Reasoning**:\n",
        "Provide the detailed explanation of text data cleaning and preprocessing for topic modeling in a markdown cell.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0937b88"
      },
      "source": [
        "# Explanation of text data cleaning and preprocessing for topic modeling\n",
        "\n",
        "## Text Data Cleaning and Preprocessing Steps for Topic Modeling\n",
        "\n",
        "Preparing text data for topic modeling is crucial to ensure accurate and meaningful results. The typical steps involved are:\n",
        "\n",
        "1.  **Removing Punctuation:** Eliminating characters like periods, commas, question marks, etc.\n",
        "2.  **Converting Text to Lowercase:** Standardizing the text by converting all characters to lowercase.\n",
        "3.  **Removing Stop Words:** Removing common words that do not carry significant meaning (e.g., \"the\", \"a\", \"is\").\n",
        "4.  **Tokenization:** Breaking down the text into individual words or tokens.\n",
        "5.  **Stemming or Lemmatization (Optional):** Reducing words to their root form (stemming) or their base form (lemmatization). While not strictly required for all topic modeling techniques, it can sometimes improve results by grouping variations of the same word.\n",
        "\n",
        "## Purpose of Each Step\n",
        "\n",
        "*   **Removing Punctuation:** Punctuation marks generally do not contribute to the meaning of words and can introduce noise. Removing them helps in focusing on the words themselves.\n",
        "*   **Converting Text to Lowercase:** This step ensures that the same word with different capitalization is treated as the same word (e.g., \"Topic\" and \"topic\"). This is important for accurate frequency counts and topic identification.\n",
        "*   **Removing Stop Words:** Stop words are highly frequent words that appear in almost all documents and do not help in distinguishing between topics. Removing them reduces the dimensionality of the data and helps the topic model focus on more informative words.\n",
        "*   **Tokenization:** This is a fundamental step that breaks the continuous text into discrete units (tokens) which are the basic building blocks for further analysis and processing by topic modeling algorithms.\n",
        "*   **Stemming or Lemmatization:** These techniques aim to reduce word variations to a common base form. For example, \"running,\" \"runs,\" and \"ran\" might all be reduced to \"run.\" This can help in grouping related words and improving the coherence of identified topics. Lemmatization is generally preferred over stemming as it produces actual words, whereas stemming might produce non-words.\n",
        "\n",
        "## Python Libraries for Text Preprocessing\n",
        "\n",
        "Several powerful Python libraries are available for text preprocessing tasks:\n",
        "\n",
        "*   **NLTK (Natural Language Toolkit):** A comprehensive library providing a wide range of tools for text processing, including tokenization, stop word removal, stemming, and lemmatization.\n",
        "*   **spaCy:** An efficient library for advanced natural language processing tasks, including tokenization, lemmatization, and part-of-speech tagging. It is generally faster and more memory-efficient than NLTK for large datasets.\n",
        "*   **scikit-learn:** While primarily a machine learning library, scikit-learn provides utilities for text feature extraction, such as `CountVectorizer` and `TfidfVectorizer`, which can perform tokenization and stop word removal.\n",
        "\n",
        "## Preprocessing Considerations for Specific Data Sources\n",
        "\n",
        "The specific data sources (GitHub issues, Notion pages, communication platform messages) might require some tailored preprocessing:\n",
        "\n",
        "*   **GitHub Issues:** May contain code snippets, links, mentions (@users), and issue numbers (#numbers). It might be necessary to remove or handle these specific elements during preprocessing to avoid them interfering with topic modeling.\n",
        "*   **Notion Pages:** Can contain various formatting elements, links, and potentially embedded content. Extracting the plain text content is crucial, and then similar considerations as GitHub issues regarding links and mentions might apply.\n",
        "*   **Communication Platform Messages:** Often contain informal language, emojis, mentions (@users), hashtags (#topics), and links. Cleaning might involve handling emojis, removing mentions and hashtags if they are not relevant to the topic, and addressing informal language or slang.\n",
        "\n",
        "In all cases, a preliminary exploration of the data is recommended to identify any domain-specific noise or patterns that need to be addressed during the preprocessing stage."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eb89af3"
      },
      "source": [
        "## Topic modeling\n",
        "\n",
        "### Subtask:\n",
        "Apply a topic modeling technique (e.g., Latent Dirichlet Allocation (LDA) or Non-negative Matrix Factorization (NMF)) to the preprocessed text data to identify underlying topics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40d4384f"
      },
      "source": [
        "**Reasoning**:\n",
        "Explain the concepts of topic modeling and the chosen techniques, then demonstrate how to apply one using scikit-learn with a dummy dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e54ce14"
      },
      "source": [
        "# 1. Explain the concept of topic modeling and its purpose\n",
        "print(\"## Topic Modeling Explained\")\n",
        "print(\"Topic modeling is a type of statistical modeling for discovering the abstract 'topics' that occur in a collection of documents.\")\n",
        "print(\"In the context of integrating tools, topic modeling can help to automatically categorize and understand the main themes present in data from various sources like GitHub issues, Notion pages, and communication platforms.\")\n",
        "print(\"This allows for insights into common issues, project areas, or discussion points across different tools, enabling better organization, analysis, and potentially automation.\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# 2. Describe two common topic modeling techniques\n",
        "print(\"## Common Topic Modeling Techniques\")\n",
        "print(\"### Latent Dirichlet Allocation (LDA)\")\n",
        "print(\"LDA is a generative statistical model that assumes each document is a mixture of topics and each topic is a mixture of words.\")\n",
        "print(\"It works by trying to find the word-topic and document-topic distributions that best explain the observed data (the words in the documents).\")\n",
        "print(\"\\n\")\n",
        "print(\"### Non-negative Matrix Factorization (NMF)\")\n",
        "print(\"NMF is a linear algebraic technique that factorizes a non-negative matrix (like a document-term matrix) into two lower-rank non-negative matrices.\")\n",
        "print(\"One matrix represents the document-topic distribution, and the other represents the topic-word distribution.\")\n",
        "print(\"NMF is often more computationally efficient than LDA and can sometimes produce more interpretable topics, especially for shorter texts.\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# 3. Discuss factors for choosing a technique\n",
        "print(\"## Factors for Choosing a Topic Modeling Technique\")\n",
        "print(\"- **Nature of the data:** LDA is probabilistic and often works well with longer documents. NMF is algebraic and can be effective for shorter texts or when interpretability is a priority.\")\n",
        "print(\"- **Computational resources:** NMF is generally faster and requires less memory than LDA.\")\n",
        "print(\"- **Interpretability of results:** Both can produce interpretable topics, but some find NMF results slightly easier to understand directly from the word loadings.\")\n",
        "print(\"- **Prior knowledge:** If you have some prior knowledge about the potential topics, you might be able to fine-tune the models accordingly.\")\n",
        "print(\"For this demonstration, we will use NMF due to its relative simplicity and efficiency for example purposes.\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# 4. Apply one technique using scikit-learn\n",
        "print(\"## Applying Non-negative Matrix Factorization (NMF) with scikit-learn\")\n",
        "print(\"Here's a general outline of the steps involved:\")\n",
        "print(\"1. Create a document-term matrix: This matrix represents the frequency of words in each document. `TfidfVectorizer` from scikit-learn is a common choice, which also weights words based on their importance across documents.\")\n",
        "print(\"2. Choose the number of topics: This is often determined through experimentation or evaluation metrics.\")\n",
        "print(\"3. Apply NMF: Fit the NMF model to the document-term matrix.\")\n",
        "print(\"4. Analyze the results: Examine the topic-word distributions to understand the themes of each topic and the document-topic distributions to see which topics are most prominent in each document.\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# Demonstrate with dummy data\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import NMF\n",
        "\n",
        "# Dummy text data\n",
        "documents = [\n",
        "    \"This is a document about machine learning and data science.\",\n",
        "    \"Topic modeling is a technique in natural language processing.\",\n",
        "    \"Data analysis and visualization are important for understanding data.\",\n",
        "    \"Machine learning algorithms are used for predictive modeling.\",\n",
        "    \"Natural language processing techniques help in analyzing text data.\"\n",
        "]\n",
        "\n",
        "# Create a TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Apply NMF\n",
        "n_components = 2 # Choosing 2 topics for demonstration\n",
        "nmf = NMF(n_components=n_components, random_state=1)\n",
        "nmf.fit(X)\n",
        "\n",
        "# Display the top words for each topic\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "print(\"Top words for each topic:\")\n",
        "for topic_idx, topic in enumerate(nmf.components_):\n",
        "    print(f\"Topic #{topic_idx + 1}:\")\n",
        "    print(\" \".join([feature_names[i] for i in topic.argsort()[:-10 - 1:-1]]))\n",
        "\n",
        "# Display the topic distribution for each document\n",
        "print(\"\\nTopic distribution for each document:\")\n",
        "document_topics = nmf.transform(X)\n",
        "for doc_idx, topic_distribution in enumerate(document_topics):\n",
        "    print(f\"Document #{doc_idx + 1}: {topic_distribution}\")\n",
        "\n",
        "# 5. Mention potential challenges and considerations\n",
        "print(\"\\n## Challenges and Considerations\")\n",
        "print(\"- **Determining the optimal number of topics:** There's no single best way; methods include coherence scores, perplexity, or simply human judgment.\")\n",
        "print(\"- **Interpreting the results:** Understanding what the identified topics represent requires careful examination of the top words and the documents associated with each topic.\")\n",
        "print(\"- **Preprocessing quality:** The quality of the topic modeling results heavily depends on the effectiveness of the preprocessing steps.\")\n",
        "print(\"- **Computational cost:** For very large datasets, training topic models can be computationally intensive.\")\n",
        "print(\"- **Evaluating model performance:** Quantifying how 'good' a topic model is can be challenging.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf1c0ca2"
      },
      "source": [
        "## Topic assignment and integration\n",
        "\n",
        "### Subtask:\n",
        "Assign the identified topics back to the original data points and integrate this information into your tools (e.g., add topic tags to Notion pages or GitHub issues).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0efadde7"
      },
      "source": [
        "**Reasoning**:\n",
        "Explain how to assign topics to original data points and integrate this information into external tools.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87bfbcc1"
      },
      "source": [
        "# 1. Explain how to associate topic assignments with original documents\n",
        "print(\"## Associating Topic Assignments with Original Documents\")\n",
        "print(\"After applying a topic model (like NMF or LDA), you get a document-topic distribution for each document. This distribution indicates the probability or weight of each topic in that document.\")\n",
        "print(\"The most straightforward way to associate topics with original documents is to assign the topic with the highest probability or weight to each document.\")\n",
        "print(\"For example, if a document's topic distribution is [0.1, 0.8, 0.1], and topic 2 has the highest weight (0.8), you would assign 'Topic 2' to this document.\")\n",
        "print(\"Alternatively, you could assign the top N topics to a document, especially if a document covers multiple themes.\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# 2. Describe how to integrate topic information into tools\n",
        "print(\"## Integrating Topic Information into Tools\")\n",
        "print(\"Integrating topic information into tools like Notion or GitHub involves adding the identified topics as metadata to the original data points (pages, issues, etc.).\")\n",
        "print(\"- **Notion:** You can add a 'Select' or 'Multi-select' property to your database to store the assigned topic(s). Each topic can be a distinct option in the select property.\")\n",
        "print(\"- **GitHub:** You can use labels to tag issues with the assigned topics. Each topic can correspond to a specific label.\")\n",
        "print(\"- **Other tools:** Similar mechanisms like tags, categories, or custom fields can be used depending on the tool's capabilities.\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# 3. Discuss considerations for updating/adding information programmatically via APIs\n",
        "print(\"## Programmatic Integration via APIs or Integration Platforms\")\n",
        "print(\"Manually assigning topics can be tedious, especially for large datasets. Programmatic integration is essential for automation.\")\n",
        "print(\"Most modern tools like Notion and GitHub provide APIs that allow you to read and update data.\")\n",
        "print(\"- **Using APIs directly:** You can write scripts (e.g., in Python) that use the tool's API to fetch data, apply topic modeling, and then use the API again to update the original data points with the assigned topics.\")\n",
        "print(\"  - This requires handling API authentication, understanding the API endpoints for updating properties/labels, and managing rate limits.\")\n",
        "print(\"- **Using Integration Platforms (like n8n):** Integration platforms provide visual workflows and pre-built nodes for interacting with various applications' APIs.\")\n",
        "print(\"  - You can create a workflow that fetches data from a source node (e.g., GitHub), passes it to a custom code node for topic modeling, and then uses an update node (e.g., GitHub Update Issue, Notion Update Page) to add the topic information.\")\n",
        "print(\"  - This simplifies the process by abstracting away some of the API complexities and providing a visual interface for workflow management.\")\n",
        "print(\"\\n\")\n",
        "print(\"Considerations for programmatic updates:\")\n",
        "print(\"- **Authentication and Authorization:** Securely managing API keys or tokens.\")\n",
        "print(\"- **Rate Limits:** Respecting the API's rate limits to avoid being blocked.\")\n",
        "print(\"- **Error Handling:** Implementing mechanisms to handle potential API errors or network issues.\")\n",
        "print(\"- **Idempotency:** Ensuring that running the update process multiple times doesn't create duplicate or incorrect entries.\")\n",
        "print(\"- **Data Mapping:** Correctly mapping the processed data and topic assignments back to the original data points in the target tool.\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# 4. Mention the importance of mapping topics to meaningful labels\n",
        "print(\"## Mapping Topics to Meaningful Labels\")\n",
        "print(\"Topic modeling algorithms typically output topics as a collection of words. For easier understanding and integration, it's crucial to map these word-based topics to meaningful, human-readable labels.\")\n",
        "print(\"This mapping can be done by examining the top words in each topic and interpreting the underlying theme.\")\n",
        "print(\"For example, a topic with top words like 'bug', 'error', 'fix', 'issue' could be labeled as 'Bug Fixing'.\")\n",
        "print(\"These meaningful labels are what you would use as tags or properties in your integrated tools.\")\n",
        "print(\"This step is often manual and requires domain expertise to accurately interpret the topics.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7856c1dc"
      },
      "source": [
        "## Visualization and analysis\n",
        "\n",
        "### Subtask:\n",
        "Visualize the topics and their distribution to gain insights into the key themes in your data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dfd1894"
      },
      "source": [
        "**Reasoning**:\n",
        "I will provide a detailed explanation of common visualization techniques for topic modeling results, discuss the insights gained from them, mention relevant tools, and explain how visualizations aid in refinement, fulfilling the subtask requirements.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ea6965f0"
      },
      "source": [
        "# 1. Explain common visualization techniques for topic modeling results\n",
        "print(\"## Visualizing Topic Modeling Results\")\n",
        "print(\"Visualizing the results of topic modeling is crucial for understanding the identified themes and their distribution within the data.\")\n",
        "print(\"Common visualization techniques include:\")\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"### Word Clouds for Topics\")\n",
        "print(\"Word clouds visually represent the most important words in each topic. The size of the word in the cloud is proportional to its weight or frequency within that topic.\")\n",
        "print(\"This provides a quick overview of the key terms that define each discovered topic.\")\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"### Bar Plots for Topic Distribution per Document\")\n",
        "print(\"Bar plots can show the distribution of topics within individual documents or across a set of documents.\")\n",
        "print(\"For a single document, a bar plot can show the weight or probability of each topic, highlighting the dominant theme(s).\")\n",
        "print(\"Across documents, stacked bar plots or heatmaps can illustrate how topic prevalence varies.\")\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"### Intertopic Distance Maps (e.g., using t-SNE or PCA)\")\n",
        "print(\"These maps visualize the relationships between the identified topics in a 2D space.\")\n",
        "print(\"Topics that are semantically similar tend to appear closer together on the map, while dissimilar topics are further apart.\")\n",
        "print(\"This helps in understanding the overall structure of the topics and identifying potential overlaps or distinct themes.\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# 2. Discuss what insights can be gained from these visualizations\n",
        "print(\"## Insights Gained from Visualizations\")\n",
        "print(\"Visualizations offer several key insights into the topic modeling results:\")\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"- **Understanding Topic Composition:** Word clouds clearly show which words are most representative of each topic, helping in the interpretation and labeling of topics.\")\n",
        "print(\"- **Identifying Dominant Topics in Documents:** Bar plots of topic distribution per document reveal which topics are most prominent in specific pieces of text, allowing for categorization and focused analysis.\")\n",
        "print(\"- **Observing Relationships Between Topics:** Intertopic distance maps help in understanding how topics relate to each other. Are there clusters of related topics? Are some topics very distinct? This can inform how you group or utilize topics.\")\n",
        "print(\"- **Assessing Topic Coherence:** By looking at the words in a word cloud, you can intuitively assess if the words within a topic are semantically related and form a coherent theme.\")\n",
        "print(\"- **Identifying Noise or Undesired Topics:** Visualizations can sometimes highlight topics that are dominated by noise or irrelevant terms, indicating the need for further preprocessing or model refinement.\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# 3. Mention potential tools or libraries for creating these visualizations\n",
        "print(\"## Tools and Libraries for Visualization\")\n",
        "print(\"Several Python libraries are well-suited for creating these visualizations:\")\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"- **Matplotlib and Seaborn:** These are general-purpose plotting libraries that can be used to create bar plots for topic distributions and custom visualizations based on topic-word weights.\")\n",
        "print(\"- **wordcloud:** A specific library for generating word clouds from text data or word frequencies.\")\n",
        "print(\"- **pyLDAvis:** Although named for LDA, pyLDAvis is a powerful interactive visualization tool that works well with topic models from libraries like scikit-learn (including NMF). It provides interactive intertopic distance maps and displays the most salient terms for selected topics.\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# 4. Briefly describe how these visualizations can inform the refinement and iteration process\n",
        "print(\"## Informing Refinement and Iteration\")\n",
        "print(\"Visualizations are invaluable in the iterative process of refining a topic model:\")\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"- **Adjusting the Number of Topics:** The intertopic distance map can help assess if the chosen number of topics is appropriate. Too many overlapping topics might suggest reducing the number, while a sparse map could indicate the need for more topics.\")\n",
        "print(\"- **Improving Preprocessing:** If word clouds show irrelevant terms dominating topics, it signals the need to revisit and enhance the text preprocessing steps (e.g., adding more stop words, improving tokenization).\")\n",
        "print(\"- **Model Parameter Tuning:** Visualizations can help evaluate the impact of different model parameters (e.g., alpha and beta in LDA) on the resulting topics and their interpretability.\")\n",
        "print(\"- **Labeling and Interpreting Topics:** Visualizations are the primary tool for understanding what each topic represents and assigning meaningful labels, which can be refined based on visual inspection.\")\n",
        "print(\"- **Identifying Outlier Documents:** Examining topic distributions per document can help identify documents that don't clearly belong to any topic or have a very uniform distribution, which might require further investigation.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b00d6fc8"
      },
      "source": [
        "## Workflow automation\n",
        "\n",
        "### Subtask:\n",
        "Automate the entire process using your integration hub (n8n) to periodically collect data, perform topic modeling, and update your tools.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95c27a08"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires explaining how to automate the topic modeling process using an integration hub like n8n. I will use a markdown cell to provide this explanation, covering the workflow steps, benefits, and considerations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2f6d97ff"
      },
      "source": [
        "# Explanation of automating the topic modeling workflow using an integration hub like n8n\n",
        "\n",
        "## Automating the Topic Modeling Workflow with an Integration Hub\n",
        "\n",
        "An integration hub like n8n can orchestrate the entire end-to-end topic modeling workflow, transforming it from a manual process into an automated one that runs periodically. This ensures that your topic assignments and insights are always up-to-date across your integrated tools.\n",
        "\n",
        "### Typical n8n Workflow for Topic Modeling\n",
        "\n",
        "An n8n workflow for this task would typically involve a series of connected nodes, each performing a specific part of the process:\n",
        "\n",
        "1.  **Trigger Node:** This node initiates the workflow. It could be a **Schedule** node to run the workflow at regular intervals (e.g., daily, weekly) or a webhook/event-based trigger if you want to process data as it arrives (e.g., a new GitHub issue is created).\n",
        "2.  **Data Collection Nodes:** These nodes connect to your data sources (GitHub, Notion, communication platforms, etc.). You would use specific nodes for each service (e.g., **GitHub Node**, **Notion Node**, **Slack Node**, **Discord Node**) configured to retrieve the relevant text data (issue bodies, page content, messages).\n",
        "3.  **Data Preprocessing Node(s):** This is where the cleaning and preprocessing of the collected text data happens. This could be a **Code Node** (using Python or JavaScript) or a series of nodes for specific text manipulation tasks if available within n8n's built-in nodes or community nodes. The code would perform steps like removing punctuation, lowercasing, removing stop words, and tokenization.\n",
        "4.  **Topic Modeling Node:** This node applies the topic modeling algorithm. Similar to preprocessing, this would likely be a **Code Node** where you implement or call a script that runs your chosen topic modeling technique (LDA, NMF) using libraries like scikit-learn. This node would output the document-topic distributions and potentially the topic-word distributions.\n",
        "5.  **Topic Assignment Node:** This node processes the output of the topic modeling step to assign the most relevant topic(s) back to each original document. This could also be a **Code Node** that determines the dominant topic(s) based on the document-topic distributions and prepares the data for updating the target tools.\n",
        "6.  **Data Integration/Update Nodes:** These nodes connect to your target tools where you want to store or display the topic information. You would use nodes for the specific services (e.g., **GitHub Node**, **Notion Node**) configured to update the original data points (issues, pages) with the assigned topic labels (e.g., adding a label in GitHub, updating a select property in Notion).\n",
        "7.  **Error Handling Nodes:** n8n allows you to add nodes to catch and handle errors that might occur at any stage of the workflow (e.g., a data collection node fails, an API update fails). This ensures the workflow doesn't silently fail and can notify you of issues.\n",
        "8.  **Notification Node (Optional):** You could add a node (e.g., **Email Node**, **Slack Node**) to send a notification about the workflow's success or failure.\n",
        "\n",
        "### Benefits of Using an Integration Platform for Automation\n",
        "\n",
        "Using an integration platform like n8n for this workflow offers significant advantages:\n",
        "\n",
        "*   **Automation and Scheduling:** Easily schedule the workflow to run automatically at defined intervals, eliminating the need for manual execution.\n",
        "*   **Connectivity:** Access a wide range of pre-built integrations (nodes) for popular services, simplifying data collection and integration without writing custom API connectors for each tool.\n",
        "*   **Visual Workflow Design:** Build and manage the entire process using a drag-and-drop interface, making it easier to understand, modify, and maintain the workflow.\n",
        "*   **Error Handling and Monitoring:** Built-in error handling capabilities and execution logs help identify and troubleshoot issues in the workflow.\n",
        "*   **Scalability:** Integration platforms can often handle increasing data volumes and complexity as your needs grow.\n",
        "*   **Centralized Management:** Manage all your data collection, processing, and integration workflows from a single platform.\n",
        "\n",
        "### Considerations for Setting Up and Maintaining an Automated Workflow\n",
        "\n",
        "Setting up and maintaining an automated topic modeling workflow requires careful consideration of several factors:\n",
        "\n",
        "*   **Data Volume and Processing Time:** For large datasets, the topic modeling step can be computationally intensive. Ensure your n8n environment or the external service running the topic modeling code has sufficient resources. Monitor execution times to avoid timeouts.\n",
        "*   **API Rate Limits:** Be mindful of the API rate limits of the integrated tools (GitHub, Notion, etc.) when collecting and updating data. Design your workflow to respect these limits, potentially by adding waits or processing data in batches.\n",
        "*   **Topic Model Updates:** Decide how often you need to retrain your topic model. As new data arrives, the topics might evolve. The automated workflow can be configured to periodically retrain the model.\n",
        "*   **Topic Interpretation and Labeling:** While the workflow is automated, interpreting the meaning of newly discovered topics and assigning meaningful labels is still a human task. You'll need a process for reviewing topic model outputs and updating the labels used in your integration.\n",
        "*   **Schema Changes in Tools:** Changes in the APIs or data structures of your integrated tools could break the workflow. Regular monitoring and testing are necessary to ensure the workflow remains compatible.\n",
        "*   **Authentication and Security:** Securely manage API credentials and ensure your n8n instance is properly secured.\n",
        "*   **Handling Data Noise and Evolution:** The quality of the topic modeling output depends on the input data. The automated process should ideally include robust preprocessing steps, and you may need to refine these as the nature of your data changes.\n",
        "\n",
        "By leveraging an integration hub like n8n, you can create a powerful and automated system for continuous topic modeling, providing ongoing insights into your data across various tools."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e692cf4"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt failed due to a `SyntaxError` because the markdown text was placed directly within a Python `print` statement without proper escaping or multi-line string formatting. I need to fix the code to correctly print the markdown content as a multi-line string.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkDo9NEeoFZ5"
      },
      "source": [
        "# Explanation of automating the topic modeling workflow using an integration hub like n8n\n",
        "\n",
        "markdown_explanation = \"\"\"\n",
        "## Automating the Topic Modeling Workflow with an Integration Hub\n",
        "\n",
        "An integration hub like n8n can orchestrate the entire end-to-end topic modeling workflow, transforming it from a manual process into an automated one that runs periodically. This ensures that your topic assignments and insights are always up-to-date across your integrated tools.\n",
        "\n",
        "### Typical n8n Workflow for Topic Modeling\n",
        "\n",
        "An n8n workflow for this task would typically involve a series of connected nodes, each performing a specific part of the process:\n",
        "\n",
        "1.  **Trigger Node:** This node initiates the workflow. It could be a **Schedule** node to run the workflow at regular intervals (e.g., daily, weekly) or a webhook/event-based trigger if you want to process data as it arrives (e.g., a new GitHub issue is created).\n",
        "2.  **Data Collection Nodes:** These nodes connect to your data sources (GitHub, Notion, communication platforms, etc.). You would use specific nodes for each service (e.g., **GitHub Node**, **Notion Node**, **Slack Node**, **Discord Node**) configured to retrieve the relevant text data (issue bodies, page content, messages).\n",
        "3.  **Data Preprocessing Node(s):** This is where the cleaning and preprocessing of the collected text data happens. This could be a **Code Node** (using Python or JavaScript) or a series of nodes for specific text manipulation tasks if available within n8n's built-in nodes or community nodes. The code would perform steps like removing punctuation, lowercasing, removing stop words, and tokenization.\n",
        "4.  **Topic Modeling Node:** This node applies the topic modeling algorithm. Similar to preprocessing, this would likely be a **Code Node** where you implement or call a script that runs your chosen topic modeling technique (LDA, NMF) using libraries like scikit-learn. This node would output the document-topic distributions and potentially the topic-word distributions.\n",
        "5.  **Topic Assignment Node:** This node processes the output of the topic modeling step to assign the most relevant topic(s) back to each original document. This could also be a **Code Node** that determines the dominant topic(s) based on the document-topic distributions and prepares the data for updating the target tools.\n",
        "6.  **Data Integration/Update Nodes:** These nodes connect to your target tools where you want to store or display the topic information. You would use nodes for the specific services (e.g., **GitHub Node**, **Notion Node**) configured to update the original data points (issues, pages) with the assigned topic labels (e.g., adding a label in GitHub, updating a select property in Notion).\n",
        "7.  **Error Handling Nodes:** n8n allows you to add nodes to catch and handle errors that might occur at any stage of the workflow (e.g., a data collection node fails, an API update fails). This ensures the workflow doesn't silently fail and can notify you of issues.\n",
        "8.  **Notification Node (Optional):** You could add a node (e.g., **Email Node**, **Slack Node**) to send a notification about the workflow's success or failure.\n",
        "\n",
        "### Benefits of Using an Integration Platform for Automation\n",
        "\n",
        "Using an integration platform like n8n for this workflow offers significant advantages:\n",
        "\n",
        "*   **Automation and Scheduling:** Easily schedule the workflow to run automatically at defined intervals, eliminating the need for manual execution.\n",
        "*   **Connectivity:** Access a wide range of pre-built integrations (nodes) for popular services, simplifying data collection and integration without writing custom API connectors for each tool.\n",
        "*   **Visual Workflow Design:** Build and manage the entire process using a drag-and-drop interface, making it easier to understand, modify, and maintain the workflow.\n",
        "*   **Error Handling and Monitoring:** Built-in error handling capabilities and execution logs help identify and troubleshoot issues in the workflow.\n",
        "*   **Scalability:** Integration platforms can often handle increasing data volumes and complexity as your needs grow.\n",
        "*   **Centralized Management:** Manage all your data collection, processing, and integration workflows from a single platform.\n",
        "\n",
        "### Considerations for Setting Up and Maintaining an Automated Workflow\n",
        "\n",
        "Setting up and maintaining an automated topic modeling workflow requires careful consideration of several factors:\n",
        "\n",
        "*   **Data Volume and Processing Time:** For large datasets, the topic modeling step can be computationally intensive. Ensure your n8n environment or the external service running the topic modeling code has sufficient resources. Monitor execution times to avoid timeouts.\n",
        "*   **API Rate Limits:** Be mindful of the API rate limits of the integrated tools (GitHub, Notion, etc.) when collecting and updating data. Design your workflow to respect these limits, potentially by adding waits or processing data in batches.\n",
        "*   **Topic Model Updates:** Decide how often you need to retrain your topic model. As new data arrives, the topics might evolve. The automated workflow can be configured to periodically retrain the model.\n",
        "*   **Topic Interpretation and Labeling:** While the workflow is automated, interpreting the meaning of newly discovered topics and assigning meaningful labels is still a human task. You'll need a process for reviewing topic model outputs and updating the labels used in your integration.\n",
        "*   **Schema Changes in Tools:** Changes in the APIs or data structures of your integrated tools could break the workflow. Regular monitoring and testing are necessary to ensure the workflow remains compatible.\n",
        "*   **Authentication and Security:** Securely manage API credentials and ensure your n8n instance is properly secured.\n",
        "*   **Handling Data Noise and Evolution:** The quality of the topic modeling output depends on the input data. The automated process should ideally include robust preprocessing steps, and you may need to refine these as the nature of your data changes.\n",
        "\n",
        "By leveraging an integration hub like n8n, you can create a powerful and automated system for continuous topic modeling, providing ongoing insights into your data across various tools.\n",
        "\"\"\"\n",
        "\n",
        "print(markdown_explanation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f987185"
      },
      "source": [
        "## Refinement and iteration\n",
        "\n",
        "### Subtask:\n",
        "Continuously refine your topic modeling approach based on the results and your evolving needs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f3b41ea"
      },
      "source": [
        "**Reasoning**:\n",
        "Explain the importance of continuous refinement, methods for evaluating topic model quality, how feedback informs refinement, how to iterate on various aspects of the model, and the role of monitoring the automated workflow. All these points can be covered in a single markdown explanation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98449dda"
      },
      "source": [
        "# Explain the importance of continuous refinement and related aspects\n",
        "\n",
        "markdown_explanation = \"\"\"\n",
        "## Continuously Refining Your Topic Modeling Approach\n",
        "\n",
        "Topic modeling is not a one-time process. To ensure the identified topics remain relevant, accurate, and useful over time, continuous refinement is essential. Data evolves, your needs change, and initial model parameters might not be optimal in the long run.\n",
        "\n",
        "### 1. Importance of Continuous Refinement\n",
        "\n",
        "*   **Adapting to Evolving Data:** The language and themes in your data sources (GitHub issues, communication messages) can change over time. New projects, tools, or discussions introduce new terminology and topics. Continuous refinement allows your model to capture these changes.\n",
        "*   **Improving Topic Quality:** Initial topic models might produce topics that are not clearly defined, contain irrelevant words, or overlap significantly. Refinement helps in obtaining more coherent and interpretable topics.\n",
        "*   **Maintaining Relevance:** As your team's focus or project goals shift, the topics you are interested in might also change. Refining the model can help align the discovered topics with your current analytical needs.\n",
        "*   **Optimizing Performance:** Iterating on the model and preprocessing steps can improve the performance of topic assignment and the overall utility of the topic model.\n",
        "\n",
        "### 2. Methods for Evaluating Topic Model Quality\n",
        "\n",
        "Evaluating the quality of a topic model can be done using a combination of automatic metrics and human judgment:\n",
        "\n",
        "*   **Topic Coherence:** This metric measures the semantic similarity between the high-scoring words in a topic. A high coherence score suggests that the words within a topic are related and the topic is interpretable. There are different variations of coherence scores (e.g., UMass, C_v).\n",
        "*   **Perplexity:** This metric, commonly used with LDA, measures how well the model predicts a held-out set of documents. A lower perplexity generally indicates a better model, but it doesn't always correlate perfectly with human interpretability.\n",
        "*   **Human Evaluation:** This is arguably the most important evaluation method. It involves subject matter experts reviewing the top words for each topic and a sample of documents assigned to those topics to assess their interpretability and relevance. This can also involve tasks like asking humans to match topics to documents or rate topic quality.\n",
        "*   **Qualitative Analysis of Topic Assignments:** Reviewing how topics are assigned to documents in your integrated tools can reveal if the assignments make sense and are useful for your workflow.\n",
        "\n",
        "### 3. How Feedback Informs the Refinement Process\n",
        "\n",
        "Feedback from various sources should inform your refinement efforts:\n",
        "\n",
        "*   **Visualization Feedback:**\n",
        "    *   **Word Clouds:** If word clouds show irrelevant words or a mix of unrelated terms in a topic, it indicates issues with preprocessing or the need to adjust the number of topics.\n",
        "    *   **Intertopic Distance Maps:** Overlapping topic clusters might suggest reducing the number of topics, while isolated topics could be too specific or noisy.\n",
        "    *   **Topic Distribution Plots:** If documents are not clearly assigned to dominant topics or have very flat distributions, it might indicate the need for model tuning or more topics.\n",
        "*   **Feedback from Integrated Tools:**\n",
        "    *   **User Feedback:** Users interacting with topic tags in Notion or GitHub might report that topic assignments are inaccurate, confusing, or not useful. This is direct feedback on the model's practical utility.\n",
        "    *   **Workflow Effectiveness:** If the automated workflows based on topic assignments are not producing the desired results (e.g., incorrect routing of issues), it signals a problem with the underlying topic model.\n",
        "*   **Human Evaluation Results:** Explicit human evaluation provides structured feedback on topic interpretability and relevance, guiding adjustments to the model or the number of topics.\n",
        "\n",
        "### 4. Iterating on Preprocessing, Number of Topics, and Algorithm\n",
        "\n",
        "Refinement is an iterative process involving adjustments to different parts of the topic modeling pipeline:\n",
        "\n",
        "*   **Iterating on Preprocessing:**\n",
        "    *   **Stop Word Lists:** Add domain-specific stop words (e.g., project names, common jargon) that don't contribute to topics. Remove words that appear to be stop words but were missed.\n",
        "    *   **Tokenization:** Adjust tokenization rules to handle specific characters or phrases relevant to your data (e.g., keeping hashtags if they represent topics).\n",
        "    *   **Stemming/Lemmatization:** Experiment with or without these techniques, or try different approaches, to see what produces more coherent topics.\n",
        "    *   **Handling Special Characters/Code:** Improve rules for removing or handling code snippets, links, mentions, etc., that might introduce noise.\n",
        "*   **Iterating on the Number of Topics:**\n",
        "    *   Start with a reasonable range based on domain knowledge or initial evaluation.\n",
        "    *   Train models with different numbers of topics.\n",
        "    *   Evaluate the models using coherence scores, perplexity, and human evaluation.\n",
        "    *   Use intertopic distance maps to visually assess topic separation. Choose the number of topics that yields interpretable and well-separated themes relevant to your needs.\n",
        "*   **Iterating on the Topic Modeling Algorithm:**\n",
        "    *   If using LDA, experiment with hyperparameters like alpha and beta.\n",
        "    *   If using NMF, consider different initializations or parameters.\n",
        "    *   If one algorithm isn't performing well, consider trying a different one (e.g., switching from LDA to NMF or vice versa) or exploring more advanced techniques if necessary.\n",
        "\n",
        "### 5. Role of Monitoring the Automated Workflow\n",
        "\n",
        "Monitoring the automated workflow is critical for continuous refinement and ensuring the system's reliability:\n",
        "\n",
        "*   **Execution Monitoring:** Regularly check the execution logs of your n8n workflow to identify any failures or errors in data collection, preprocessing, topic modeling, or integration steps.\n",
        "*   **Data Quality Monitoring:** Implement checks to ensure the collected data is in the expected format and volume.\n",
        "*   **Topic Model Output Monitoring:** Periodically review the output of the topic modeling step within the workflow logs or by outputting results to a staging area. Check for unexpected topics or significant shifts in topic distributions that might indicate issues or the need for retraining.\n",
        "*   **Integration Monitoring:** Verify that the topic information is being correctly added or updated in your target tools (Notion, GitHub).\n",
        "*   **Performance Monitoring:** Track the execution time of the workflow, especially the computationally intensive topic modeling step, to ensure it completes within acceptable limits.\n",
        "*   **Alerting:** Set up alerts for workflow failures or significant anomalies in the monitoring metrics.\n",
        "\n",
        "By continuously monitoring and refining your topic modeling approach based on evaluation metrics, visualization feedback, user input, and workflow performance, you can maintain a robust and valuable system for understanding your data across your integrated tools.\n",
        "\"\"\"\n",
        "\n",
        "print(markdown_explanation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d3c0c8e"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The initial attempt to collect data from external tools via n8n failed as the AI model cannot directly interact with external platforms.\n",
        "*   A detailed conceptual explanation of text data cleaning and preprocessing steps for topic modeling was provided, including removing punctuation, converting to lowercase, removing stop words, tokenization, stemming/lemmatization, and relevant Python libraries (NLTK, spaCy, scikit-learn), along with preprocessing considerations for specific data sources (GitHub issues, Notion pages, communication messages).\n",
        "*   The concept of topic modeling and its purpose were explained, along with descriptions of two common techniques, Latent Dirichlet Allocation (LDA) and Non-negative Matrix Factorization (NMF), including factors for choosing between them.\n",
        "*   A practical demonstration of applying NMF using scikit-learn with dummy data was provided, showing how to create a document-term matrix, apply the model, and display top words per topic and topic distribution per document.\n",
        "*   Potential challenges in topic modeling, such as determining the optimal number of topics and interpreting results, were discussed.\n",
        "*   The process of associating topic assignments with original documents based on document-topic distributions was explained, along with methods for integrating this information into tools like Notion (using properties) and GitHub (using labels).\n",
        "*   The importance of programmatic integration via APIs or integration platforms (like n8n) for automating topic assignment updates was highlighted, covering considerations such as authentication, rate limits, and error handling.\n",
        "*   The necessity of mapping word-based topics to meaningful human-readable labels for practical integration was emphasized.\n",
        "*   Common visualization techniques for topic modeling results were described, including word clouds, bar plots for topic distribution, and intertopic distance maps, along with the insights gained from each.\n",
        "*   Relevant Python libraries for visualization (Matplotlib, Seaborn, wordcloud, pyLDAvis) were mentioned.\n",
        "*   The role of visualizations in informing the refinement process, such as adjusting the number of topics and improving preprocessing, was explained.\n",
        "*   A detailed explanation of automating the entire topic modeling workflow using an integration hub like n8n was provided, outlining the typical nodes involved (Trigger, Data Collection, Preprocessing, Topic Modeling, Topic Assignment, Data Integration/Update, Error Handling, Notification), the benefits of using such a platform, and critical considerations for setup and maintenance (data volume, API limits, topic model updates, human interpretation, etc.).\n",
        "*   The importance of continuously refining the topic modeling approach was stressed, covering the necessity of adapting to evolving data, improving topic quality, maintaining relevance, and optimizing performance.\n",
        "*   Methods for evaluating topic model quality were detailed, including topic coherence, perplexity, and human evaluation.\n",
        "*   How feedback from visualizations, integrated tools (user feedback, workflow effectiveness), and human evaluation informs the refinement process was explained.\n",
        "*   Strategies for iterating on preprocessing, the number of topics, and the chosen algorithm were outlined.\n",
        "*   The crucial role of monitoring the automated workflow (execution, data quality, model output, integration, performance) for continuous refinement and system reliability was emphasized.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   Implementing the described workflow in an integration platform like n8n would be the logical next step to operationalize the topic modeling process.\n",
        "*   Developing a process for human review and labeling of identified topics is crucial for the practical utility of the automated system.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3ce5bd4"
      },
      "source": [
        "# Task\n",
        "Create a plan to automate the process of collecting LLM research history, generating visual glyphs from the research, and creating podcast episodes based on the research, leveraging the previously discussed Colab queue and scheduling mechanisms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d79e6ee0"
      },
      "source": [
        "## Collect and normalize llm histories\n",
        "\n",
        "### Subtask:\n",
        "Gather research history from various LLM sources (ChatGPT, Claude, Gemini, Grok logs), export them to a unified folder, and preprocess the text by stripping prompts/responses, chunking by theme/topic, and tagging with archetypes. Save each processed block as a structured JSON file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c237f7d"
      },
      "source": [
        "## Generate glyphs automatically\n",
        "\n",
        "### Subtask:\n",
        "Develop a process to generate visual symbols (glyphs) from the processed research blocks. This involves extracting keywords, metaphors, or archetypes and using a text-to-symbol algorithm or script (potentially involving libraries like `cairosvg` or `drawSvg`) to create SVG glyphs. Store the generated glyphs and associated metadata.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0316525e"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to outline a conceptual process for generating visual glyphs from text data, discuss potential approaches and libraries, describe the storage structure, and explain the integration with other parts of the workflow. This can be done using a markdown cell to provide a detailed explanation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "504606df"
      },
      "source": [
        "markdown_explanation = \"\"\"\n",
        "## Generating Visual Symbols (Glyphs) from Processed Research Blocks\n",
        "\n",
        "This subtask focuses on transforming the processed text blocks from LLM research history into visual symbols or glyphs. This process aims to provide a non-textual representation of the key concepts, themes, or sentiments within each research block.\n",
        "\n",
        "### 1. Conceptual Process for Glyph Generation\n",
        "\n",
        "The conceptual process for generating visual glyphs from text data involves several steps:\n",
        "\n",
        "*   **Input:** Receive a processed text block, likely in a structured format (e.g., the JSON files from the previous, albeit failed, subtask), containing the cleaned text, extracted keywords, identified archetypes, or thematic tags.\n",
        "*   **Feature Extraction/Selection:** From the processed text block, identify the core elements that should influence the visual glyph. This could involve:\n",
        "    *   **Keywords:** High-frequency or highly relevant terms.\n",
        "    *   **Metaphors/Analogies:** Figurative language used in the research.\n",
        "    *   **Archetypes/Themes:** The pre-assigned or topic-modeled themes.\n",
        "    *   **Sentiment:** The overall emotional tone (positive, negative, neutral).\n",
        "    *   **Structural elements:** Paragraph breaks, headings, code blocks (though these might be removed in preprocessing).\n",
        "*   **Mapping to Visual Elements:** Translate the extracted features into visual attributes of the glyph. This is the core of the text-to-symbol conversion. For example:\n",
        "    *   Keywords could influence specific shapes or icons.\n",
        "    *   Sentiment could dictate color palettes (e.g., warm colors for positive, cool for negative).\n",
        "    *   Archetypes could map to predefined symbols or combinations of shapes.\n",
        "    *   The complexity of the text could influence the complexity of the glyph.\n",
        "*   **Symbol Generation:** Use a text-to-symbol algorithm or script to programmatically create the visual glyph based on the mapped visual elements. This step will produce the actual visual representation, likely in a vector format like SVG.\n",
        "*   **Metadata Association:** Store the generated glyph along with relevant metadata, linking it back to the original text block and the features used to generate it.\n",
        "\n",
        "### 2. Potential Approaches and Python Libraries\n",
        "\n",
        "Several approaches and Python libraries can be used for the text-to-symbol conversion:\n",
        "\n",
        "*   **Rule-Based Mapping:** Define a set of rules that map specific keywords, archetypes, or sentiment scores to predefined visual elements (shapes, colors, patterns). A script would apply these rules to generate the SVG code.\n",
        "    *   **Libraries:** Text processing libraries like **NLTK** or **spaCy** can help with keyword extraction and sentiment analysis. Libraries like **drawSvg** or **svgwrite** can be used to programmatically generate SVG code by drawing shapes, paths, and text.\n",
        "*   **Algorithmic/Generative Art:** Use algorithms that take the text features as input parameters to generate unique visual patterns or structures. This could involve applying principles from generative art where the input text influences the output visual.\n",
        "    *   **Libraries:** Libraries for creative coding or data visualization might be relevant, potentially combined with image processing libraries if raster elements are involved, though SVG is preferred for scalability. **drawSvg** or **svgwrite** would still be key for SVG output.\n",
        "*   **Pre-trained Text-to-Image Models (with constraints):** While full text-to-image models like DALL-E or Midjourney are powerful, they might be overkill and difficult to control for generating simple, consistent glyphs. However, a constrained version or fine-tuned model specifically trained to output symbolic representations might be a future possibility. This is less practical with standard Python libraries and would likely involve external services or specialized models.\n",
        "*   **Using Icon Font Libraries:** Map keywords or archetypes to specific icons within an existing icon font (like Font Awesome or Material Icons). While not truly generative, it provides a symbolic representation. The SVG for the chosen icon can then be retrieved.\n",
        "    *   **Libraries:** Standard text processing libraries to identify the keyword/archetype. You would need access to the icon font files or an API to retrieve the SVG for a given icon name.\n",
        "\n",
        "For generating SVG glyphs programmatically, libraries like `drawSvg` or `svgwrite` are highly suitable as they allow you to define shapes, colors, and layouts directly in Python code, which can then be exported as SVG files. `cairosvg` is useful for converting SVG to other formats (like PNG or PDF), but `drawSvg` or `svgwrite` are for *creating* the SVG.\n",
        "\n",
        "### 3. Storage Structure for Glyphs and Metadata\n",
        "\n",
        "The generated SVG glyphs and their associated metadata should be stored in a way that allows for easy retrieval and linking to the original research blocks.\n",
        "\n",
        "A possible storage structure could be:\n",
        "\n",
        "*   **Directory Structure:** Organize files in a logical directory structure. For instance, a main directory for glyphs, with subdirectories perhaps based on the archetype or the date of generation.\n",
        "    *   `/data/glyphs/`\n",
        "    *   `/data/glyphs/by_archetype/innovation/glyph_123.svg`\n",
        "    *   `/data/glyphs/by_date/2025-10-03/glyph_456.svg`\n",
        "*   **File Naming Convention:** Use a consistent naming convention for the SVG files, perhaps including a unique identifier linked to the original research block (e.g., `glyph_<research_block_id>.svg`).\n",
        "*   **Metadata Storage:** Store the metadata associated with each glyph. This could be done in several ways:\n",
        "    *   **Separate Metadata File:** A JSON or YAML file alongside each SVG file (e.g., `glyph_<research_block_id>.json`) containing:\n",
        "        *   `research_block_id`: Link to the original processed text block.\n",
        "        *   `extracted_keywords`: List of keywords used.\n",
        "        *   `assigned_archetypes`: List of archetypes.\n",
        "        *   `sentiment_score`: Sentiment value.\n",
        "        *   `generation_parameters`: Parameters used by the glyph generation algorithm.\n",
        "        *   `generated_at`: Timestamp of generation.\n",
        "        *   `svg_file_path`: Path to the generated SVG file.\n",
        "    *   **Database Entry:** Store all metadata in a database (e.g., SQLite, PostgreSQL). The database table would include fields for all the metadata points and a column storing the path to the SVG file or even the SVG content directly (though storing large SVG strings in a database might have performance implications).\n",
        "    *   **Embedding Metadata in SVG (less common for complex metadata):** SVG allows for embedding metadata, but it's typically not used for extensive structured data like this.\n",
        "\n",
        "Storing the SVG files separately and linking them via metadata in a structured file or database is generally a flexible and scalable approach.\n",
        "\n",
        "### 4. Integration with Other Workflow Steps\n",
        "\n",
        "The glyph generation process integrates into the overall workflow as follows:\n",
        "\n",
        "*   **Input from Data Collection and Preprocessing:** The glyph generation step receives the processed research blocks as input. This means the data collection, normalization, chunking, and initial tagging (including archetype assignment) must be completed before glyph generation can occur. The quality of the input data directly impacts the quality and relevance of the generated glyphs.\n",
        "*   **Output for Podcast Generation:** The generated glyphs and their metadata serve as inputs for the podcast episode generation step. The glyphs can be used:\n",
        "    *   As visual cues or chapter markers if the podcast includes a visual component (e.g., a video podcast).\n",
        "    *   As symbolic representations to reference during the audio narration (e.g., \"As indicated by the 'innovation' glyph...\").\n",
        "    *   The metadata can inform the script generation, ensuring the podcast discusses the key themes represented by the glyphs.\n",
        "*   **Integration with Automation (n8n):** The glyph generation process would be another step in the automated n8n workflow. After the data collection and preprocessing nodes, there would be a node (likely a **Code Node** calling a Python script) that performs the glyph generation based on the processed data. This node would output the paths to the generated SVG files and the associated metadata, which can then be passed to subsequent nodes for storage and podcast generation. The automation ensures that as new research history is collected, corresponding glyphs are automatically generated.\n",
        "*   **Refinement Loop:** Feedback from the podcast generation (e.g., are the glyphs useful or understandable in the narrative?) or from human review of the glyphs can inform the refinement of the glyph generation algorithm, the feature extraction process, or even the upstream preprocessing steps.\n",
        "\n",
        "By integrating glyph generation into the automated pipeline, each processed research block can be automatically associated with a visual symbol, enriching the data and providing a new layer of information for subsequent steps like podcast creation.\n",
        "\"\"\"\n",
        "print(markdown_explanation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "087cd642"
      },
      "source": [
        "## Generate podcast episodes\n",
        "\n",
        "### Subtask:\n",
        "For clusters of research blocks or specific themes, generate podcast episode content. This involves summarizing the text, adding a narrative layer, and using a Text-to-Speech (TTS) engine to convert the script into audio files (e.g., MP3s). Generate show notes from the metadata.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dcc34ad"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to provide a detailed explanation of how to generate podcast episode content from processed research blocks, including selecting blocks, scripting, TTS conversion, show notes generation, and storage. This is a conceptual explanation, so a markdown cell is appropriate.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6420c76"
      },
      "source": [
        "markdown_explanation = \"\"\"\n",
        "## Generating Podcast Episode Content from Research Blocks\n",
        "\n",
        "This subtask outlines the process of transforming processed and potentially clustered/themed research blocks into content for a podcast episode, including scripting, audio generation via Text-to-Speech (TTS), and generating show notes.\n",
        "\n",
        "### 1. Grouping or Selecting Processed Research Blocks\n",
        "\n",
        "Before generating a podcast episode, you need to define the scope and content by grouping or selecting relevant research blocks. This can be done based on the insights gained from previous steps:\n",
        "\n",
        "*   **Using Topic Modeling Results:** Select all research blocks that have been assigned to a particular topic identified during the topic modeling phase. This allows you to create episodes focused on specific themes (e.g., an episode on \"LLM Training Techniques\" or \"Applications of Generative AI\").\n",
        "*   **Using Archetype Tags:** Group blocks based on the archetype tags assigned during the initial preprocessing or subsequent analysis. This can lead to episodes exploring research related to specific conceptual frameworks or patterns.\n",
        "*   **Using Clusters from RDT (if implemented):** If you have built a system like the RDT (Reciprocal Document Transfer) for semantic similarity search, you could group blocks that are semantically close to each other based on embeddings or other clustering techniques.\n",
        "*   **Manual Selection:** Based on a review of the research blocks, glyphs, or initial topic modeling results, you might manually select a set of interesting blocks for a specific episode.\n",
        "*   **Combining Criteria:** You can combine criteria, e.g., selecting blocks that belong to a specific topic AND are tagged with a certain archetype.\n",
        "\n",
        "The output of this step is a collection of processed research blocks that will form the basis of a single podcast episode.\n",
        "\n",
        "### 2. Generating a Narrative Script for a Podcast Episode\n",
        "\n",
        "Creating a compelling narrative script from a collection of technical research blocks requires transforming raw information into an engaging audio format.\n",
        "\n",
        "*   **Summarize Key Points:** For each selected research block, extract the core findings, important concepts, and significant insights. This can be done programmatically (e.g., using extractive or abstractive summarization techniques) or through a human review process.\n",
        "*   **Synthesize Across Blocks:** Identify common threads, contrasting ideas, or a logical flow across the selected blocks. Structure the script to present these points in a coherent sequence. If using clustered blocks, highlight the relationships and nuances within the cluster.\n",
        "*   **Incorporate Insights from Glyphs/Metadata:** Reference the visual glyphs associated with the blocks where appropriate in the narrative. For example, \"As visualized by the 'complexity' glyph, this research highlights the intricate nature of...\" or \"The metadata indicates this block discusses a technique particularly relevant to the 'innovation' archetype.\" Use metadata like timestamps, authors (if available), or source tools to add context.\n",
        "*   **Add Narrative Layer:** Weave the summarized points and insights into a narrative flow. This involves using transition phrases, maintaining a consistent tone, and ensuring the language is suitable for an audio format.\n",
        "*   **Include Introduction and Conclusion:** Start with an introduction that sets the stage, introduces the topic/theme of the episode, and briefly mentions the research sources. Conclude with a summary of the main takeaways and potentially a look ahead.\n",
        "*   **Structure for Audio:** Write the script with spoken delivery in mind. Use shorter sentences, clear language, and consider pacing.\n",
        "\n",
        "**Potential Script Generation Approaches:**\n",
        "\n",
        "*   **Template-Based Generation:** Use predefined script templates that are populated with summarized content and metadata from the selected blocks.\n",
        "*   **Natural Language Generation (NLG):** Employ NLG models (like larger LLMs, potentially via an API if available and suitable for the environment) to generate a more fluid and creative narrative based on the input summaries and metadata.\n",
        "*   **Human Editing/Refinement:** Even with automated generation, human review and editing will likely be necessary to ensure accuracy, clarity, and engagement.\n",
        "\n",
        "### 3. Converting Script to Audio using Text-to-Speech (TTS)\n",
        "\n",
        "Once the narrative script is ready, a TTS engine converts the text into spoken audio.\n",
        "\n",
        "*   **TTS Engine Selection:** Choose a suitable TTS engine. Options include:\n",
        "    *   **Cloud-based TTS APIs:** Services like Google Cloud Text-to-Speech, Amazon Polly, or Microsoft Azure Text to Speech offer high-quality, natural-sounding voices and various language options. These require API access and potentially cost.\n",
        "    *   **Open-source TTS Libraries:** Libraries like `gTTS` (Google Text-to-Speech, unofficial), `pyttsx3` (offline, cross-platform), or models from `transformers` library (e.g., using models like Tacotron2 or FastSpeech2 with a vocoder) can be used. Quality and naturalness vary depending on the library and model.\n",
        "*   **API/Library Integration:** Integrate the chosen TTS engine into your workflow (likely within a **Code Node** in n8n or a dedicated script).\n",
        "*   **Text-to-Audio Conversion:** Pass the generated narrative script to the TTS engine's API or function. Specify the desired voice (male/female, accent, speaking style) and output format (MP3 is common for podcasts).\n",
        "*   **Audio File Generation:** The TTS engine will return the audio data, which you should save as an audio file (e.g., `episode_<theme>_<date>.mp3`).\n",
        "\n",
        "**Potential Python Libraries for TTS:**\n",
        "\n",
        "*   `gTTS`: Simple interface to Google's online TTS. Requires internet access.\n",
        "*   `pyttsx3`: An offline, cross-platform TTS library. Uses native speech engines.\n",
        "*   `transformers` (Hugging Face): Provides access to various state-of-the-art TTS models, often requiring additional dependencies and potentially significant computational resources.\n",
        "\n",
        "Using cloud-based APIs generally provides higher quality and more voice options but adds external dependencies and costs. Open-source libraries offer more control and can run offline but might have less natural-sounding voices.\n",
        "\n",
        "### 4. Generating Show Notes from Metadata\n",
        "\n",
        "Show notes provide listeners with supplementary information and context for the episode.\n",
        "\n",
        "*   **Extract Relevant Metadata:** Gather metadata from the selected research blocks, such as titles, authors (if available), source URLs, dates, and assigned topics or archetypes.\n",
        "*   **Include Script Highlights:** Extract key quotes or summary points directly from the generated narrative script.\n",
        "*   **Format Show Notes:** Structure the extracted information into a readable format suitable for a podcast episode description or a separate webpage. This could include:\n",
        "    *   Episode Title and Summary\n",
        "    *   List of Research Blocks Covered (with links if available)\n",
        "    *   Key Concepts or Topics Discussed\n",
        "    *   Relevant Archetypes\n",
        "    *   Links to Related Resources (e.g., original papers if accessible)\n",
        "    *   Timestamp markers for different segments (optional, requires aligning script points with audio timestamps).\n",
        "*   **Automate Generation:** This process can be largely automated by creating a template for the show notes and populating it with the collected metadata and script highlights.\n",
        "\n",
        "### 5. Storing Generated Audio Files and Show Notes\n",
        "\n",
        "Organized storage is essential for managing your generated podcast episodes.\n",
        "\n",
        "*   **Directory Structure:** Use a logical directory structure to store audio files and show notes, linked to the themes or dates of the episodes.\n",
        "    *   `/podcast_episodes/`\n",
        "    *   `/podcast_episodes/by_theme/LLM_Training_Techniques/episode_2025-10-03.mp3`\n",
        "    *   `/podcast_episodes/by_theme/LLM_Training_Techniques/episode_2025-10-03.md` (or .json, .txt)\n",
        "    *   `/podcast_episodes/by_date/2025-10-03/episode_LLM_Training_Techniques.mp3`\n",
        "*   **File Naming:** Use a consistent naming convention that includes the episode theme/topic and date.\n",
        "*   **Linking to Original Data:** Ensure that the metadata stored with the podcast episode (e.g., in a separate JSON file or within the show notes) includes references or links (like HCDI names or unique IDs) back to the original research blocks that were used to create the episode. This traceability is crucial for auditing and understanding the source material.\n",
        "*   **Centralized Metadata Index:** Maintain a central index (e.g., in a database or a structured file) that lists all generated episodes, their themes, file paths, and links to the source research blocks.\n",
        "\n",
        "### 6. Considerations for Podcast Generation\n",
        "\n",
        "Several factors need consideration to produce high-quality and useful podcast episodes:\n",
        "\n",
        "*   **TTS Voice Selection:** Choose a voice that is clear, natural, and appropriate for the content. Experiment with different voices offered by your chosen TTS engine.\n",
        "*   **Audio File Size and Format:** MP3 is standard for podcasts. Consider the bitrate to balance audio quality and file size. For very long episodes, you might need to split the audio into smaller segments.\n",
        "*   **Narrative Flow and Engagement:** While automation can help, the script needs to flow well and be engaging for listeners. This might require iterative refinement of the script generation process and potentially some human editing.\n",
        "*   **Background Music/Sound Effects:** Consider adding intro/outro music or subtle background sounds to enhance the listening experience (this would require additional audio processing steps).\n",
        "*   **Consistency:** Maintain a consistent format and style for episodes covering similar themes.\n",
        "*   **Content Accuracy:** Ensure the summaries and narrative accurately reflect the content of the research blocks.\n",
        "*   **Computational Resources:** TTS conversion, especially with advanced models, can be computationally intensive. Ensure your environment has sufficient resources.\n",
        "\n",
        "By implementing these steps, you can automate the creation of podcast episodes that summarize and present the key insights from your collected LLM research history, making the information more accessible and digestible.\n",
        "\"\"\"\n",
        "print(markdown_explanation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a457f6e"
      },
      "source": [
        "## Automate the workflow\n",
        "\n",
        "### Subtask:\n",
        "Integrate the collection, glyph generation, and podcast generation steps into an automated workflow. This can leverage the `todo.json` queue and scheduling mechanisms discussed previously to run periodically without manual triggering.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49ee6a0c"
      },
      "source": [
        "**Reasoning**:\n",
        "Explain how to automate the workflow using the `todo.json` queue and a scheduler, detailing the steps and benefits, in a markdown cell as this subtask is purely explanatory.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87c69a62"
      },
      "source": [
        "markdown_explanation = \"\"\"\n",
        "## Automating the Workflow with a Queue and Scheduler\n",
        "\n",
        "This subtask focuses on integrating the previously discussed steps (collection, glyph generation, podcast generation) into a cohesive, automated workflow leveraging a `todo.json` queue and a scheduler. This automation is key to continuously processing new LLM research history without manual intervention.\n",
        "\n",
        "### 1. Using the `todo.json` Queue\n",
        "\n",
        "The `todo.json` file serves as a central queue to manage the data flow and task sequence for the automated workflow. Each entry in the `todo.json` file represents a unit of work that needs to be processed through one or more stages of the pipeline.\n",
        "\n",
        "*   **Structure:** Each entry in `todo.json` could be a JSON object representing a raw or partially processed research block. It should include:\n",
        "    *   A unique identifier for the research block.\n",
        "    *   The raw text content (initially).\n",
        "    *   Metadata (source, timestamp, etc.).\n",
        "    *   Status flags or a `processing_stage` field (e.g., \"raw\", \"preprocessed\", \"glyph_generated\", \"podcast_ready\").\n",
        "    *   File paths to generated artifacts (e.g., `processed_file_path`, `glyph_file_path`, `audio_file_path`).\n",
        "*   **Managing Task Sequence:** The automated workflow reads from `todo.json`. Each step of the workflow would look for entries that are ready for its specific stage.\n",
        "    *   The **Collection/Preprocessing** step looks for entries in a \"raw\" state (or adds new entries from collected data). Upon completion, it updates the `processing_stage` to \"preprocessed\" and adds the `processed_file_path`.\n",
        "    *   The **Glyph Generation** step looks for entries in a \"preprocessed\" state. Upon completion, it updates the `processing_stage` to \"glyph_generated\" and adds the `glyph_file_path`.\n",
        "    *   The **Podcast Generation** step might look for groups of entries (based on topic or archetype) where all blocks within the group are in a \"glyph_generated\" state. Upon completion for a group (resulting in one podcast episode), it could update the status of the involved blocks (e.g., to \"podcast_included\") and add the `audio_file_path` to a relevant metadata entry.\n",
        "*   **Decoupling Steps:** The `todo.json` queue decouples the different stages. If one stage fails, it doesn't necessarily block other stages from processing different items. It also allows for easier debugging and reprocessing of specific items by manually adjusting their status in the queue.\n",
        "\n",
        "### 2. Configuring a Scheduler for Periodic Triggering\n",
        "\n",
        "A scheduler is needed to initiate the automated workflow at regular intervals. This eliminates the need for manual triggers.\n",
        "\n",
        "*   **Options:**\n",
        "    *   **Cron (Unix/Linux):** A standard command-line utility for scheduling tasks. You can configure a cron job to execute a script that starts your workflow processing (e.g., a Python script that reads `todo.json` and triggers the relevant processing functions).\n",
        "    *   **n8n Schedule Node:** If using n8n as the automation platform, the **Schedule Node** is a built-in way to trigger a workflow at specified times or intervals (e.g., every hour, daily at midnight).\n",
        "    *   **Operating System Schedulers:** Windows Task Scheduler or similar tools on other operating systems can also be used to trigger scripts.\n",
        "    *   **Cloud-based Schedulers:** If running on a cloud platform, services like AWS CloudWatch Events, Google Cloud Scheduler, or Azure Scheduler can trigger functions or containers that run your workflow.\n",
        "*   **Configuration:** The scheduler is configured to run the main workflow entry point (a script or the n8n workflow) according to the desired frequency. The frequency should be chosen based on how often you expect new research data to be available and how quickly you need updated outputs.\n",
        "\n",
        "### 3. Workflow Representation in an Automation Platform (e.g., n8n)\n",
        "\n",
        "Using an automation platform like n8n provides a visual way to represent and connect the different steps of the workflow.\n",
        "\n",
        "*   **Nodes:** Each major step of the process (or smaller sub-steps) would typically be represented by a node:\n",
        "    *   **Schedule Trigger Node:** Starts the workflow.\n",
        "    *   **File Reading Node:** Reads the `todo.json` queue.\n",
        "    *   **Filter/Conditional Node:** Checks the `processing_stage` of each item to route it to the correct next step.\n",
        "    *   **Code Node (Preprocessing):** Executes the Python script for cleaning, chunking, and initial tagging. Updates the `todo.json` entry.\n",
        "    *   **Code Node (Glyph Generation):** Executes the Python script for generating SVG glyphs. Updates the `todo.json` entry.\n",
        "    *   **Code Node (Podcast Content Generation):** Executes the Python script for grouping blocks, generating the script, and calling the TTS engine. Manages related `todo.json` entries (e.g., marking blocks as processed for podcasting).\n",
        "    *   **File Writing Node:** Writes updates back to the `todo.json` file.\n",
        "    *   **Storage Nodes:** Nodes to save the generated glyphs (SVG files) and podcast audio files to a designated storage location (local filesystem, cloud storage like Google Drive or S3).\n",
        "    *   **Error Handling Nodes:** Catch and manage errors at each stage.\n",
        "    *   **Notification Nodes:** Send alerts on success or failure.\n",
        "*   **Connections:** Arrows connect the nodes, defining the flow of data and execution sequence based on the logic (e.g., an item processed by the Preprocessing node is then passed to the Glyph Generation node if successful).\n",
        "\n",
        "### 4. Passing Parameters and Data Between Steps\n",
        "\n",
        "Data and parameters need to be passed between the nodes (steps) in the automated workflow.\n",
        "\n",
        "*   **`todo.json` as the Primary Data Carrier:** The `todo.json` file itself acts as the persistent store for the state and output of each processing step for individual research blocks. Each node reads the relevant information from an entry and writes its output back to that entry or a new associated entry in the file.\n",
        "*   **Node Output:** In n8n, the output of one node becomes the input for the next connected node. Nodes can output JSON data. For example, the \"Preprocessing\" node might output the updated JSON entry for a research block, which is then received by the \"Glyph Generation\" node.\n",
        "*   **File Paths:** Instead of passing large binary data (like SVG content or audio) directly between nodes, it's more efficient to pass file paths. A node generates a file (e.g., an SVG glyph) and writes its path to the `todo.json` entry or the node's output, and the next node uses this path to access the file.\n",
        "*   **Parameters:** Configuration parameters (like the number of topics for the model, the chosen TTS voice, output directories) can be managed:\n",
        "    *   Within the n8n node configuration itself.\n",
        "    *   In a separate configuration file read by the workflow or individual code nodes.\n",
        "    *   As environment variables.\n",
        "\n",
        "### 5. Benefits of Automation\n",
        "\n",
        "Automating this workflow provides significant benefits:\n",
        "\n",
        "*   **Continuous Updates:** Ensures that as new LLM research history is collected (even if manually placed into a designated input folder initially, before a fully automated collection method is established), it is automatically processed, and updated glyphs and podcast episodes are generated periodically.\n",
        "*   **Reduced Manual Effort:** Eliminates the need for manual triggering of each step, freeing up time and reducing the potential for human error.\n",
        "*   **Scalability:** A well-designed automated workflow can handle increasing volumes of research data more efficiently than manual processing.\n",
        "*   **Consistency:** Ensures that the processing steps are applied consistently every time the workflow runs.\n",
        "*   **Timeliness:** Provides more timely insights and outputs compared to manual, ad-hoc processing.\n",
        "*   **Traceability:** Using the `todo.json` queue and linking artifacts back to original research blocks provides a clear audit trail of what was processed and what was generated.\n",
        "\n",
        "By implementing this automated workflow using a `todo.json` queue and a scheduler within an integration platform, you create a robust system for continuously transforming raw LLM research history into valuable, easily digestible outputs like visual glyphs and podcast episodes.\n",
        "\"\"\"\n",
        "print(markdown_explanation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a30cc473"
      },
      "source": [
        "## Integration and storage\n",
        "\n",
        "### Subtask:\n",
        "Ensure the generated glyphs, podcast episodes, and any associated metadata are stored in an organized manner (e.g., in Google Drive folders as outlined) and potentially integrated back into your knowledge management tools (like Notion or Obsidian) if desired.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cf071c4"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to explain the importance of organized storage, recommend a storage structure, explain integration with knowledge management tools, discuss enriching metadata, and mention the benefits of integration. All these points can be covered comprehensively in a markdown explanation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b63c645e"
      },
      "source": [
        "markdown_explanation = \"\"\"\n",
        "## Organized Storage and Knowledge Management Integration\n",
        "\n",
        "This subtask focuses on ensuring that the generated artifacts (glyphs, podcast episodes, and associated metadata) are stored in an organized manner and potentially integrated back into your knowledge management tools.\n",
        "\n",
        "### 1. Importance of Organized Storage\n",
        "\n",
        "Organized storage is crucial for several reasons:\n",
        "\n",
        "*   **Accessibility:** Makes it easy to find and retrieve specific glyphs, audio files, or metadata based on themes, dates, or source material.\n",
        "*   **Traceability:** Allows you to link back from the generated artifacts to the original research blocks and their sources.\n",
        "*   **Maintainability:** Simplifies managing the growing collection of generated content, making it easier to archive, backup, or clean up old files.\n",
        "*   **Integration:** Provides a structured basis for integrating the artifacts into other tools and workflows.\n",
        "*   **Backup and Redundancy:** Storing data in a well-defined structure facilitates regular backups and ensures that your valuable generated content is not lost.\n",
        "\n",
        "### 2. Recommended Storage Structure\n",
        "\n",
        "A recommended storage structure leverages nested folders and cloud storage for accessibility and backup:\n",
        "\n",
        "*   **Base Directory:** A main directory for all generated outputs, e.g., `/generated_llm_research/`.\n",
        "*   **Artifact Type Subdirectories:** Create subdirectories for each type of artifact:\n",
        "    *   `/generated_llm_research/glyphs/`\n",
        "    *   `/generated_llm_research/podcast_episodes/`\n",
        "    *   `/generated_llm_research/metadata/` (for central metadata files or indices)\n",
        "*   **Organization within Subdirectories:** Within each artifact subdirectory, use further nested folders based on criteria relevant to your workflow:\n",
        "    *   **By Topic/Theme:** `/generated_llm_research/podcast_episodes/topic_modeling/episode_llm_training.mp3`\n",
        "    *   **By Date:** `/generated_llm_research/glyphs/2025/10/03/glyph_abcd12ef.svg`\n",
        "    *   **By Source:** `/generated_llm_research/metadata/source_chatgpt/block_ghi34jk5.json`\n",
        "    *   **By Archetype:** `/generated_llm_research/glyphs/archetype_innovation/glyph_lmn67op8.svg`\n",
        "*   **File Naming:** Use consistent, descriptive file names that include relevant identifiers (e.g., unique block ID, topic name, date).\n",
        "*   **Cloud Storage:** Store this entire structure in a cloud storage service like **Google Drive**, Dropbox, or OneDrive.\n",
        "    *   **Accessibility:** Allows access from multiple devices and platforms.\n",
        "    *   **Backup and Sync:** Provides automatic backup and synchronization.\n",
        "    *   **Sharing:** Facilitates sharing artifacts with others if needed.\n",
        "    *   **Integration:** Many knowledge management tools and automation platforms have built-in integrations with cloud storage services.\n",
        "\n",
        "### 3. Integration with Knowledge Management Tools (Notion, Obsidian)\n",
        "\n",
        "Integrating the generated artifacts and their metadata back into knowledge management tools like Notion or Obsidian enhances their value by linking research history with its visual and audio summaries.\n",
        "\n",
        "*   **Linking to Files in Cloud Storage:** The most common method is to add links within your Notion pages or Obsidian notes that point directly to the files stored in Google Drive (or other cloud storage). Cloud storage services provide shareable links for individual files.\n",
        "    *   **Notion:** Create a \"URL\" property in your database entries (e.g., for a research block or a topic page) and paste the shareable link to the corresponding glyph SVG or podcast MP3 file.\n",
        "    *   **Obsidian:** Use standard Markdown link syntax `[Glyph SVG](link_to_file_in_drive)` within your notes.\n",
        "*   **Embedding Content:** Some tools allow embedding content directly from cloud storage or by using embed codes.\n",
        "    *   **Notion:** Supports embedding content from Google Drive, allowing you to view PDFs, documents, or even potentially render SVGs directly within the page.\n",
        "    *   **Obsidian:** Plugins might exist for embedding specific file types or integrating with cloud storage services. You can embed images (including SVG if supported) using `![]()` syntax if the file is locally accessible or linked correctly.\n",
        "*   **Using APIs/Plugins for More Direct Integration:**\n",
        "    *   **Notion API:** The Notion API allows programmatic creation and updating of database pages. You could potentially write a script (or use an n8n workflow) to create a new Notion page for each generated podcast episode or update existing research block entries with links or properties pointing to the generated artifacts.\n",
        "    *   **Obsidian Plugins:** The Obsidian community has developed various plugins. Look for plugins that integrate with cloud storage, media embedding, or potentially offer ways to import structured metadata.\n",
        "    *   **n8n Integration Nodes:** As mentioned in the automation section, n8n has dedicated nodes for Notion and other services, enabling automated updates based on your workflow.\n",
        "\n",
        "### 4. Enriching Knowledge Management Entries with Metadata\n",
        "\n",
        "The metadata generated throughout the workflow can significantly enrich the entries in your knowledge management tools, providing valuable context for the linked artifacts.\n",
        "\n",
        "*   **Metadata from Preprocessing:** Original source URL, timestamp, author, initial archetype tags.\n",
        "*   **Metadata from Topic Modeling:** Assigned topic(s) with their probabilities/weights.\n",
        "*   **Metadata from Glyph Generation:** Keywords used, sentiment score, link to the generated SVG file.\n",
        "*   **Metadata from Podcast Generation:** Link to the audio file, generated show notes summary, list of included research blocks.\n",
        "\n",
        "Use these metadata points as properties in your Notion database or frontmatter/inline fields in your Obsidian notes.\n",
        "\n",
        "*   **Notion Database Properties:** Create properties for \"Topic,\" \"Archetype,\" \"Source URL,\" \"Podcast Episode Link,\" \"Glyph SVG Link,\" \"Keywords,\" etc. This makes your database sortable, filterable, and searchable based on these criteria.\n",
        "*   **Obsidian Frontmatter/Inline Fields:** Use YAML frontmatter at the top of your Markdown files (`--- topic: [LLM Training] keywords: [model, data, training] ---`) or inline fields (`Topic:: [[LLM Training]] Keywords:: model, data, training`) to store metadata directly in your notes.\n",
        "\n",
        "This metadata provides context for the glyphs and podcast episodes, explains *why* they were generated, and links them back to the underlying research.\n",
        "\n",
        "### 5. Benefits of this Integration\n",
        "\n",
        "Integrating generated artifacts and metadata into your knowledge management tools offers numerous benefits:\n",
        "\n",
        "*   **Searchable and Interconnected Knowledge Base:** Creates a central, searchable repository of your LLM research history, complete with visual and audio summaries. You can easily find research on a specific topic, see its visual representation, and listen to a summarized podcast episode.\n",
        "*   **Enhanced Understanding:** The combination of original text, metadata, glyphs, and audio provides multiple modalities for understanding the research, catering to different learning preferences.\n",
        "*   **Improved Organization:** Your knowledge management tool becomes a dynamic index for all your research and generated content.\n",
        "*   **Facilitates Sharing and Collaboration:** Makes it easier to share curated research insights, visual summaries, and audio content with others.\n",
        "*   **Basis for Further Analysis:** The structured data and links within your knowledge management tool can serve as a foundation for further analysis or content creation.\n",
        "\n",
        "By carefully planning your storage structure and leveraging the integration capabilities of your knowledge management tools, you can effectively manage the outputs of your automated workflow and create a rich, interconnected knowledge base of LLM research history.\n",
        "\"\"\"\n",
        "print(markdown_explanation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "039fe6b6"
      },
      "source": [
        "## Refinement and iteration\n",
        "\n",
        "### Subtask:\n",
        "Establish a process for reviewing the generated glyphs and podcast episodes, gathering feedback, and refining the algorithms, preprocessing steps, narrative generation, and TTS settings to improve the quality and relevance of the outputs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ce20dfe"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to explain the importance of continuous refinement, methods for evaluating topic model quality, how feedback informs refinement, how to iterate on various aspects of the model, and the role of monitoring the automated workflow. All these points can be covered in a single markdown explanation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1c9ad9d"
      },
      "source": [
        "# Explain the importance of continuous refinement and related aspects\n",
        "\n",
        "markdown_explanation = \"\"\"\n",
        "## Continuously Refining Your Topic Modeling Approach\n",
        "\n",
        "Topic modeling is not a one-time process. To ensure the identified topics remain relevant, accurate, and useful over time, continuous refinement is essential. Data evolves, your needs change, and initial model parameters might not be optimal in the long run.\n",
        "\n",
        "### 1. Importance of Continuous Refinement\n",
        "\n",
        "*   **Adapting to Evolving Data:** The language and themes in your data sources (GitHub issues, communication messages) can change over time. New projects, tools, or discussions introduce new terminology and topics. Continuous refinement allows your model to capture these changes.\n",
        "*   **Improving Topic Quality:** Initial topic models might produce topics that are not clearly defined, contain irrelevant words, or overlap significantly. Refinement helps in obtaining more coherent and interpretable topics.\n",
        "*   **Maintaining Relevance:** As your team's focus or project goals shift, the topics you are interested in might also change. Refining the model can help align the discovered topics with your current analytical needs.\n",
        "*   **Optimizing Performance:** Iterating on the model and preprocessing steps can improve the performance of topic assignment and the overall utility of the topic model.\n",
        "\n",
        "### 2. Methods for Evaluating Topic Model Quality\n",
        "\n",
        "Evaluating the quality of a topic model can be done using a combination of automatic metrics and human judgment:\n",
        "\n",
        "*   **Topic Coherence:** This metric measures the semantic similarity between the high-scoring words in a topic. A high coherence score suggests that the words within a topic are related and the topic is interpretable. There are different variations of coherence scores (e.g., UMass, C_v).\n",
        "*   **Perplexity:** This metric, commonly used with LDA, measures how well the model predicts a held-out set of documents. A lower perplexity generally indicates a better model, but it doesn't always correlate perfectly with human interpretability.\n",
        "*   **Human Evaluation:** This is arguably the most important evaluation method. It involves subject matter experts reviewing the top words for each topic and a sample of documents assigned to those topics to assess their interpretability and relevance. This can also involve tasks like asking humans to match topics to documents or rate topic quality.\n",
        "*   **Qualitative Analysis of Topic Assignments:** Reviewing how topics are assigned to documents in your integrated tools can reveal if the assignments make sense and are useful for your workflow.\n",
        "\n",
        "### 3. How Feedback Informs the Refinement Process\n",
        "\n",
        "Feedback from various sources should inform your refinement efforts:\n",
        "\n",
        "*   **Visualization Feedback:**\n",
        "    *   **Word Clouds:** If word clouds show irrelevant words or a mix of unrelated terms in a topic, it indicates issues with preprocessing or the need to adjust the number of topics.\n",
        "    *   **Intertopic Distance Maps:** Overlapping topic clusters might suggest reducing the number of topics, while isolated topics could be too specific or noisy.\n",
        "    *   **Topic Distribution Plots:** If documents are not clearly assigned to dominant topics or have very flat distributions, it might indicate the need for model tuning or more topics.\n",
        "*   **Feedback from Integrated Tools:**\n",
        "    *   **User Feedback:** Users interacting with topic tags in Notion or GitHub might report that topic assignments are inaccurate, confusing, or not useful. This is direct feedback on the model's practical utility.\n",
        "    *   **Workflow Effectiveness:** If the automated workflows based on topic assignments are not producing the desired results (e.g., incorrect routing of issues), it signals a problem with the underlying topic model.\n",
        "*   **Human Evaluation Results:** Explicit human evaluation provides structured feedback on topic interpretability and relevance, guiding adjustments to the model or the number of topics.\n",
        "\n",
        "### 4. Iterating on Preprocessing, Number of Topics, and Algorithm\n",
        "\n",
        "Refinement is an iterative process involving adjustments to different parts of the topic modeling pipeline:\n",
        "\n",
        "*   **Iterating on Preprocessing:**\n",
        "    *   **Stop Word Lists:** Add domain-specific stop words (e.g., project names, common jargon) that don't contribute to topics. Remove words that appear to be stop words but were missed.\n",
        "    *   **Tokenization:** Adjust tokenization rules to handle specific characters or phrases relevant to your data (e.g., keeping hashtags if they represent topics).\n",
        "    *   **Stemming/Lemmatization:** Experiment with or without these techniques, or try different approaches, to see what produces more coherent topics.\n",
        "    *   **Handling Special Characters/Code:** Improve rules for removing or handling code snippets, links, mentions, etc., that might introduce noise.\n",
        "*   **Iterating on the Number of Topics:**\n",
        "    *   Start with a reasonable range based on domain knowledge or initial evaluation.\n",
        "    *   Train models with different numbers of topics.\n",
        "    *   Evaluate the models using coherence scores, perplexity, and human evaluation.\n",
        "    *   Use intertopic distance maps to visually assess topic separation. Choose the number of topics that yields interpretable and well-separated themes relevant to your needs.\n",
        "*   **Iterating on the Topic Modeling Algorithm:**\n",
        "    *   If using LDA, experiment with hyperparameters like alpha and beta.\n",
        "    *   If using NMF, consider different initializations or parameters.\n",
        "    *   If one algorithm isn't performing well, consider trying a different one (e.g., switching from LDA to NMF or vice versa) or exploring more advanced techniques if necessary.\n",
        "\n",
        "### 5. Role of Monitoring the Automated Workflow\n",
        "\n",
        "Monitoring the automated workflow is critical for continuous refinement and ensuring the system's reliability:\n",
        "\n",
        "*   **Execution Monitoring:** Regularly check the execution logs of your n8n workflow to identify any failures or errors in data collection, preprocessing, topic modeling, or integration steps.\n",
        "*   **Data Quality Monitoring:** Implement checks to ensure the collected data is in the expected format and volume.\n",
        "*   **Topic Model Output Monitoring:** Periodically review the output of the topic modeling step within the workflow logs or by outputting results to a staging area. Check for unexpected topics or significant shifts in topic distributions that might indicate issues or the need for retraining.\n",
        "*   **Integration Monitoring:** Verify that the topic information is being correctly added or updated in your target tools (Notion, GitHub).\n",
        "*   **Performance Monitoring:** Track the execution time of the workflow, especially the computationally intensive topic modeling step, to ensure it completes within acceptable limits.\n",
        "*   **Alerting:** Set up alerts for workflow failures or significant anomalies in the monitoring metrics.\n",
        "\n",
        "By continuously monitoring and refining your topic modeling approach based on evaluation metrics, visualization feedback, user input, and workflow performance, you can maintain a robust and valuable system for understanding your data across your integrated tools.\n",
        "\"\"\"\n",
        "\n",
        "print(markdown_explanation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "663777ef"
      },
      "source": [
        "### Summary::\\**`bold text`**\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The initial step of collecting and normalizing LLM research histories from external sources like ChatGPT, Claude, Gemini, and Grok logs failed due to security and privacy constraints preventing access to external data.\n",
        "*   A conceptual process for generating visual glyphs from processed text was outlined, involving extracting features (keywords, archetypes, sentiment) and mapping them to visual attributes using libraries like `drawSvg` or `svgwrite`.\n",
        "*   A method for storing generated glyphs and metadata was defined, suggesting a structured directory system and separate metadata files (JSON/YAML) or a database.\n",
        "*   The process for generating podcast episodes was detailed, including selecting research blocks based on themes/topics, creating a narrative script (potentially using NLG), converting the script to audio using TTS engines (like cloud-based APIs or open-source libraries), and generating show notes from metadata.\n",
        "*   An automated workflow was planned using a `todo.json` queue to manage the processing stages of research blocks and a scheduler (like cron or n8n's schedule node) to trigger the workflow periodically.\n",
        "*   The importance of organized storage for all generated artifacts (glyphs, audio, metadata) was emphasized, recommending a structured folder system in cloud storage (e.g., Google Drive) for accessibility and backup.\n",
        "*   Integration with knowledge management tools like Notion or Obsidian was suggested by linking or embedding files and using metadata to enrich entries.\n",
        "*   A process for continuous refinement of the underlying models and processes (especially topic modeling, preprocessing, and the number of topics) was outlined, stressing the importance of feedback from evaluations, visualizations, and workflow monitoring.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The primary roadblock is accessing the raw LLM research history logs. Future steps must prioritize establishing a secure and permissible method for collecting this data, perhaps through user-initiated exports or approved APIs, before the rest of the automation pipeline can be implemented.\n",
        "*   Develop and implement the Python scripts or workflow nodes for glyph generation, podcast scripting, and TTS conversion based on the detailed plans created in the successful subtasks, using placeholder or sample data initially if real data access remains an issue.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add `%load_ext cudf.pandas` before importing pandas to speed up operations using GPU"
      ],
      "metadata": {
        "id": "9Xd3Hd1Jz1VV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add `%load_ext cudf.pandas` before importing pandas to speed up operations using GPU"
      ],
      "metadata": {
        "id": "p5M_0CKOz1wo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext cudf.pandas\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Randomly generated dataset of parking violations-\n",
        "# Define the number of rows\n",
        "num_rows = 1000000\n",
        "\n",
        "states = [\"NY\", \"NJ\", \"CA\", \"TX\"]\n",
        "violations = [\"Double Parking\", \"Expired Meter\", \"No Parking\",\n",
        "              \"Fire Hydrant\", \"Bus Stop\"]\n",
        "vehicle_types = [\"SUBN\", \"SDN\"]\n",
        "\n",
        "# Create a date range\n",
        "start_date = \"2022-01-01\"\n",
        "end_date = \"2022-12-31\"\n",
        "dates = pd.date_range(start=start_date, end=end_date, freq='D')\n",
        "\n",
        "# Generate random data\n",
        "data = {\n",
        "    \"Registration State\": np.random.choice(states, size=num_rows),\n",
        "    \"Violation Description\": np.random.choice(violations, size=num_rows),\n",
        "    \"Vehicle Body Type\": np.random.choice(vehicle_types, size=num_rows),\n",
        "    \"Issue Date\": np.random.choice(dates, size=num_rows),\n",
        "    \"Ticket Number\": np.random.randint(1000000000, 9999999999, size=num_rows)\n",
        "}\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# How does the parking violations change from day to day segmented by vehicle type\n",
        "# Averaged using a 7-day rolling mean\n",
        "\n",
        "daily_counts = df.groupby(['Issue Date', 'Vehicle Body Type']\n",
        "                          ).size().unstack(fill_value=0)\n",
        "\n",
        "# Calculate a 7-day rolling mean of daily violations for each vehicle type\n",
        "rolling_means = daily_counts.rolling(window=7).mean()\n",
        "\n",
        "# Display the rolling means for each vehicle type over time\n",
        "rolling_means.tail(100).plot(figsize=(14, 7),\n",
        "                             title=\"7-Day Rolling Average of Parking Violations by Vehicle Type\")\n",
        "plt.ylabel(\"Average Number of Violations\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ftff3ZEGz2W4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add `%load_ext cudf.pandas` before importing pandas to speed up operations using GPU\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ftl557Lsz2u0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add `%load_ext cuml.accel` before importing sklearn to speed up operations using GPU\n"
      ],
      "metadata": {
        "id": "PT7JO270z3KL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add `%load_ext cuml.accel` before importing sklearn to speed up operations using GPU\n"
      ],
      "metadata": {
        "id": "Th8AU2Bzz3g3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add `%load_ext cuml.accel` before importing sklearn to speed up operations using GPU\n"
      ],
      "metadata": {
        "id": "xhPRwtblz33-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add `%load_ext cuml.accel` before importing sklearn to speed up operations using GPU\n"
      ],
      "metadata": {
        "id": "4fuqI5_Sz4MZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add `%load_ext cuml.accel` before importing sklearn to speed up operations using GPU\n"
      ],
      "metadata": {
        "id": "T6RI74lNz4gD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add `%load_ext cuml.accel` before importing sklearn to speed up operations using GPU\n"
      ],
      "metadata": {
        "id": "Rj_hKjJrz47o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add `%load_ext cuml.accel` before importing sklearn to speed up operations using GPU\n"
      ],
      "metadata": {
        "id": "PalNMwz9z5Nq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add `%load_ext cudf.pandas` before importing pandas to speed up operations using GPU"
      ],
      "metadata": {
        "id": "etwhZWZ7z505"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Example form fields\n",
        "# @markdown Forms support many types of fields.\n",
        "\n",
        "no_type_checking = ''  # @param\n",
        "string_type = 'example'  # @param {type: \"string\"}\n",
        "slider_value = 142  # @param {type: \"slider\", min: 100, max: 200}\n",
        "number = 102  # @param {type: \"number\"}\n",
        "date = '2010-11-05'  # @param {type: \"date\"}\n",
        "pick_me = \"monday\"  # @param ['monday', 'tuesday', 'wednesday', 'thursday']\n",
        "select_or_input = \"apples\" # @param [\"apples\", \"bananas\", \"oranges\"] {allow-input: true}\n",
        "# @markdown ---\n"
      ],
      "metadata": {
        "id": "PQ0Qyohbz6Og"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Example form fields\n",
        "# @markdown Forms support many types of fields.\n",
        "\n",
        "no_type_checking = ''  # @param\n",
        "string_type = 'example'  # @param {type: \"string\"}\n",
        "slider_value = 142  # @param {type: \"slider\", min: 100, max: 200}\n",
        "number = 102  # @param {type: \"number\"}\n",
        "date = '2010-11-05'  # @param {type: \"date\"}\n",
        "pick_me = \"monday\"  # @param ['monday', 'tuesday', 'wednesday', 'thursday']\n",
        "select_or_input = \"apples\" # @param [\"apples\", \"bananas\", \"oranges\"] {allow-input: true}\n",
        "# @markdown ---\n"
      ],
      "metadata": {
        "id": "w7GN3Av9z6iO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ee\n",
        "import geemap\n",
        "ee.Initialize()\n",
        "m = geemap.Map()\n",
        "\n",
        "# Load a Landsat 8 image\n",
        "image = (ee.ImageCollection('LANDSAT/LC08/C02/T1_L2')\n",
        "              .filter(ee.Filter.greaterThan('CLOUD_COVER', 30))\n",
        "              .filter(ee.Filter.lessThan('CLOUD_COVER', 60)).first())\n",
        "\n",
        "# Define a function to mask clouds using the QA_PIXEL band\n",
        "def maskL8sr(image):\n",
        "  # Bit 2 is cirrus, bit 3 is cloud , bit 4 is cloud shadow\n",
        "  cloudShadowBitMask = (1 << 2) | (1 << 3) | (1 << 4)\n",
        "  # Get the pixel QA band\n",
        "  qa = image.select('QA_PIXEL')\n",
        "  # Both flags should be set to zero, indicating clear conditions.\n",
        "  mask = qa.bitwiseAnd(cloudShadowBitMask).eq(0)\n",
        "  return image.updateMask(mask)\n",
        "\n",
        "# Apply the cloud mask\n",
        "masked_image = maskL8sr(image)\n",
        "\n",
        "# Define visualization parameters\n",
        "vis_params = {\n",
        "  'bands': ['SR_B4', 'SR_B3', 'SR_B2'],\n",
        "  'min': 0,\n",
        "  'max': 50000\n",
        "}\n",
        "\n",
        "# Center the map on the image\n",
        "m.centerObject(image, 7)\n",
        "\n",
        "# Add both original and masked images to the map\n",
        "m.add_layer(image, vis_params, 'Original Image')\n",
        "m.add_layer(masked_image, vis_params, 'Masked Image')\n",
        "m"
      ],
      "metadata": {
        "id": "3bqZStfpz7Aq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "PROJECT_ID = \"\" # @param {type: \"string\"}\n",
        "auth.authenticate_user(project_id=PROJECT_ID)"
      ],
      "metadata": {
        "id": "78st_pqMz7b4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ee\n",
        "import geemap\n",
        "ee.Initialize()\n",
        "m = geemap.Map()\n",
        "\n",
        "# Define coordinates for Paris\n",
        "x = 2.3522\n",
        "y = 48.8566\n",
        "\n",
        "# Create a point geometry for Paris\n",
        "paris_point = ee.Geometry.Point([x, y])\n",
        "\n",
        "# Create a 10km buffer around Paris\n",
        "buffer = paris_point.buffer(10000)\n",
        "\n",
        "# Change the dates as necessary.\n",
        "start_date = '2023-07-01'\n",
        "end_date = '2023-07-15'\n",
        "\n",
        "# Load a Sentinel-2 composite. It's also possible to use median() or other ways\n",
        "# to produce a composite.\n",
        "image = (ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')\n",
        "              .filterDate(start_date, end_date)\n",
        "              .mosaic())\n",
        "\n",
        "# Calculate NDVI\n",
        "ndvi = image.normalizedDifference(['B8', 'B4'])\n",
        "\n",
        "# Calculate mean NDVI within the buffer\n",
        "mean_ndvi = ndvi.reduceRegion(\n",
        "  reducer=ee.Reducer.mean(),\n",
        "  geometry=buffer,\n",
        "  scale=100 # Scale in meters, might need to be changed.\n",
        ")\n",
        "\n",
        "# Print the mean NDVI\n",
        "print('Mean NDVI:', mean_ndvi.get('nd').getInfo())\n",
        "\n",
        "# Display the buffer and NDVI on the map\n",
        "m.set_center(x, y, 10)\n",
        "m.add_layer(buffer, {}, 'Buffer')\n",
        "m.add_layer(\n",
        "    ndvi,\n",
        "    {'min': -1, 'max': 1, 'palette': ['red', 'white', 'green']}, 'NDVI')\n",
        "m"
      ],
      "metadata": {
        "id": "E1AADTz9z7x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import ai\n",
        "\n",
        "response = ai.generate_text(\"What is the capital of England\", model_name='google/gemini-2.0-flash-lite')\n",
        "print(response)"
      ],
      "metadata": {
        "id": "BOHUfxhNz8gb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#code is not necessary for colab.ai, but is useful in fomatting text chunks\n",
        "import sys\n",
        "from google.colab import ai\n",
        "\n",
        "\n",
        "class LineWrapper:\n",
        "    def __init__(self, max_length=80):\n",
        "        self.max_length = max_length\n",
        "        self.current_line_length = 0\n",
        "\n",
        "    def print(self, text_chunk):\n",
        "        i = 0\n",
        "        n = len(text_chunk)\n",
        "        while i < n:\n",
        "            start_index = i\n",
        "            while i < n and text_chunk[i] not in ' \\n': # Find end of word\n",
        "                i += 1\n",
        "            current_word = text_chunk[start_index:i]\n",
        "\n",
        "            delimiter = \"\"\n",
        "            if i < n: # If not end of chunk, we found a delimiter\n",
        "                delimiter = text_chunk[i]\n",
        "                i += 1 # Consume delimiter\n",
        "\n",
        "            if current_word:\n",
        "                needs_leading_space = (self.current_line_length > 0)\n",
        "\n",
        "                # Case 1: Word itself is too long for a line (must be broken)\n",
        "                if len(current_word) > self.max_length:\n",
        "                    if needs_leading_space: # Newline if current line has content\n",
        "                        sys.stdout.write('\\n')\n",
        "                        self.current_line_length = 0\n",
        "                    for char_val in current_word: # Break the long word\n",
        "                        if self.current_line_length >= self.max_length:\n",
        "                            sys.stdout.write('\\n')\n",
        "                            self.current_line_length = 0\n",
        "                        sys.stdout.write(char_val)\n",
        "                        self.current_line_length += 1\n",
        "                # Case 2: Word doesn't fit on current line (print on new line)\n",
        "                elif self.current_line_length + (1 if needs_leading_space else 0) + len(current_word) > self.max_length:\n",
        "                    sys.stdout.write('\\n')\n",
        "                    sys.stdout.write(current_word)\n",
        "                    self.current_line_length = len(current_word)\n",
        "                # Case 3: Word fits on current line\n",
        "                else:\n",
        "                    if needs_leading_space:\n",
        "                        # Define punctuation that should not have a leading space\n",
        "                        # when they form an entire \"word\" (token) following another word.\n",
        "                        no_leading_space_punctuation = {\n",
        "                            \",\", \".\", \";\", \":\", \"!\", \"?\",        # Standard sentence punctuation\n",
        "                            \")\", \"]\", \"}\",                     # Closing brackets\n",
        "                            \"'s\", \"'S\", \"'re\", \"'RE\", \"'ve\", \"'VE\", # Common contractions\n",
        "                            \"'m\", \"'M\", \"'ll\", \"'LL\", \"'d\", \"'D\",\n",
        "                            \"n't\", \"N'T\",\n",
        "                            \"...\", \"…\"                          # Ellipses\n",
        "                        }\n",
        "                        if current_word not in no_leading_space_punctuation:\n",
        "                            sys.stdout.write(' ')\n",
        "                            self.current_line_length += 1\n",
        "                    sys.stdout.write(current_word)\n",
        "                    self.current_line_length += len(current_word)\n",
        "\n",
        "            if delimiter == '\\n':\n",
        "                sys.stdout.write('\\n')\n",
        "                self.current_line_length = 0\n",
        "            elif delimiter == ' ':\n",
        "                # If line is full and a space delimiter arrives, it implies a wrap.\n",
        "                if self.current_line_length >= self.max_length:\n",
        "                    sys.stdout.write('\\n')\n",
        "                    self.current_line_length = 0\n",
        "\n",
        "        sys.stdout.flush()\n",
        "\n",
        "\n",
        "wrapper = LineWrapper()\n",
        "for chunk in ai.generate_text('Give me a long winded description about the evolution of the Roman Empire.', model_name='google/gemini-2.0-flash', stream=True):\n",
        "  wrapper.print(chunk)"
      ],
      "metadata": {
        "id": "NWVWImgXz87_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#code is not necessary for colab.ai, but is useful in fomatting text chunks\n",
        "import sys\n",
        "from google.colab import ai\n",
        "\n",
        "\n",
        "class LineWrapper:\n",
        "    def __init__(self, max_length=80):\n",
        "        self.max_length = max_length\n",
        "        self.current_line_length = 0\n",
        "\n",
        "    def print(self, text_chunk):\n",
        "        i = 0\n",
        "        n = len(text_chunk)\n",
        "        while i < n:\n",
        "            start_index = i\n",
        "            while i < n and text_chunk[i] not in ' \\n': # Find end of word\n",
        "                i += 1\n",
        "            current_word = text_chunk[start_index:i]\n",
        "\n",
        "            delimiter = \"\"\n",
        "            if i < n: # If not end of chunk, we found a delimiter\n",
        "                delimiter = text_chunk[i]\n",
        "                i += 1 # Consume delimiter\n",
        "\n",
        "            if current_word:\n",
        "                needs_leading_space = (self.current_line_length > 0)\n",
        "\n",
        "                # Case 1: Word itself is too long for a line (must be broken)\n",
        "                if len(current_word) > self.max_length:\n",
        "                    if needs_leading_space: # Newline if current line has content\n",
        "                        sys.stdout.write('\\n')\n",
        "                        self.current_line_length = 0\n",
        "                    for char_val in current_word: # Break the long word\n",
        "                        if self.current_line_length >= self.max_length:\n",
        "                            sys.stdout.write('\\n')\n",
        "                            self.current_line_length = 0\n",
        "                        sys.stdout.write(char_val)\n",
        "                        self.current_line_length += 1\n",
        "                # Case 2: Word doesn't fit on current line (print on new line)\n",
        "                elif self.current_line_length + (1 if needs_leading_space else 0) + len(current_word) > self.max_length:\n",
        "                    sys.stdout.write('\\n')\n",
        "                    sys.stdout.write(current_word)\n",
        "                    self.current_line_length = len(current_word)\n",
        "                # Case 3: Word fits on current line\n",
        "                else:\n",
        "                    if needs_leading_space:\n",
        "                        # Define punctuation that should not have a leading space\n",
        "                        # when they form an entire \"word\" (token) following another word.\n",
        "                        no_leading_space_punctuation = {\n",
        "                            \",\", \".\", \";\", \":\", \"!\", \"?\",        # Standard sentence punctuation\n",
        "                            \")\", \"]\", \"}\",                     # Closing brackets\n",
        "                            \"'s\", \"'S\", \"'re\", \"'RE\", \"'ve\", \"'VE\", # Common contractions\n",
        "                            \"'m\", \"'M\", \"'ll\", \"'LL\", \"'d\", \"'D\",\n",
        "                            \"n't\", \"N'T\",\n",
        "                            \"...\", \"…\"                          # Ellipses\n",
        "                        }\n",
        "                        if current_word not in no_leading_space_punctuation:\n",
        "                            sys.stdout.write(' ')\n",
        "                            self.current_line_length += 1\n",
        "                    sys.stdout.write(current_word)\n",
        "                    self.current_line_length += len(current_word)\n",
        "\n",
        "            if delimiter == '\\n':\n",
        "                sys.stdout.write('\\n')\n",
        "                self.current_line_length = 0\n",
        "            elif delimiter == ' ':\n",
        "                # If line is full and a space delimiter arrives, it implies a wrap.\n",
        "                if self.current_line_length >= self.max_length:\n",
        "                    sys.stdout.write('\\n')\n",
        "                    self.current_line_length = 0\n",
        "\n",
        "        sys.stdout.flush()\n",
        "\n",
        "\n",
        "wrapper = LineWrapper()\n",
        "for chunk in ai.generate_text('Give me a long winded description about the evolution of the Roman Empire.', model_name='google/gemini-2.0-flash', stream=True):\n",
        "  wrapper.print(chunk)"
      ],
      "metadata": {
        "id": "YsOrUFB-z9TB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#code is not necessary for colab.ai, but is useful in fomatting text chunks\n",
        "import sys\n",
        "from google.colab import ai\n",
        "\n",
        "\n",
        "class LineWrapper:\n",
        "    def __init__(self, max_length=80):\n",
        "        self.max_length = max_length\n",
        "        self.current_line_length = 0\n",
        "\n",
        "    def print(self, text_chunk):\n",
        "        i = 0\n",
        "        n = len(text_chunk)\n",
        "        while i < n:\n",
        "            start_index = i\n",
        "            while i < n and text_chunk[i] not in ' \\n': # Find end of word\n",
        "                i += 1\n",
        "            current_word = text_chunk[start_index:i]\n",
        "\n",
        "            delimiter = \"\"\n",
        "            if i < n: # If not end of chunk, we found a delimiter\n",
        "                delimiter = text_chunk[i]\n",
        "                i += 1 # Consume delimiter\n",
        "\n",
        "            if current_word:\n",
        "                needs_leading_space = (self.current_line_length > 0)\n",
        "\n",
        "                # Case 1: Word itself is too long for a line (must be broken)\n",
        "                if len(current_word) > self.max_length:\n",
        "                    if needs_leading_space: # Newline if current line has content\n",
        "                        sys.stdout.write('\\n')\n",
        "                        self.current_line_length = 0\n",
        "                    for char_val in current_word: # Break the long word\n",
        "                        if self.current_line_length >= self.max_length:\n",
        "                            sys.stdout.write('\\n')\n",
        "                            self.current_line_length = 0\n",
        "                        sys.stdout.write(char_val)\n",
        "                        self.current_line_length += 1\n",
        "                # Case 2: Word doesn't fit on current line (print on new line)\n",
        "                elif self.current_line_length + (1 if needs_leading_space else 0) + len(current_word) > self.max_length:\n",
        "                    sys.stdout.write('\\n')\n",
        "                    sys.stdout.write(current_word)\n",
        "                    self.current_line_length = len(current_word)\n",
        "                # Case 3: Word fits on current line\n",
        "                else:\n",
        "                    if needs_leading_space:\n",
        "                        # Define punctuation that should not have a leading space\n",
        "                        # when they form an entire \"word\" (token) following another word.\n",
        "                        no_leading_space_punctuation = {\n",
        "                            \",\", \".\", \";\", \":\", \"!\", \"?\",        # Standard sentence punctuation\n",
        "                            \")\", \"]\", \"}\",                     # Closing brackets\n",
        "                            \"'s\", \"'S\", \"'re\", \"'RE\", \"'ve\", \"'VE\", # Common contractions\n",
        "                            \"'m\", \"'M\", \"'ll\", \"'LL\", \"'d\", \"'D\",\n",
        "                            \"n't\", \"N'T\",\n",
        "                            \"...\", \"…\"                          # Ellipses\n",
        "                        }\n",
        "                        if current_word not in no_leading_space_punctuation:\n",
        "                            sys.stdout.write(' ')\n",
        "                            self.current_line_length += 1\n",
        "                    sys.stdout.write(current_word)\n",
        "                    self.current_line_length += len(current_word)\n",
        "\n",
        "            if delimiter == '\\n':\n",
        "                sys.stdout.write('\\n')\n",
        "                self.current_line_length = 0\n",
        "            elif delimiter == ' ':\n",
        "                # If line is full and a space delimiter arrives, it implies a wrap.\n",
        "                if self.current_line_length >= self.max_length:\n",
        "                    sys.stdout.write('\\n')\n",
        "                    self.current_line_length = 0\n",
        "\n",
        "        sys.stdout.flush()\n",
        "\n",
        "\n",
        "wrapper = LineWrapper()\n",
        "for chunk in ai.generate_text('Give me a long winded description about the evolution of the Roman Empire.', model_name='google/gemini-2.0-flash', stream=True):\n",
        "  wrapper.print(chunk)"
      ],
      "metadata": {
        "id": "s0ed7Y0Yz90I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Only text-to-text input/output is supported\n",
        "from google.colab import ai\n",
        "\n",
        "response = ai.generate_text(\"What is the capital of France?\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "9W_qRnoNz-dB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import ai\n",
        "\n",
        "stream = ai.generate_text(\"Tell me a short story.\", stream=True)\n",
        "for text in stream:\n",
        "  print(text, end='')"
      ],
      "metadata": {
        "id": "7dH24l98z_OP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ee\n",
        "import geemap\n",
        "ee.Initialize()\n",
        "m = geemap.Map()\n",
        "\n",
        "# Load SRTM DEM\n",
        "srtm = ee.Image('USGS/SRTMGL1_003')\n",
        "\n",
        "# Select elevation band\n",
        "elevation = srtm.select('elevation')\n",
        "\n",
        "# Define visualization parameters\n",
        "vis_params = {\n",
        "  'min': 0,\n",
        "  'max': 4000,\n",
        "  'palette': ['0000ff', '00ffff', 'ffff00', 'ff0000'],\n",
        "}\n",
        "\n",
        "# Add layer to the map\n",
        "m.set_center(-122.3321, 47.6062, 10)\n",
        "m.add_layer(elevation, vis_params, 'Elevation')\n",
        "m"
      ],
      "metadata": {
        "id": "8TMe_LQIz_yo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext cudf.pandas\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define the number of rows\n",
        "num_rows = 1000000\n",
        "\n",
        "states = [\"NY\", \"NJ\", \"CA\", \"TX\"]\n",
        "violations = [\"Double Parking\", \"Expired Meter\", \"No Parking\", \"Fire Hydrant\",\n",
        "              \"Bus Stop\"]\n",
        "vehicle_types = [\"SUBN\", \"SDN\"]\n",
        "\n",
        "# Generate random data for Dataset 1\n",
        "data1 = {\n",
        "    \"Registration State\": np.random.choice(states, size=num_rows),\n",
        "    \"Ticket Number\": np.random.randint(1000000000, 9999999999, size=num_rows)\n",
        "}\n",
        "\n",
        "# Generate random data for Dataset 2\n",
        "data2 = {\n",
        "    \"Ticket Number\": np.random.choice(data1['Ticket Number'], size=num_rows),  # Reusing ticket numbers to ensure matches\n",
        "    \"Violation Description\": np.random.choice(violations, size=num_rows)\n",
        "}\n",
        "\n",
        "# Create DataFrames\n",
        "df1 = pd.DataFrame(data1)\n",
        "df2 = pd.DataFrame(data2)\n",
        "\n",
        "# Perform an inner join on 'Ticket Number'\n",
        "merged_df = pd.merge(df1, df2, on=\"Ticket Number\", how=\"inner\")\n",
        "\n",
        "# Display some of the joined data\n",
        "print(merged_df.head())"
      ],
      "metadata": {
        "id": "xNhQFIjYz506"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext cuml.accel\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "import umap\n",
        "\n",
        "X, y = make_classification(n_samples=100000, n_features=20, n_classes=5, n_informative=5, random_state=0)\n",
        "X_train, _, y_train, _ = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "umap_model = umap.UMAP(n_neighbors=15, n_components=2, random_state=42, min_dist=0.0)\n",
        "X_train_umap = umap_model.fit_transform(X_train)\n",
        "y_train\n",
        "# Plot the UMAP result\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(X_train_umap[:, 0], X_train_umap[:, 1], c=y_train, cmap='Spectral', s=10)\n",
        "plt.colorbar(label=\"Activity\")\n",
        "plt.title(\"UMAP projection\")\n",
        "plt.xlabel(\"UMAP Component 1\")\n",
        "plt.ylabel(\"UMAP Component 2\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TXaoXOqkz5Nq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext cuml.accel\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "X, y = make_classification(n_samples=100000, n_features=100, random_state=0)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=100, max_depth=5, max_features=1.0, n_jobs=-1)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "D_7GtQJ_z47p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext cuml.accel\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
        "pca = PCA(n_components=2)\n",
        "pca.fit(X)\n",
        "\n",
        "print(pca.explained_variance_ratio_)\n",
        "print(pca.singular_values_)"
      ],
      "metadata": {
        "id": "TFSAPYkOz4gD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext cuml.accel\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "X, y = make_classification(n_samples=1000000, n_features=200, random_state=0)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "clf = LogisticRegression()\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "EhMONjy-z4MZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext cuml.accel\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "X, y = make_classification(n_samples=100000, n_features=100, random_state=0)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "y_pred = knn.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "PjqJMSyEz33-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext cuml.accel\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "n_samples = 1000\n",
        "n_features = 2\n",
        "n_clusters = 3\n",
        "X, _ = make_blobs(n_samples=n_samples, n_features=n_features, centers=n_clusters, random_state=42)\n",
        "\n",
        "kmeans = KMeans(n_clusters=n_clusters, max_iter=100)\n",
        "kmeans.fit(X)\n",
        "\n",
        "labels = kmeans.labels_\n",
        "print(silhouette_score(X, labels))"
      ],
      "metadata": {
        "id": "IZRajgi0z3g3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext cuml.accel\n",
        "import hdbscan\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "n_samples = 1000\n",
        "n_features = 2\n",
        "n_clusters = 3\n",
        "X, _ = make_blobs(n_samples=n_samples, n_features=n_features, centers=n_clusters, random_state=42)\n",
        "\n",
        "clus = hdbscan.HDBSCAN()\n",
        "clus.fit(X)\n",
        "\n",
        "print(silhouette_score(X, clus.labels_))"
      ],
      "metadata": {
        "id": "eGYZTV2Pz3KM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext cudf.pandas\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# Define the species categories\n",
        "species_categories = ['setosa', 'versicolor', 'virginica']\n",
        "flower_color_categories = ['red','yellow','green']\n",
        "\n",
        "# Define the range for each attribute based on typical iris flower measurements\n",
        "sepal_length_range = (4.0, 8.0)\n",
        "\n",
        "# Create data for 1,000,000 samples\n",
        "n = 1000000\n",
        "data = {\n",
        "    'sepal_length': [random.uniform(*sepal_length_range) for _ in range(n)],\n",
        "    'flower_color': [random.choice(flower_color_categories) for _ in range(n)],\n",
        "    'species': [random.choice(species_categories) for _ in range(n)]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "df.groupby(['species','flower_color']).size().sort_values(ascending=False)"
      ],
      "metadata": {
        "id": "YESTYRC5z2u1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext cudf.pandas\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define the number of rows\n",
        "num_rows = 1000000\n",
        "\n",
        "# Define the possible values\n",
        "states = [\"NY\", \"NJ\", \"CA\", \"TX\"]\n",
        "violations = [\"Double Parking\", \"Expired Meter\", \"No Parking\", \"Fire Hydrant\", \"Bus Stop\"]\n",
        "vehicle_types = [\"SUBN\", \"SDN\"]\n",
        "\n",
        "start_date = \"2022-01-01\"\n",
        "end_date = \"2022-12-31\"\n",
        "# Create a date range\n",
        "dates = pd.date_range(start=start_date, end=end_date, freq='D')\n",
        "\n",
        "# Generate random data\n",
        "data = {\n",
        "    \"Registration State\": np.random.choice(states, size=num_rows),\n",
        "    \"Violation Description\": np.random.choice(violations, size=num_rows),\n",
        "    \"Vehicle Body Type\": np.random.choice(vehicle_types, size=num_rows),\n",
        "    \"Issue Date\": np.random.choice(dates, size=num_rows),\n",
        "    \"Ticket Number\": np.random.randint(1000000000, 9999999999, size=num_rows)\n",
        "}\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Adding issue weekday based on the \"Issue Date\"\n",
        "weekday_names = {\n",
        "    0: \"Monday\",\n",
        "    1: \"Tuesday\",\n",
        "    2: \"Wednesday\",\n",
        "    3: \"Thursday\",\n",
        "    4: \"Friday\",\n",
        "    5: \"Saturday\",\n",
        "    6: \"Sunday\",\n",
        "}\n",
        "\n",
        "df[\"issue_weekday\"] = df[\"Issue Date\"].dt.weekday.map(weekday_names)\n",
        "\n",
        "# Grouping by issue_weekday and counting the Summons Number\n",
        "df.groupby([\"Issue Date\"])[\"Ticket Number\"\n",
        "].count().sort_values()"
      ],
      "metadata": {
        "id": "GQUd7XvQz1wp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext cudf.pandas\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Randomly generated dataset of parking violations-\n",
        "# Define the number of rows\n",
        "num_rows = 1000000\n",
        "\n",
        "states = [\"NY\", \"NJ\", \"CA\", \"TX\"]\n",
        "violations = [\"Double Parking\", \"Expired Meter\", \"No Parking\",\n",
        "              \"Fire Hydrant\", \"Bus Stop\"]\n",
        "vehicle_types = [\"SUBN\", \"SDN\"]\n",
        "\n",
        "# Create a date range\n",
        "start_date = \"2022-01-01\"\n",
        "end_date = \"2022-12-31\"\n",
        "dates = pd.date_range(start=start_date, end=end_date, freq='D')\n",
        "\n",
        "# Generate random data\n",
        "data = {\n",
        "    \"Registration State\": np.random.choice(states, size=num_rows),\n",
        "    \"Violation Description\": np.random.choice(violations, size=num_rows),\n",
        "    \"Vehicle Body Type\": np.random.choice(vehicle_types, size=num_rows),\n",
        "    \"Issue Date\": np.random.choice(dates, size=num_rows),\n",
        "    \"Ticket Number\": np.random.randint(1000000000, 9999999999, size=num_rows)\n",
        "}\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Which parking violation is most commonly committed by vehicles from various U.S states?\n",
        "\n",
        "(df[[\"Registration State\", \"Violation Description\"]]  # get only these two columns\n",
        " .value_counts()  # get the count of offences per state and per type of offence\n",
        " .groupby(\"Registration State\")  # group by state\n",
        " .head(1)  # get the first row in each group (the type of offence with the largest count)\n",
        " .sort_index()  # sort by state name\n",
        " .reset_index()\n",
        ")"
      ],
      "metadata": {
        "id": "5SU1WxcAz1VW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}