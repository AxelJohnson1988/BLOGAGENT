{
  "id_hash": "dc1904cd6796",
  "summary": "#!/usr/bin/env python3\n\"\"\"\nClustering and Embeddings System for MUSE Pantheon\nGroups MemoryBlocks semantically and generates embeddings\n\"\"\"\nimport sys\nimport argparse\nimport json\nimport logging\nfrom p...",
  "content": "#!/usr/bin/env python3\n\"\"\"\nClustering and Embeddings System for MUSE Pantheon\nGroups MemoryBlocks semantically and generates embeddings\n\"\"\"\nimport sys\nimport argparse\nimport json\nimport logging\nfrom pathlib import Path\nfrom typing import List, Dict, Any, Optional\ntry:\n    import numpy as np\n    NUMPY_AVAILABLE = True\nexcept ImportError:\n    NUMPY_AVAILABLE = False\n    # Simple numpy replacement for basic operations\n    class np:\n        @staticmethod\n        def array(data):\n            return data\n        \n        @staticmethod\n        def zeros(shape):\n            if isinstance(shape, tuple):\n                if len(shape) == 2:\n                    return [[0.0] * shape[1] for _ in range(shape[0])]\n                else:\n                    return [0.0] * shape[0]\n            else:\n                return [0.0] * shape\n        \n        @staticmethod\n        def mean(data, axis=None):\n            if axis is None:\n                flat = [item for sublist in data for item in (sublist if isinstance(sublist, list) else [sublist])]\n                return sum(flat) / len(flat) if flat else 0\n            elif axis == 0:\n                if not data or not data[0]:\n                    return []\n                return [sum(col) / len(data) for col in zip(*data)]\n            else:\n                return [sum(row) / len(row) if row else 0 for row in data]\n        \n        @staticmethod\n        def sqrt(x):\n            return x ** 0.5\n        \n        @staticmethod\n        def sum(data):\n            if isinstance(data, list) and isinstance(data[0], list):\n                return sum(sum(row) for row in data)\n            else:\n                return sum(data)\n        \n        @staticmethod\n        def argmin(data, axis=None):\n            if axis == 1:\n                return [min(range(len(row)), key=lambda i: row[i]) for row in data]\n            else:\n                flat = [item for sublist in data for item in (sublist if isinstance(sublist, list) else [sublist])]\n                return min(range(len(flat)), key=lambda i: flat[i])\n        \n        @staticmethod\n        def any(mask):\n            return any(mask)\n        \n        @staticmethod\n        def allclose(a, b, rtol=1e-4):\n            if len(a) != len(b):\n                return False\n            for i in range(len(a)):\n                if isinstance(a[i], list):\n                    if not np.allclose(a[i], b[i], rtol):\n                        return False\n                else:\n                    if abs(a[i] - b[i]) > rtol * max(abs(a[i]), abs(b[i])):\n                        return False\n            return True\n\nfrom collections import defaultdict\nimport math\nimport random\n\n# Add common directory to path for imports\nsys.path.append(str(Path(__file__).parent.parent.parent / \"common\"))\nfrom memory_block import MemoryBlock\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n\nclass EmbeddingGenerator:\n    \"\"\"Generates embeddings for MemoryBlocks using TF-IDF.\"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize the embedding generator.\"\"\"\n        self.vocabulary = {}\n        self.idf_scores = {}\n        self.stop_words = {\n            'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by',\n            'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has',\n            'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might',\n            'must', 'can', 'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it',\n            'we', 'they', 'my', 'your', 'his', 'her', 'its', 'our', 'their'\n        }\n    \n    def fit_transform(self, memory_blocks: List[MemoryBlock]) -> List[List[float]]:\n        \"\"\"Fit the model and transform memory blocks to embeddings.\"\"\"\n        # Build vocabulary and compute IDF scores\n        self._build_vocabulary(memory_blocks)\n        \n        # Generate TF-IDF vectors\n        embeddings = []\n        for block in memory_blocks:\n            embedding = self._generate_tfidf_vector(block)\n            embeddings.append(embedding)\n        \n        return embeddings\n    \n    def _build_vocabulary(self, memory_blocks: List[MemoryBlock]):\n        \"\"\"Build vocabulary and compute IDF scores.\"\"\"\n        # Collect all documents\n        documents = []\n        for block in memory_blocks:\n            text = f\"{block.summary} {block.content} {' '.join(block.topics)}\"\n            documents.append(self._preprocess_text(text))\n        \n        # Build vocabulary\n        word_counts = defaultdict(int)\n        for doc in documents:\n            for word in set(doc):  # Use set to count document frequency, not word frequency\n                word_counts[word] += 1\n        \n        # Filter vocabulary (remove very rare and very common words)\n        min_df = max(1, len(documents) // 100)  # Minimum document frequency\n        max_df = len(documents) * 0.8  # Maximum document frequency (80%)\n        \n        self.vocabulary = {}\n        vocab_index = 0\n        for word, count in word_counts.items():\n            if min_df <= count <= max_df:\n                self.vocabulary[word] = vocab_index\n                vocab_index += 1\n        \n        # Compute IDF scores\n        self.idf_scores = {}\n        for word, index in self.vocabulary.items():\n            df = word_counts[word]\n            idf = math.log(len(documents) / df)\n            self.idf_scores[word] = idf\n        \n        logger.info(f\"Built vocabulary with {len(self.vocabulary)} words\")\n    \n    def _preprocess_text(self, text: str) -> List[str]:\n        \"\"\"Preprocess text for vectorization.\"\"\"\n        # Simple preprocessing\n        text = text.lower()\n        words = []\n        \n        current_word = \"\"\n        for char in text:\n            if char.isalnum():\n                current_word += char\n            else:\n                if current_word and len(current_word) > 2 and current_word not in self.stop_words:\n                    words.append(current_word)\n                current_word = \"\"\n        \n        # Don't forget the last word\n        if current_word and len(current_word) > 2 and current_word not in self.stop_words:\n            words.append(current_word)\n        \n        return words\n    \n    def _generate_tfidf_vector(self, memory_block: MemoryBlock) -> List[float]:\n        \"\"\"Generate TF-IDF vector for a memory block.\"\"\"\n        text = f\"{memory_block.summary} {memory_block.content} {' '.join(memory_block.topics)}\"\n        words = self._preprocess_text(text)\n        \n        # Count word frequencies\n        word_freq = defaultdict(int)\n        for word in words:\n            word_freq[word] += 1\n        \n        # Generate TF-IDF vector\n        vector = [0.0] * len(self.vocabulary)\n        \n        for word, freq in word_freq.items():\n            if word in self.vocabulary:\n                tf = freq / len(words) if words else 0\n                idf = self.idf_scores[word]\n                tfidf = tf * idf\n                vector[self.vocabulary[word]] = tfidf\n        \n        return vector\n\n\nclass SemanticClustering:\n    \"\"\"Clusters MemoryBlocks based on semantic similarity.\"\"\"\n    \n    def __init__(self, n_clusters: int = None):\n        \"\"\"Initialize the clustering system.\"\"\"\n        self.n_clusters = n_clusters\n        self.cluster_centers = None\n        self.labels = None\n    \n    def fit_predict(self, embeddings: List[List[float]], memory_blocks: List[MemoryBlock]) -> List[int]:\n        \"\"\"Fit clustering model and predict cluster labels.\"\"\"\n        if self.n_clusters is None:\n            # Automatic cluster number selection\n            self.n_clusters = max(2, min(10, len(memory_blocks) // 5))\n        \n        logger.info(f\"Clustering {len(memory_blocks)} blocks into {self.n_clusters} clusters\")\n        \n        # Simple K-means implementation\n        self.labels = self._kmeans_clustering(embeddings)\n        \n        return self.labels\n    \n    def _kmeans_clustering(self, embeddings: List[List[float]], max_iterations: int = 100) -> List[int]:\n        \"\"\"Simple K-means clustering implementation.\"\"\"\n        n_samples = len(embeddings)\n        n_features = len(embeddings[0]) if embeddings else 0\n        \n        if n_samples == 0 or n_features == 0:\n            return []\n        \n        # Initialize centroids randomly\n        centroids = []\n        for k in range(self.n_clusters):\n            centroid = [random.random() for _ in range(n_features)]\n            centroids.append(centroid)\n        \n        for iteration in range(max_iterations):\n            # Assign points to closest centroid\n            distances = self._calculate_distances(embeddings, centroids)\n            new_labels = [min(range(len(distances[i])), key=lambda k: distances[i][k]) for i in range(len(distances))]\n            \n            # Update centroids\n            new_centroids = []\n            for k in range(self.n_clusters):\n                # Find points assigned to this cluster\n                cluster_points = [embeddings[i] for i in range(len(embeddings)) if new_labels[i] == k]\n                \n                if cluster_points:\n                    # Calculate mean of cluster points\n                    centroid = [0.0] * n_features\n                    for point in cluster_points:\n                        for j in range(n_features):\n                            centroid[j] += point[j]\n                    centroid = [c / len(cluster_points) for c in centroid]\n                else:\n                    # Keep old centroid if no points assigned\n                    centroid = centroids[k]\n                \n                new_centroids.append(centroid)\n            \n            # Check for convergence\n            if iteration > 0 and self._centroids_equal(centroids, new_centroids):\n                logger.info(f\"Clustering converged after {iteration + 1} iterations\")\n                break\n            \n            centroids = new_centroids\n        \n        self.cluster_centers = centroids\n        return new_labels\n    \n    def _centroids_equal(self, a: List[List[float]], b: List[List[float]], rtol: float = 1e-4) -> bool:\n        \"\"\"Check if centroids are approximately equal.\"\"\"\n        if len(a) != len(b):\n            return False\n        for i in range(len(a)):\n            if len(a[i]) != len(b[i]):\n                return False\n            for j in range(len(a[i])):\n                if abs(a[i][j] - b[i][j]) > rtol * max(abs(a[i][j]), abs(b[i][j]), 1e-8):\n                    return False\n        return True\n    \n    def _calculate_distances(self, points: List[List[float]], centroids: List[List[float]]) -> List[List[float]]:\n        \"\"\"Calculate Euclidean distances between points and centroids.\"\"\"\n        distances = []\n        \n        for point in points:\n            point_distances = []\n            for centroid in centroids:\n                distance = 0.0\n                for i in range(len(point)):\n                    distance += (point[i] - centroid[i]) ** 2\n                distance = distance ** 0.5\n                point_distances.append(distance)\n            distances.append(point_distances)\n        \n        return distances\n\n\nclass ClusterAnalyzer:\n    \"\"\"Analyzes and describes clusters.\"\"\"\n    \n    def analyze_clusters(self, memory_blocks: List[MemoryBlock], labels: List[int]) -> Dict[int, Dict[str, Any]]:\n        \"\"\"Analyze clusters and generate descriptions.\"\"\"\n        cluster_analysis = defaultdict(lambda: {\n            'blocks': [],\n            'archetypes': defaultdict(int),\n            'projects': defaultdict(int),\n            'topics': defaultdict(int),\n            'file_types': defaultdict(int),\n            'description': '',\n            'representative_block': None\n        })\n        \n        # Group blocks by cluster\n        for block, label in zip(memory_blocks, labels):\n            cluster_analysis[label]['blocks'].append(block)\n            cluster_analysis[label]['archetypes'][block.archetype] += 1\n            cluster_analysis[label]['projects'][block.project] += 1\n            cluster_analysis[label]['file_types'][block.file_type or 'unknown'] += 1\n            \n            for topic in block.topics:\n                cluster_analysis[label]['topics'][topic] += 1\n        \n        # Generate descriptions and find representative blocks\n        for cluster_id, data in cluster_analysis.items():\n            blocks = data['blocks']\n            \n            # Find most common elements\n            top_archetype = max(data['archetypes'].items(), key=lambda x: x[1])[0]\n            top_project = max(data['projects'].items(), key=lambda x: x[1])[0]\n            top_topics = sorted(data['topics'].items(), key=lambda x: x[1], reverse=True)[:3]\n            \n            # Generate description\n            data['description'] = f\"Cluster of {len(blocks)} blocks, primarily {top_archetype} archetype in {top_project} project. Key topics: {', '.join([topic for topic, count in top_topics])}\"\n            \n            # Find representative block (longest content)\n            data['representative_block'] = max(blocks, key=lambda b: len(b.content))\n        \n        return dict(cluster_analysis)\n\n\nclass ClusterVisualizer:\n    \"\"\"Creates visualizations of clusters.\"\"\"\n    \n    def create_cluster_visualization(self, embeddings: List[List[float]], labels: List[int], \n                                   memory_blocks: List[MemoryBlock], output_dir: Path):\n        \"\"\"Create cluster visualization using dimensionality reduction.\"\"\"\n        try:\n            # Simple 2D projection using PCA\n            reduced_embeddings = self._simple_pca(embeddings, n_components=2)\n            \n            # Create HTML visualization\n            self._create_html_plot(reduced_embeddings, labels, memory_blocks, output_dir)\n            \n        except Exception as e:\n            logger.error(f\"Visualization failed: {str(e)}\")\n    \n    def _simple_pca(self, data: List[List[float]], n_components: int = 2) -> List[List[float]]:\n        \"\"\"Simple PCA implementation.\"\"\"\n        if not data or not data[0]:\n            return []\n        \n        n_samples = len(data)\n        n_features = len(data[0])\n        \n        # Center the data\n        mean = [sum(data[i][j] for i in range(n_samples)) / n_samples for j in range(n_features)]\n        centered_data = [[data[i][j] - mean[j] for j in range(n_features)] for i in range(n_samples)]\n        \n        # For simplicity, just project onto first 2 dimensions\n        # In a real implementation, we'd compute eigenvalues/eigenvectors\n        if n_features >= 2:\n            reduced_data = [[row[0], row[1]] for row in centered_data]\n        elif n_features == 1:\n            reduced_data = [[row[0], 0.0] for row in centered_data]\n        else:\n            reduced_data = [[0.0, 0.0] for _ in range(n_samples)]\n        \n        return reduced_data\n    \n    def _create_html_plot(self, embeddings_2d: List[List[float]], labels: List[int], \n                         memory_blocks: List[MemoryBlock], output_dir: Path):\n        \"\"\"Create interactive HTML plot.\"\"\"\n        colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown', 'pink', 'gray', 'olive', 'cyan']\n        \n        html_content = \"\"\"\n<!DOCTYPE html>\n<html>\n<head>\n    <title>MUSE Pantheon Cluster Visualization</title>\n    <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>\n</head>\n<body>\n    <h1>MUSE Pantheon Memory Block Clusters</h1>\n    <div id=\"plot\" style=\"width:100%;height:600px;\"></div>\n    <script>\n\"\"\"\n        \n        # Prepare data for each cluster\n        cluster_data = defaultdict(lambda: {'x': [], 'y': [], 'text': [], 'name': ''})\n        \n        for i, (embedding, label, block) in enumerate(zip(embeddings_2d, labels, memory_blocks)):\n            cluster_data[label]['x'].append(embedding[0])\n            cluster_data[label]['y'].append(embedding[1])\n            cluster_data[label]['text'].append(f\"{block.summary[:100]}...\")\n            cluster_data[label]['name'] = f\"Cluster {label}\"\n        \n        # Generate Plotly traces\n        traces = []\n        for cluster_id, data in cluster_data.items():\n            color = colors[cluster_id % len(colors)]\n            trace = f\"\"\"{{\n                x: {data['x']},\n                y: {data['y']},\n                text: {json.dumps(data['text'])},\n                mode: 'markers',\n                type: 'scatter',\n                name: '{data['name']}',\n                marker: {{\n                    color: '{color}',\n                    size: 10\n                }},\n                hovertemplate: '%{{text}}<extra></extra>'\n            }}\"\"\"\n            traces.append(trace)\n        \n        html_content += f\"\"\"\n        var data = [{', '.join(traces)}];\n        \n        var layout = {{\n            title: 'Memory Block Clusters (PCA Projection)',\n            xaxis: {{ title: 'PC1' }},\n            yaxis: {{ title: 'PC2' }},\n            hovermode: 'closest'\n        }};\n        \n        Plotly.newPlot('plot', data, layout);\n    </script>\n</body>\n</html>\"\"\"\n        \n        output_file = output_dir / \"cluster_visualization.html\"\n        with open(output_file, 'w') as f:\n            f.write(html_content)\n        \n        logger.info(f\"Cluster visualization saved to: {output_file}\")\n\n\ndef main():\n    \"\"\"Main entry point.\"\"\"\n    parser = argparse.ArgumentParser(description=\"Clustering and Embeddings for MUSE Pantheon\")\n    parser.add_argument('--blocks-dir', required=True,\n                       help='Directory containing MemoryBlock JSON files')\n    parser.add_argument('--output-dir', required=True,\n                       help='Output directory for clustering results')\n    parser.add_argument('--n-clusters', type=int,\n                       help='Number of clusters (auto-detected if not specified)')\n    parser.add_argument('--assign-projects', action='store_true',\n                       help='Also run project assignment based on clusters')\n    \n    args = parser.parse_args()\n    \n    blocks_dir = Path(args.blocks_dir)\n    output_dir = Path(args.output_dir)\n    output_dir.mkdir(parents=True, exist_ok=True)\n    \n    # Load memory blocks\n    logger.info(f\"Loading memory blocks from: {blocks_dir}\")\n    memory_blocks = []\n    \n    for json_file in blocks_dir.glob(\"*.json\"):\n        # Skip non-memory block files\n        if not json_file.name.startswith(\"memory_block_\"):\n            continue\n            \n        try:\n            with open(json_file, 'r') as f:\n                data = json.load(f)\n                block = MemoryBlock.from_dict(data)\n                memory_blocks.append(block)\n        except Exception as e:\n            logger.error(f\"Failed to load {json_file}: {str(e)}\")\n    \n    if not memory_blocks:\n        logger.error(\"No memory blocks found!\")\n        return 1\n    \n    logger.info(f\"Loaded {len(memory_blocks)} memory blocks\")\n    \n    # Generate embeddings\n    logger.info(\"Generating embeddings...\")\n    embedding_generator = EmbeddingGenerator()\n    embeddings = embedding_generator.fit_transform(memory_blocks)\n    \n    # Perform clustering\n    logger.info(\"Performing clustering...\")\n    clustering = SemanticClustering(n_clusters=args.n_clusters)\n    labels = clustering.fit_predict(embeddings, memory_blocks)\n    \n    # Analyze clusters\n    logger.info(\"Analyzing clusters...\")\n    analyzer = ClusterAnalyzer()\n    cluster_analysis = analyzer.analyze_clusters(memory_blocks, labels)\n    \n    # Create visualization\n    logger.info(\"Creating visualization...\")\n    visualizer = ClusterVisualizer()\n    visualizer.create_cluster_visualization(embeddings, labels, memory_blocks, output_dir)\n    \n    # Save results\n    results = {\n        'clustering_results': {\n            'n_clusters': len(set(labels)),\n            'cluster_labels': labels,\n            'cluster_analysis': {\n                str(k): {\n                    'description': v['description'],\n                    'n_blocks': len(v['blocks']),\n                    'top_archetype': max(v['archetypes'].items(), key=lambda x: x[1])[0],\n                    'top_project': max(v['projects'].items(), key=lambda x: x[1])[0],\n                    'representative_block_id': v['representative_block'].id_hash\n                }\n                for k, v in cluster_analysis.items()\n            }\n        },\n        'memory_blocks_with_clusters': [\n            {\n                'id_hash': block.id_hash,\n                'cluster_label': label,\n                'summary': block.summary,\n                'archetype': block.archetype,\n                'project': block.project\n            }\n            for block, label in zip(memory_blocks, labels)\n        ]\n    }\n    \n    # Save clustering results\n    results_file = output_dir / \"clustering_results.json\"\n    with open(results_file, 'w') as f:\n        json.dump(results, f, indent=2)\n    \n    logger.info(f\"Clustering results saved to: {results_file}\")\n    logger.info(f\"Found {len(set(labels))} clusters\")\n    \n    return 0\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())",
  "topics": [
    "return",
    "py",
    "cluster",
    "embeddings",
    "data",
    "import",
    "block"
  ],
  "skills": [
    "nano_warden_universal_ingest_py.py",
    "nano_warden_code_analyzer.py",
    "nano_warden_dependency_tracker.py",
    "nano_warden_memory_system.py"
  ],
  "date": "2025-09-17",
  "project": "muse.pantheon",
  "archetype": "Alchemist",
  "created_at": "2025-09-17T18:19:13.043745+00:00",
  "source_path": "warden/tools/cluster_embeddings.py",
  "file_type": ".py",
  "pii_redacted": false,
  "consent_logged": true,
  "ethics_review": "passed",
  "links": [],
  "parent_blocks": [],
  "metadata": {
    "file_size": 20979,
    "source": "file_ingest",
    "validation_status": "passed",
    "project_assigned_by": "auto_classifier",
    "original_project": "muse.pantheon",
    "project_hierarchy": {
      "domain": "muse",
      "category": "pantheon",
      "subcategory": "core",
      "description": "Core MUSE Pantheon memory system"
    }
  }
}